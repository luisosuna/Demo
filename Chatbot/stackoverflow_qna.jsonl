{"question": "How do I split the definition of a long string over multiple lines?", "tags": ["python", "string", "multiline", "multilinestring"], "link": "https://stackoverflow.com/questions/10660435/how-do-i-split-the-definition-of-a-long-string-over-multiple-lines", "answer_count": 31, "answers": {"id": 24331604, "body": "Breaking lines by  \\  works for me.  Here is an example:   longStr = \"This is a very long string \" \\         \"that I wrote to help somebody \" \\         \"who had a question about \" \\         \"writing long strings in Python\"", "score": 199}}
{"question": "How to prettyprint a JSON file?", "tags": ["python", "json", "formatting", "pretty-print"], "link": "https://stackoverflow.com/questions/12943819/how-to-prettyprint-a-json-file", "answer_count": 15, "answers": {"id": 48111364, "body": "After reading the data with the  json  standard library module, use the  pprint  standard library module to display the parsed data. Example:   import json import pprint  json_data = None with open('file_name.txt', 'r') as f:     data = f.read()     json_data = json.loads(data)  # print json to screen with human-friendly formatting pprint.pprint(json_data, compact=True)  # write json to file with human-friendly formatting pretty_json_str = pprint.pformat(json_data, compact=True).replace(\"'\",'\"')  with open('file_name.json', 'w') as f:     f.write(pretty_json_str)     The default  indent  is 1, so you may want to specify your own.   By default, pprint will output lists like this:   'not compact': ['pprint',                 'will',                 'output',                 'lists',                 'like',                 'this'],    And that is no better than  json.dump()  or  json.dumps()  with an  indent  specified.   With   compact=True , pprint will output lists something like this:   'compact': ['pprint', 'with compact=True', 'will', 'output',             'lists', 'like', 'this'],    Note that you can specify where it will wrap with the  width  argument.   It is necessary to replace 'single quotes' with 'double quotes' in the string returned by pprint.pformat(), because single quotes are not valid json. When you look at the file in your text editor, it may be unable to highlight the json properly.   In any case, you may want to save it as valid json, so that you can as a human can simply read your json files comfortably, if it is not more important that they are formatted without spacing so they can be computer-processed with blinding speed.   Ultimately, the output will look like this:   {'address': {'city': 'New York',              'postalCode': '10021-3100',              'state': 'NY',              'streetAddress': '21 2nd Street'},  'age': 27,  'children': [],  'firstName': 'John',  'isAlive': True,  'lastName': 'Smith'}", "score": 160}}
{"question": "How to read a file line-by-line into a list?", "tags": ["python", "string", "file", "readlines"], "link": "https://stackoverflow.com/questions/3277503/how-to-read-a-file-line-by-line-into-a-list", "answer_count": 28, "answers": {"id": 3277512, "body": "This is more explicit than necessary, but does what you want.   with open(\"file.txt\") as file_in:     lines = []     for line in file_in:         lines.append(line)", "score": 724}}
{"question": "Getting the class name of an instance", "tags": ["python", "introspection", "instanceof", "python-datamodel"], "link": "https://stackoverflow.com/questions/510972/getting-the-class-name-of-an-instance", "answer_count": 12, "answers": {"id": 510988, "body": "type()  ?   >>> class A: ...     def whoami(self): ...         print(type(self).__name__) ... >>> >>> class B(A): ...     pass ... >>> >>> >>> o = B() >>> o.whoami() 'B' >>>", "score": 192}}
{"question": "How do I check if a string represents a number (float or int)?", "tags": ["python", "casting", "floating-point", "type-conversion", "integer"], "link": "https://stackoverflow.com/questions/354038/how-do-i-check-if-a-string-represents-a-number-float-or-int", "answer_count": 41, "answers": {"id": 23639915, "body": "TL;DR  The best solution is  s.replace('.','',1).isdigit()   I did some  benchmarks  comparing the different approaches   def is_number_tryexcept(s):     \"\"\" Returns True if string is a number. \"\"\"     try:         float(s)         return True     except ValueError:         return False         import re     def is_number_regex(s):     \"\"\" Returns True if string is a number. \"\"\"     if re.match(\"^\\d+?\\.\\d+?$\", s) is None:         return s.isdigit()     return True   def is_number_repl_isdigit(s):     \"\"\" Returns True if string is a number. \"\"\"     return s.replace('.','',1).isdigit()    If the string is not a number, the except-block is quite slow. But more importantly, the try-except method is the only approach that handles scientific notations correctly.   funcs = [           is_number_tryexcept,            is_number_regex,           is_number_repl_isdigit           ]  a_float = '.1234'  print('Float notation \".1234\" is not supported by:') for f in funcs:     if not f(a_float):         print('\\t -', f.__name__)    Float notation \".1234\" is not supported by:     is_number_regex   scientific1 = '1.000000e+50' scientific2 = '1e50'   print('Scientific notation \"1.000000e+50\" is not supported by:') for f in funcs: if not f(scientific1): print('\\t -', f. name )   print('Scientific notation \"1e50\" is not supported by:') for f in funcs: if not f(scientific2): print('\\t -', f. name )       Scientific notation \"1.000000e+50\" is not supported by:     is_number_regex   is_number_repl_isdigit  Scientific notation \"1e50\" is not supported by:   is_number_regex   is_number_repl_isdigit     EDIT: The benchmark results   import timeit  test_cases = ['1.12345', '1.12.345', 'abc12345', '12345'] times_n = {f.__name__:[] for f in funcs}  for t in test_cases:     for f in funcs:         f = f.__name__         times_n[f].append(min(timeit.Timer('%s(t)' %f,                        'from __main__ import %s, t' %f)                               .repeat(repeat=3, number=1000000)))    where the following functions were tested   from re import match as re_match from re import compile as re_compile  def is_number_tryexcept(s):     \"\"\" Returns True if string is a number. \"\"\"     try:         float(s)         return True     except ValueError:         return False  def is_number_regex(s):     \"\"\" Returns True if string is a number. \"\"\"     if re_match(\"^\\d+?\\.\\d+?$\", s) is None:         return s.isdigit()     return True   comp = re_compile(\"^\\d+?\\.\\d+?$\")      def compiled_regex(s):     \"\"\" Returns True if string is a number. \"\"\"     if comp.match(s) is None:         return s.isdigit()     return True   def is_number_repl_isdigit(s):     \"\"\" Returns True if string is a number. \"\"\"     return s.replace('.','',1).isdigit()", "score": 342}}
{"question": "How do I get the row count of a Pandas DataFrame?", "tags": ["python", "pandas", "dataframe"], "link": "https://stackoverflow.com/questions/15943769/how-do-i-get-the-row-count-of-a-pandas-dataframe", "answer_count": 20, "answers": {"id": 18317067, "body": "Use  len(df)  :-).   __len__()  is documented with \"Returns length of index\".   Timing info, set up the same way as in  root's answer :   In [7]: timeit len(df.index) 1000000 loops, best of 3: 248 ns per loop  In [8]: timeit len(df) 1000000 loops, best of 3: 573 ns per loop    Due to one additional function call, it is of course correct to say that it is a bit slower than calling  len(df.index)  directly. But this should not matter in most cases. I find  len(df)  to be quite readable.", "score": 285}}
{"question": "Meaning of @classmethod and @staticmethod for beginner", "tags": ["python", "oop", "static-methods", "class-method"], "link": "https://stackoverflow.com/questions/12179271/meaning-of-classmethod-and-staticmethod-for-beginner", "answer_count": 12, "answers": {"id": 12179325, "body": "@classmethod  means: when this method is called, we pass the class as the first argument instead of the instance of that class (as we normally do with methods). This means you can use the class and its properties inside that method rather than a particular instance.   @staticmethod  means:  when this method is called, we don't pass an instance of the class to it (as we normally do with methods). This means you can put a function inside a class but you can't access the instance of that class (this is useful when your method does not use the instance).", "score": 307}}
{"question": "What&#39;s the canonical way to check for type in Python?", "tags": ["python", "types"], "link": "https://stackoverflow.com/questions/152580/whats-the-canonical-way-to-check-for-type-in-python", "answer_count": 19, "answers": {"id": 152592, "body": "isinstance(o, str)  will return  True  if  o  is an  str  or is of a type that inherits from  str .   type(o) is str  will return  True  if and only if  o  is a  str . It will return  False  if  o  is of a type that inherits from  str .", "score": 86}}
{"question": "How can I install packages using pip according to the requirements.txt file from a local directory?", "tags": ["python", "virtualenv", "pip"], "link": "https://stackoverflow.com/questions/7225900/how-can-i-install-packages-using-pip-according-to-the-requirements-txt-file-from", "answer_count": 18, "answers": {"id": 22046133, "body": "For virtualenv to install all files in the  requirements.txt  file.     cd to the directory where  requirements.txt  is located   activate your virtualenv   run:   pip install -r requirements.txt  in your shell", "score": 202}}
{"question": "Proper way to declare custom exceptions in modern Python?", "tags": ["python", "python-3.x", "exception", "python-dataclasses"], "link": "https://stackoverflow.com/questions/1319615/proper-way-to-declare-custom-exceptions-in-modern-python", "answer_count": 17, "answers": {"id": 26938914, "body": "\"What is the proper way to declare custom exceptions in modern Python?\"     This is fine unless your exception is really a type of a more specific exception:   class MyException(Exception):     pass    Or better (maybe perfect), instead of  pass  give a docstring:   class MyException(Exception):     \"\"\"Raise for my specific kind of exception\"\"\"    Subclassing Exception Subclasses   From the  docs     Exception   All built-in, non-system-exiting exceptions are derived from this class. All user-defined exceptions should also be derived from this class.     That means that  if  your exception is a type of a more specific exception, subclass that exception instead of the generic  Exception  (and the result will be that you still derive from  Exception  as the docs recommend). Also, you can at least provide a docstring (and not be forced to use the  pass  keyword):   class MyAppValueError(ValueError):     '''Raise when my specific value is wrong'''    Set attributes you create yourself with a custom  __init__ . Avoid passing a dict as a positional argument, future users of your code will thank you. If you use the deprecated message attribute, assigning it yourself will avoid a  DeprecationWarning :   class MyAppValueError(ValueError):     '''Raise when a specific subset of values in context of app is wrong'''     def __init__(self, message, foo, *args):         self.message = message # without this you may get DeprecationWarning         # Special attribute you desire with your Error,          # perhaps the value that caused the error?:         self.foo = foo                  # allow users initialize misc. arguments as any other builtin Error         super(MyAppValueError, self).__init__(message, foo, *args)     There's really no need to write your own  __str__  or  __repr__ . The built-in ones are very nice, and your  cooperative inheritance  ensures that you use them.   Critique of the top answer     Maybe I missed the question, but why not:   class MyException(Exception):     pass      Again, the problem with the above is that in order to catch it, you'll either have to name it specifically (importing it if created elsewhere) or catch Exception, (but you're probably not prepared to handle all types of Exceptions, and you should only catch exceptions you are prepared to handle). Similar criticism to the below, but additionally that's not the way to initialize via  super , and you'll get a  DeprecationWarning  if you access the message attribute:     Edit: to override something (or pass extra args), do this:   class ValidationError(Exception):     def __init__(self, message, errors):          # Call the base class constructor with the parameters it needs         super(ValidationError, self).__init__(message)          # Now for your custom code...         self.errors = errors    That way you could pass dict of error messages to the second param, and get to it later with  e.errors .     It also requires exactly two arguments to be passed in (aside from the  self .) No more, no less. That's an interesting constraint that future users may not appreciate.   To be direct - it violates  Liskov substitutability .   I'll demonstrate both errors:   >>> ValidationError('foo', 'bar', 'baz').message  Traceback (most recent call last):   File \" \", line 1, in       ValidationError('foo', 'bar', 'baz').message TypeError: __init__() takes exactly 3 arguments (4 given)  >>> ValidationError('foo', 'bar').message __main__:1: DeprecationWarning: BaseException.message has been deprecated as of Python 2.6 'foo'    Compared to:   >>> MyAppValueError('foo', 'FOO', 'bar').message 'foo'", "score": 327}}
{"question": "Generate random integers between 0 and 9", "tags": ["python", "random", "integer"], "link": "https://stackoverflow.com/questions/3996904/generate-random-integers-between-0-and-9", "answer_count": 24, "answers": {"id": 3996919, "body": "Try this:   from random import randrange, uniform  # randrange gives you an integral value irand = randrange(0, 10)  # uniform gives you a floating-point value frand = uniform(0, 10)", "score": 196}}
{"question": "fatal error: Python.h: No such file or directory", "tags": ["python", "gcc", "python-c-api"], "link": "https://stackoverflow.com/questions/21530577/fatal-error-python-h-no-such-file-or-directory", "answer_count": 37, "answers": {"id": 57698471, "body": "For  Python 3.7  and  Ubuntu  in particular, I needed   sudo apt install libpython3.7-dev   . I think at some point names were changed from  pythonm.n-dev  to this.   for Python 3.6, 3.8 through 3.10 (and counting\u2026) similarly:   sudo apt install libpython3.6-dev \u2007   sudo apt install libpython3.8-dev \u2007   sudo apt install libpython3.9-dev   sudo apt install libpython3.10-dev   sudo apt install libpython3.11-dev   sudo apt install libpython3.12-dev", "score": 443}}
{"question": "What is the meaning of single and double underscore before an object name?", "tags": ["python", "oop", "naming-conventions", "identifier"], "link": "https://stackoverflow.com/questions/1301346/what-is-the-meaning-of-single-and-double-underscore-before-an-object-name", "answer_count": 19, "answers": {"id": 1301557, "body": "Excellent answers so far but some tidbits are missing. A single leading underscore isn't exactly  just  a convention: if you use  from foobar import * , and module  foobar  does not define an  __all__  list, the names imported from the module  do not  include those with a leading underscore. Let's say it's  mostly  a convention, since this case is a pretty obscure corner;-).   The leading-underscore convention is widely used not just for  private  names, but also for what C++ would call  protected  ones -- for example, names of methods that are fully intended to be overridden by subclasses (even ones that  have  to be overridden since in the base class they  raise NotImplementedError !-) are often single-leading-underscore names to indicate to code  using  instances of that class (or subclasses) that said methods are not meant to be called directly.   For example, to make a thread-safe queue with a different queueing discipline than FIFO, one imports Queue, subclasses Queue.Queue, and overrides such methods as  _get  and  _put ; \"client code\" never calls those (\"hook\") methods, but rather the (\"organizing\") public methods such as  put  and  get  (this is known as the  Template Method  design pattern -- see e.g.  here  for an interesting presentation based on a video of a talk of mine on the subject, with the addition of synopses of the transcript).   Edit: The video links in the description of the talks are now broken. You can find the first two videos  here  and  here .", "score": 389}}
{"question": "Relative imports for the billionth time", "tags": ["python", "python-import", "relative-path", "python-packaging", "relative-import"], "link": "https://stackoverflow.com/questions/14132789/relative-imports-for-the-billionth-time", "answer_count": 17, "answers": {"id": 65589847, "body": "There are too many long answers in a foreign language. So, I'll try to make it short.   If you write  from . import module , opposite to what you think,  module  will not be imported from current directory, but from the top level of your package! If you run  .py  file as a script, it simply doesn't know where the top level is and thus refuses to work.   If you start it like this  py -m package.module  from the directory above  package , then Python knows where the top level is. That's very similar to Java:  java -cp bin_directory package.class", "score": 79}}
{"question": "Extracting extension from filename", "tags": ["python", "filenames", "file-extension"], "link": "https://stackoverflow.com/questions/541390/extracting-extension-from-filename", "answer_count": 33, "answers": {"id": 541408, "body": "import os.path extension = os.path.splitext(filename)[1]", "score": 501}}
{"question": "How do I write JSON data to a file?", "tags": ["python", "json"], "link": "https://stackoverflow.com/questions/12309269/how-do-i-write-json-data-to-a-file", "answer_count": 19, "answers": {"id": 20776329, "body": "I would answer with slight modification with aforementioned answers and that is to write a prettified JSON file which human eyes can read better. For this, pass  sort_keys  as  True  and  indent  with 4 space characters and you are good to go. Also take care of ensuring that the ascii codes will not be written in your JSON file:   with open('data.txt', 'w') as out_file:      json.dump(json_data, out_file, sort_keys = True, indent = 4,                ensure_ascii = False)", "score": 200}}
{"question": "How can I import a module dynamically given the full path?", "tags": ["python", "python-import", "python-module"], "link": "https://stackoverflow.com/questions/67631/how-can-i-import-a-module-dynamically-given-the-full-path", "answer_count": 38, "answers": {"id": 53311583, "body": "To import your module, you need to add its directory to the environment variable, either temporarily or permanently.   Temporarily   import sys sys.path.append(\"/path/to/my/modules/\") import my_module    Permanently   Adding the following line to your  .bashrc  (or alternative) file in Linux and excecute  source ~/.bashrc  (or alternative) in the terminal:   export PYTHONPATH=\"${PYTHONPATH}:/path/to/my/modules/\"    Credit/Source:  saarrrr ,  another Stack\u00a0Exchange question", "score": 152}}
{"question": "How can I import a module dynamically given the full path?", "tags": ["python", "python-import", "python-module"], "link": "https://stackoverflow.com/questions/67631/how-can-i-import-a-module-dynamically-given-the-full-path", "answer_count": 38, "answers": {"id": 129374, "body": "The advantage of adding a path to sys.path (over using imp) is that it simplifies things when importing more than one module from a single package.  For example:   import sys # the mock-0.3.1 dir contains testcase.py, testutils.py & mock.py sys.path.append('/foo/bar/mock-0.3.1')  from testcase import TestCase from testutils import RunTests from mock import Mock, sentinel, patch", "score": 598}}
{"question": "How can I import a module dynamically given the full path?", "tags": ["python", "python-import", "python-module"], "link": "https://stackoverflow.com/questions/67631/how-can-i-import-a-module-dynamically-given-the-full-path", "answer_count": 38, "answers": {"id": 67692, "body": "Let's have  MyClass  in  module.name  module defined at  /path/to/file.py . Below is how we import  MyClass  from this module   For Python 3.5+ use ( docs ):   import importlib.util import sys spec = importlib.util.spec_from_file_location(\"module.name\", \"/path/to/file.py\") foo = importlib.util.module_from_spec(spec) sys.modules[\"module.name\"] = foo spec.loader.exec_module(foo) foo.MyClass()    For Python 3.3 and 3.4 use:   from importlib.machinery import SourceFileLoader  foo = SourceFileLoader(\"module.name\", \"/path/to/file.py\").load_module() foo.MyClass()    (Although this has been deprecated in Python 3.4.)   For Python 2 use:   import imp  foo = imp.load_source('module.name', '/path/to/file.py') foo.MyClass()    There are equivalent convenience functions for compiled Python files and DLLs.   See also  http://bugs.python.org/issue21436 .", "score": 1808}}
{"question": "How do I write JSON data to a file?", "tags": ["python", "json"], "link": "https://stackoverflow.com/questions/12309269/how-do-i-write-json-data-to-a-file", "answer_count": 19, "answers": {"id": 14870531, "body": "To get  utf8 -encoded  file as opposed to  ascii -encoded  in the accepted answer for Python 2 use:   import io, json with io.open('data.txt', 'w', encoding='utf-8') as f:   f.write(json.dumps(data, ensure_ascii=False))    The code is simpler in Python 3:   import json with open('data.txt', 'w') as f:   json.dump(data, f, ensure_ascii=False)    On Windows, the  encoding='utf-8'  argument to  open  is still necessary.   To avoid storing an encoded copy of the data in memory (result of  dumps ) and to output  utf8-encoded  bytestrings in both Python 2 and 3, use:   import json, codecs with open('data.txt', 'wb') as f:     json.dump(data, codecs.getwriter('utf-8')(f), ensure_ascii=False)    The  codecs.getwriter  call is redundant in Python 3 but required for Python 2     Readability and size:   The use of  ensure_ascii=False  gives better readability and smaller size:   >>> json.dumps({'price': '\u20ac10'}) '{\"price\": \"\\\\u20ac10\"}' >>> json.dumps({'price': '\u20ac10'}, ensure_ascii=False) '{\"price\": \"\u20ac10\"}'  >>> len(json.dumps({'\u0430\u0431\u0432\u0433\u0434': 1})) 37 >>> len(json.dumps({'\u0430\u0431\u0432\u0433\u0434': 1}, ensure_ascii=False).encode('utf8')) 17    Further improve readability by adding flags  indent=4, sort_keys=True  (as suggested by  dinos66 ) to arguments of  dump  or  dumps . This way you'll get a nicely indented sorted structure in the json file at the cost of a slightly larger file size.", "score": 344}}
{"question": "How do I write JSON data to a file?", "tags": ["python", "json"], "link": "https://stackoverflow.com/questions/12309269/how-do-i-write-json-data-to-a-file", "answer_count": 19, "answers": {"id": 12309296, "body": "data  is a Python dictionary. It needs to be encoded as JSON before writing.   Use this for maximum compatibility (Python 2 and 3):   import json with open('data.json', 'w') as f:     json.dump(data, f)    On a modern system (i.e. Python 3 and UTF-8 support), you can write a nicer file using:   import json with open('data.json', 'w', encoding='utf-8') as f:     json.dump(data, f, ensure_ascii=False, indent=4)    See  json  documentation.", "score": 3315}}
{"question": "Extracting extension from filename", "tags": ["python", "filenames", "file-extension"], "link": "https://stackoverflow.com/questions/541390/extracting-extension-from-filename", "answer_count": 33, "answers": {"id": 35188296, "body": "New in version 3.4.   import pathlib  print(pathlib.Path('/foo/bar.txt').suffix)   # Outputs: .txt  print(pathlib.Path('/foo/bar.txt').stem)   # Outputs: bar  print(pathlib.Path(\"hello/foo.bar.tar.gz\").suffixes)   # Outputs: ['.bar', '.tar', '.gz']  print(''.join(pathlib.Path(\"hello/foo.bar.tar.gz\").suffixes)) # Outputs: .bar.tar.gz  print(pathlib.Path(\"hello/foo.bar.tar.gz\").stem)   # Outputs: foo.bar.tar    I'm surprised no one has mentioned  pathlib  yet,  pathlib  IS awesome!", "score": 735}}
{"question": "Extracting extension from filename", "tags": ["python", "filenames", "file-extension"], "link": "https://stackoverflow.com/questions/541390/extracting-extension-from-filename", "answer_count": 33, "answers": {"id": 541394, "body": "Use  os.path.splitext :   >>> import os >>> filename, file_extension = os.path.splitext('/path/to/somefile.ext') >>> filename '/path/to/somefile' >>> file_extension '.ext'    Unlike most manual string-splitting attempts,  os.path.splitext  will correctly treat  /a/b.c/d  as having no extension instead of having extension  .c/d , and it will treat  .bashrc  as having no extension instead of having extension  .bashrc :   >>> os.path.splitext('/a/b.c/d') ('/a/b.c/d', '') >>> os.path.splitext('.bashrc') ('.bashrc', '')", "score": 2668}}
{"question": "Relative imports for the billionth time", "tags": ["python", "python-import", "relative-path", "python-packaging", "relative-import"], "link": "https://stackoverflow.com/questions/14132789/relative-imports-for-the-billionth-time", "answer_count": 17, "answers": {"id": 49480246, "body": "This is really a problem within python.  The origin of confusion is that people mistakenly take the relative import as path relative which is not.   For example when you write in  faa.py :   from .. import foo    This has a meaning only if  faa.py  was  identified and loaded  by python, during execution, as a part of a package. In that case, the  module's name  for  faa.py  would be for example  some_packagename.faa . If the file was loaded just because it is in the current directory, when python is run, then its name would not refer to any package and eventually relative import would fail.   A simple solution to refer modules in the current directory, is to use this:   if __package__ is None or __package__ == '':     # uses current directory visibility     import foo else:     # uses current package visibility     from . import foo", "score": 116}}
{"question": "Relative imports for the billionth time", "tags": ["python", "python-import", "relative-path", "python-packaging", "relative-import"], "link": "https://stackoverflow.com/questions/14132789/relative-imports-for-the-billionth-time", "answer_count": 17, "answers": {"id": 14132912, "body": "Script vs. Module   Here's an explanation.  The short version is that there is a big difference between directly running a Python file, and importing that file from somewhere else.   Just knowing what directory a file is in does not determine what package Python thinks it is in.   That depends, additionally, on how you load the file into Python (by running or by importing).   There are two ways to load a Python file: as the top-level script, or as a module.  A file is loaded as the top-level script if you execute it directly, for instance by typing  python myfile.py  on the command line.  It is loaded as a module when an  import  statement is encountered inside some other file.  There can only be one top-level script at a time; the top-level script is the Python file you ran to start things off.   Naming   When a file is loaded, it is given a name (which is stored in its  __name__  attribute).  If it was loaded as the top-level script, its name is  __main__ .  If it was loaded as a module, its name is the filename, preceded by the names of any packages/subpackages of which it is a part, separated by dots.   So for instance in your example:   package/     __init__.py     subpackage1/         __init__.py         moduleX.py     moduleA.py    if you imported  moduleX  (note:  imported , not directly executed), its name would be  package.subpackage1.moduleX .  If you imported  moduleA , its name would be  package.moduleA .  However, if you  directly run   moduleX  from the command line, its name will instead be  __main__ , and if you directly run  moduleA  from the command line, its name will be  __main__ .  When a module is run as the top-level script, it loses its normal name and its name is instead  __main__ .   Accessing a module NOT through its containing package   There is an additional wrinkle: the module's name depends on whether it was imported \"directly\" from the directory it is in or imported via a package.  This only makes a difference if you run Python in a directory, and try to import a file in that same directory (or a subdirectory of it).  For instance, if you start the Python interpreter in the directory  package/subpackage1  and then do  import moduleX , the name of  moduleX  will just be  moduleX , and not  package.subpackage1.moduleX .  This is because Python adds the current directory to its search path when the interpreter is entered interactively; if it finds the to-be-imported module in the current directory, it will not know that that directory is part of a package, and the package information will not become part of the module's name.   A special case is if you run the interpreter interactively (e.g., just type  python  and start entering Python code on the fly).  In this case, the name of that interactive session is  __main__ .   Now here is the crucial thing for your error message:  if a module's name has no dots, it is not considered to be part of a package .  It doesn't matter where the file actually is on disk.  All that matters is what its name is, and its name depends on how you loaded it.   Now look at the quote you included in your question:     Relative imports use a module's name attribute to determine that module's position in the package hierarchy. If the module's name does not contain any package information (e.g. it is set to 'main') then relative imports are resolved as if the module were a top-level module, regardless of where the module is actually located on the file system.     Relative imports...   Relative imports use the module's  name  to determine where it is in a package.  When you use a relative import like  from .. import foo , the dots indicate to step up some number of levels in the package hierarchy.  For instance, if your current module's name is  package.subpackage1.moduleX , then  ..moduleA  would mean  package.moduleA .  For a  from .. import  to work, the module's name must have at least as many dots as there are in the  import  statement.   ... are only relative in a package   However, if your module's name is  __main__ , it is not considered to be in a package.  Its name has no dots, and therefore you cannot use  from .. import  statements inside it.  If you try to do so, you will get the \"relative-import in non-package\" error.   Scripts can't import relative   What you probably did is you tried to run  moduleX  or the like from the command line.  When you did this, its name was set to  __main__ , which means that relative imports within it will fail, because its name does not reveal that it is in a package. Note that this will also happen if you run Python from the same directory where a module is, and then try to import that module, because, as described above, Python will find the module in the current directory \"too early\" without realizing it is part of a package.   Also remember that when you run the interactive interpreter, the \"name\" of that interactive session is always  __main__ .  Thus  you cannot do relative imports directly from an interactive session .  Relative imports are only for use within module files.   Two solutions:     If you really do want to run  moduleX  directly, but you still want it to be considered part of a package, you can do  python -m package.subpackage1.moduleX .  The  -m  tells Python to load it as a module, not as the top-level script.     Or perhaps you don't actually want to  run   moduleX , you just want to run some other script, say  myfile.py , that  uses  functions inside  moduleX .  If that is the case, put  myfile.py   somewhere else  \u2013  not  inside the  package  directory \u2013 and run it.  If inside  myfile.py  you do things like  from package.moduleA import spam , it will work fine.       Notes     For either of these solutions, the package directory ( package  in your example) must be accessible from the Python module search path ( sys.path ).  If it is not, you will not be able to use anything in the package reliably at all.     Since Python 2.6, the module's \"name\" for package-resolution purposes is determined not just by its  __name__  attributes but also by the  __package__  attribute.  That's why I'm avoiding using the explicit symbol  __name__  to refer to the module's \"name\".  Since Python 2.6 a module's \"name\" is effectively  __package__ + '.' + __name__ , or just  __name__  if  __package__  is  None .)", "score": 2392}}
{"question": "What is the meaning of single and double underscore before an object name?", "tags": ["python", "oop", "naming-conventions", "identifier"], "link": "https://stackoverflow.com/questions/1301346/what-is-the-meaning-of-single-and-double-underscore-before-an-object-name", "answer_count": 19, "answers": {"id": 1301409, "body": "_foo : Only a convention. A way for the programmer to indicate that the variable is private (whatever that means in Python).     __foo : This has real meaning. The interpreter replaces this name with  _classname__foo  as a way to ensure that the name will not overlap with a similar name in another class.     __foo__ : Only a convention. A way for the Python system to use names that won't conflict with user names.       No other form of underscores have meaning in the Python world. Also, there's no difference between class, variable, global, etc in these conventions.", "score": 578}}
{"question": "What is the meaning of single and double underscore before an object name?", "tags": ["python", "oop", "naming-conventions", "identifier"], "link": "https://stackoverflow.com/questions/1301346/what-is-the-meaning-of-single-and-double-underscore-before-an-object-name", "answer_count": 19, "answers": {"id": 1301369, "body": "Single Underscore   In a class, names with a leading underscore indicate to other programmers that the attribute or method is intended to be be used inside that class. However, privacy is not  enforced  in any way. Using leading underscores for functions in a module indicates it should not be imported from somewhere else.   From the  PEP-8  style guide:     _single_leading_underscore : weak \"internal use\" indicator. E.g.  from M import *  does not import objects whose name starts with an underscore.     Double Underscore (Name Mangling)   From  the Python docs :     Any identifier of the form  __spam  (at least two leading underscores, at most one trailing underscore) is textually replaced with  _classname__spam , where  classname  is the current class name with leading underscore(s) stripped. This mangling is done without regard to the syntactic position of the identifier, so it can be used to define class-private instance and class variables, methods, variables stored in globals, and even variables stored in instances. private to this class on instances of other classes.     And a warning from the same page:     Name mangling is intended to give classes an easy way to define \u201cprivate\u201d instance variables and methods, without having to worry about instance variables defined by derived classes, or mucking with instance variables by code outside the class. Note that the mangling rules are designed mostly to avoid accidents;  it still is possible for a determined soul to access or modify a variable that is considered private.     Example   >>> class MyClass(): ...     def __init__(self): ...             self.__superprivate = \"Hello\" ...             self._semiprivate = \", world!\" ... >>> mc = MyClass() >>> print mc.__superprivate Traceback (most recent call last):   File \" \", line 1, in   AttributeError: myClass instance has no attribute '__superprivate' >>> print mc._semiprivate , world! >>> print mc.__dict__ {'_MyClass__superprivate': 'Hello', '_semiprivate': ', world!'}", "score": 1666}}
{"question": "fatal error: Python.h: No such file or directory", "tags": ["python", "gcc", "python-c-api"], "link": "https://stackoverflow.com/questions/21530577/fatal-error-python-h-no-such-file-or-directory", "answer_count": 37, "answers": {"id": 22077790, "body": "On Ubuntu, I was running Python 3 and I had to install    sudo apt-get install python3-dev    If you want to use a version of Python that is not linked to python3, install the associated python3.x-dev package.  For example:   sudo apt-get install python3.5-dev", "score": 575}}
{"question": "fatal error: Python.h: No such file or directory", "tags": ["python", "gcc", "python-c-api"], "link": "https://stackoverflow.com/questions/21530577/fatal-error-python-h-no-such-file-or-directory", "answer_count": 37, "answers": {"id": 21530768, "body": "Looks like you haven't properly installed the header files and static libraries for python dev.  Use your package manager to install them system-wide.   For  apt  ( Ubuntu, Debian... ):   sudo apt-get install python-dev   # for python2.x installs sudo apt-get install python3-dev  # for python3.x installs    For  yum  ( CentOS, RHEL... ):   sudo yum install python-devel    # for python2.x installs sudo yum install python3-devel   # for python3.x installs    For  dnf  ( Fedora... ):   sudo dnf install python2-devel  # for python2.x installs sudo dnf install python3-devel  # for python3.x installs    For  zypper  ( openSUSE... ):   sudo zypper in python-devel   # for python2.x installs sudo zypper in python3-devel  # for python3.x installs    For  apk  ( Alpine... ):   # This is a departure from the normal Alpine naming # scheme, which uses py2- and py3- prefixes sudo apk add python2-dev  # for python2.x installs sudo apk add python3-dev  # for python3.x installs    For  apt-cyg  ( Cygwin... ):   apt-cyg install python-devel   # for python2.x installs apt-cyg install python3-devel  # for python3.x installs    Important Note :  python3-dev/devel does not automatically cover all minor versions of python3.  E.g If you are using python 3.11 you may need to install  python3.11-dev  /  python3.11-devel .", "score": 3585}}
{"question": "Generate random integers between 0 and 9", "tags": ["python", "random", "integer"], "link": "https://stackoverflow.com/questions/3996904/generate-random-integers-between-0-and-9", "answer_count": 24, "answers": {"id": 16376904, "body": "Try  random.randint :   import random print(random.randint(0, 9))      Docs state:     random.randint(a, b)    Return a random integer  N  such that  a <= N <= b . Alias for  randrange(a, b+1) .", "score": 832}}
{"question": "Generate random integers between 0 and 9", "tags": ["python", "random", "integer"], "link": "https://stackoverflow.com/questions/3996904/generate-random-integers-between-0-and-9", "answer_count": 24, "answers": {"id": 3996930, "body": "Try  random.randrange :   from random import randrange print(randrange(10))", "score": 2643}}
{"question": "Proper way to declare custom exceptions in modern Python?", "tags": ["python", "python-3.x", "exception", "python-dataclasses"], "link": "https://stackoverflow.com/questions/1319615/proper-way-to-declare-custom-exceptions-in-modern-python", "answer_count": 17, "answers": {"id": 10270732, "body": "With modern Python Exceptions, you don't need to abuse  .message , or override  .__str__()  or  .__repr__()  or any of it. If all you want is an informative message when your exception is raised, do this:   class MyException(Exception):     pass  raise MyException(\"My hovercraft is full of eels\")    That will give a traceback ending with  MyException: My hovercraft is full of eels .   If you want more flexibility from the exception, you could pass a dictionary as the argument:   raise MyException({\"message\":\"My hovercraft is full of animals\", \"animal\":\"eels\"})    However, to get at those details in an  except  block is a bit more complicated. The details are stored in the  args  attribute, which is a list. You would need to do something like this:   try:     raise MyException({\"message\":\"My hovercraft is full of animals\", \"animal\":\"eels\"}) except MyException as e:     details = e.args[0]     print(details[\"animal\"])    It is still possible to pass in multiple items to the exception and access them via tuple indexes, but this is  highly discouraged  (and was even intended for deprecation a while back). If you do need more than a single piece of information and the above method is not sufficient for you, then you should subclass  Exception  as described in the  tutorial .   class MyError(Exception):     def __init__(self, message, animal):         self.message = message         self.animal = animal     def __str__(self):         return self.message", "score": 748}}
{"question": "Proper way to declare custom exceptions in modern Python?", "tags": ["python", "python-3.x", "exception", "python-dataclasses"], "link": "https://stackoverflow.com/questions/1319615/proper-way-to-declare-custom-exceptions-in-modern-python", "answer_count": 17, "answers": {"id": 1319675, "body": "Maybe I missed the question, but why not:   class MyException(Exception):     pass    To override something (or pass extra args), do this:   class ValidationError(Exception):     def __init__(self, message, errors):                     # Call the base class constructor with the parameters it needs         super().__init__(message)                      # Now for your custom code...         self.errors = errors    That way you could pass dict of error messages to the second param, and get to it later with  e.errors .   In Python 2, you have to use this slightly more complex form of  super() :   super(ValidationError, self).__init__(message)", "score": 2039}}
{"question": "How can I install packages using pip according to the requirements.txt file from a local directory?", "tags": ["python", "virtualenv", "pip"], "link": "https://stackoverflow.com/questions/7225900/how-can-i-install-packages-using-pip-according-to-the-requirements-txt-file-from", "answer_count": 18, "answers": {"id": 10429168, "body": "This works for me:   $ pip install -r requirements.txt --no-index --find-links file:///tmp/packages    --no-index  - Ignore package index (only look at  --find-links  URLs instead).   -f, --find-links   - If   is a URL or a path to an HTML file, then parse for links to archives. If   is a local path or a  file://  URL that's a directory, then look for archives in the directory listing.", "score": 1167}}
{"question": "How can I install packages using pip according to the requirements.txt file from a local directory?", "tags": ["python", "virtualenv", "pip"], "link": "https://stackoverflow.com/questions/7225900/how-can-i-install-packages-using-pip-according-to-the-requirements-txt-file-from", "answer_count": 18, "answers": {"id": 15593865, "body": "This works for everyone:   pip install -r /path/to/requirements.txt    Explanation:     -r, --requirement < filename >     Install from the given requirements file. This option can be used multiple times.", "score": 2571}}
{"question": "What&#39;s the canonical way to check for type in Python?", "tags": ["python", "types"], "link": "https://stackoverflow.com/questions/152580/whats-the-canonical-way-to-check-for-type-in-python", "answer_count": 19, "answers": {"id": 154156, "body": "The  most  Pythonic way to check the type of an object is... not to check it.   Since Python encourages  Duck Typing , you should just  try...except  to use the object's methods the way you want to use them.  So if your function is looking for a writable file object,  don't  check that it's a subclass of  file , just try to use its  .write()  method!   Of course, sometimes these nice abstractions break down and  isinstance(obj, cls)  is what you need.  But use sparingly.", "score": 273}}
{"question": "What&#39;s the canonical way to check for type in Python?", "tags": ["python", "types"], "link": "https://stackoverflow.com/questions/152580/whats-the-canonical-way-to-check-for-type-in-python", "answer_count": 19, "answers": {"id": 152596, "body": "Use  isinstance  to check if  o  is an instance of  str  or any subclass of  str :   if isinstance(o, str):    To check if the type of  o  is exactly  str ,  excluding subclasses of  str :   if type(o) is str:    See  Built-in Functions  in the Python Library Reference for relevant information.     Checking for strings in Python 2   For Python 2, this is a better way to check if  o  is a string:   if isinstance(o, basestring):    because this will also catch Unicode strings.  unicode  is not a subclass of  str ; both  str  and  unicode  are subclasses of  basestring . In Python 3,  basestring  no longer exists since there's  a strict separation  of strings ( str ) and binary data ( bytes ).   Alternatively,  isinstance  accepts a tuple of classes. This will return  True  if  o  is an instance of any subclass of any of  (str, unicode) :   if isinstance(o, (str, unicode)):", "score": 2332}}
{"question": "Meaning of @classmethod and @staticmethod for beginner", "tags": ["python", "oop", "static-methods", "class-method"], "link": "https://stackoverflow.com/questions/12179271/meaning-of-classmethod-and-staticmethod-for-beginner", "answer_count": 12, "answers": {"id": 14605349, "body": "Rostyslav Dzinko's answer is very appropriate. I thought I could highlight one other reason you should choose  @classmethod  over  @staticmethod  when you are creating an additional constructor.   In the  example , Rostyslav used the  @classmethod   from_string  as a Factory to create  Date  objects from otherwise unacceptable parameters. The same can be done with  @staticmethod  as is shown in the code below:   class Date:   def __init__(self, month, day, year):     self.month = month     self.day   = day     self.year  = year     def display(self):     return \"{0}-{1}-{2}\".format(self.month, self.day, self.year)     @staticmethod   def millenium(month, day):     return Date(month, day, 2000)  new_year = Date(1, 1, 2013)               # Creates a new Date object millenium_new_year = Date.millenium(1, 1) # also creates a Date object.   # Proof: new_year.display()           # \"1-1-2013\" millenium_new_year.display() # \"1-1-2000\"  isinstance(new_year, Date) # True isinstance(millenium_new_year, Date) # True    Thus both  new_year  and  millenium_new_year  are instances of the  Date  class.   But, if you observe closely, the Factory process is hard-coded to create  Date  objects no matter what. What this means is that even if the  Date  class is subclassed, the subclasses will still create plain  Date  objects (without any properties of the subclass). See that in the example below:   class DateTime(Date):   def display(self):       return \"{0}-{1}-{2} - 00:00:00PM\".format(self.month, self.day, self.year)   datetime1 = DateTime(10, 10, 1990) datetime2 = DateTime.millenium(10, 10)  isinstance(datetime1, DateTime) # True isinstance(datetime2, DateTime) # False  datetime1.display() # returns \"10-10-1990 - 00:00:00PM\" datetime2.display() # returns \"10-10-2000\" because it's not a DateTime object but a Date object. Check the implementation of the millenium method on the Date class for more details.    datetime2  is not an instance of  DateTime ? WTF? Well, that's because of the  @staticmethod  decorator used.   In most cases, this is undesired. If what you want is a Factory method that is aware of the class that called it, then  @classmethod  is what you need.   Rewriting  Date.millenium  as (that's the only part of the above code that changes):   @classmethod def millenium(cls, month, day):     return cls(month, day, 2000)    ensures that the  class  is not hard-coded but rather learnt.  cls  can be any subclass. The resulting  object  will rightly be an instance of  cls .  Let's test that out:   datetime1 = DateTime(10, 10, 1990) datetime2 = DateTime.millenium(10, 10)  isinstance(datetime1, DateTime) # True isinstance(datetime2, DateTime) # True   datetime1.display() # \"10-10-1990 - 00:00:00PM\" datetime2.display() # \"10-10-2000 - 00:00:00PM\"    The reason is, as you know by now, that  @classmethod  was used instead of  @staticmethod", "score": 1026}}
{"question": "Meaning of @classmethod and @staticmethod for beginner", "tags": ["python", "oop", "static-methods", "class-method"], "link": "https://stackoverflow.com/questions/12179271/meaning-of-classmethod-and-staticmethod-for-beginner", "answer_count": 12, "answers": {"id": 12179752, "body": "Though  classmethod  and  staticmethod  are quite similar, there's a slight difference in usage for both entities:  classmethod  must have a reference to a class object as the first parameter, whereas  staticmethod  can have no parameters at all.   Example   class Date(object):          def __init__(self, day=0, month=0, year=0):         self.day = day         self.month = month         self.year = year      @classmethod     def from_string(cls, date_as_string):         day, month, year = map(int, date_as_string.split('-'))         date1 = cls(day, month, year)         return date1      @staticmethod     def is_date_valid(date_as_string):         day, month, year = map(int, date_as_string.split('-'))         return day <= 31 and month <= 12 and year <= 3999  date2 = Date.from_string('11-09-2012') is_date = Date.is_date_valid('11-09-2012')    Explanation   Let's assume an example of a class, dealing with date information (this will be our boilerplate):   class Date(object):          def __init__(self, day=0, month=0, year=0):         self.day = day         self.month = month         self.year = year    This class obviously could be used to store information about certain dates (without timezone information; let's assume all dates are presented in UTC).   Here we have  __init__ , a typical initializer of Python class instances, which receives arguments as a typical instance method, having the first non-optional argument ( self ) that holds a reference to a newly created instance.   Class Method   We have some tasks that can be nicely done using  classmethod s.   Let's assume that we want to create a lot of  Date  class instances having date information coming from an outer source encoded as a string with format 'dd-mm-yyyy'. Suppose we have to do this in different places in the source code of our project.   So what we must do here is:     Parse a string to receive day, month and year as three integer variables or a 3-item tuple consisting of that variable.   Instantiate  Date  by passing those values to the initialization call.     This will look like:   day, month, year = map(int, string_date.split('-')) date1 = Date(day, month, year)    For this purpose, C++ can implement such a feature with overloading, but Python lacks this overloading. Instead, we can use  classmethod . Let's create another  constructor .       @classmethod     def from_string(cls, date_as_string):         day, month, year = map(int, date_as_string.split('-'))         date1 = cls(day, month, year)         return date1  date2 = Date.from_string('11-09-2012')    Let's look more carefully at the above implementation, and review what advantages we have here:     We've implemented date string parsing in one place and it's reusable now.   Encapsulation works fine here (if you think that you could implement string parsing as a single function elsewhere, this solution fits the OOP paradigm far better).   cls  is the  class itself , not an instance of the class. It's pretty cool because if we inherit our  Date  class, all children will have  from_string  defined also.     Static method   What about  staticmethod ? It's pretty similar to  classmethod  but doesn't take any obligatory parameters (like a class method or instance method does).   Let's look at the next use case.   We have a date string that we want to validate somehow. This task is also logically bound to the  Date  class we've used so far, but doesn't require instantiation of it.   Here is where  staticmethod  can be useful. Let's look at the next piece of code:       @staticmethod     def is_date_valid(date_as_string):         day, month, year = map(int, date_as_string.split('-'))         return day <= 31 and month <= 12 and year <= 3999  # usage: is_date = Date.is_date_valid('11-09-2012')    So, as we can see from usage of  staticmethod , we don't have any access to what the class is---it's basically just a function,  called syntactically like a method, but without access to the object and its internals (fields and other methods), which  classmethod  does have.", "score": 3038}}
{"question": "How do I get the row count of a Pandas DataFrame?", "tags": ["python", "pandas", "dataframe"], "link": "https://stackoverflow.com/questions/15943769/how-do-i-get-the-row-count-of-a-pandas-dataframe", "answer_count": 20, "answers": {"id": 35523946, "body": "Suppose  df  is your dataframe then:   count_row = df.shape[0]  # Gives number of rows count_col = df.shape[1]  # Gives number of columns    Or, more succinctly,   r, c = df.shape", "score": 496}}
{"question": "How do I get the row count of a Pandas DataFrame?", "tags": ["python", "pandas", "dataframe"], "link": "https://stackoverflow.com/questions/15943769/how-do-i-get-the-row-count-of-a-pandas-dataframe", "answer_count": 20, "answers": {"id": 15943975, "body": "For a dataframe  df , one can use any of the following:     len(df.index)   df.shape[0]   df[df.columns[0]].count()  (==  number of non-NaN values  in first column)         Code to reproduce the plot:   import numpy as np import pandas as pd import perfplot  perfplot.save(     \"out.png\",     setup=lambda n: pd.DataFrame(np.arange(n * 3).reshape(n, 3)),     n_range=[2**k for k in range(25)],     kernels=[         lambda df: len(df.index),         lambda df: df.shape[0],         lambda df: df[df.columns[0]].count(),     ],     labels=[\"len(df.index)\", \"df.shape[0]\", \"df[df.columns[0]].count()\"],     xlabel=\"Number of rows\", )", "score": 2979}}
{"question": "How do I check if a string represents a number (float or int)?", "tags": ["python", "casting", "floating-point", "type-conversion", "integer"], "link": "https://stackoverflow.com/questions/354038/how-do-i-check-if-a-string-represents-a-number-float-or-int", "answer_count": 41, "answers": {"id": 354130, "body": "Which, not only is ugly and slow     I'd dispute both.   A regex or other string parsing method would be uglier and slower.     I'm not sure that anything much could be faster than the above.  It calls the function and returns.  Try/Catch doesn't introduce much overhead because the most common exception is caught without an extensive search of stack frames.   The issue is that any numeric conversion function has two kinds of results     A number, if the number is valid   A status code (e.g., via errno) or exception to show that no valid number could be parsed.     C (as an example) hacks around this a number of ways.  Python lays it out clearly and explicitly.   I think your code for doing this is perfect.", "score": 787}}
{"question": "How do I check if a string represents a number (float or int)?", "tags": ["python", "casting", "floating-point", "type-conversion", "integer"], "link": "https://stackoverflow.com/questions/354038/how-do-i-check-if-a-string-represents-a-number-float-or-int", "answer_count": 41, "answers": {"id": 354073, "body": "For non-negative (unsigned) integers only, use  isdigit() :   >>> a = \"03523\" >>> a.isdigit() True >>> b = \"963spam\" >>> b.isdigit() False      Documentation for  isdigit() :  Python2 ,  Python3   For Python 2 Unicode strings:  isnumeric() .", "score": 1798}}
{"question": "Getting the class name of an instance", "tags": ["python", "introspection", "instanceof", "python-datamodel"], "link": "https://stackoverflow.com/questions/510972/getting-the-class-name-of-an-instance", "answer_count": 12, "answers": {"id": 511060, "body": "Do you want the name of the class as a string?   instance.__class__.__name__", "score": 572}}
{"question": "Getting the class name of an instance", "tags": ["python", "introspection", "instanceof", "python-datamodel"], "link": "https://stackoverflow.com/questions/510972/getting-the-class-name-of-an-instance", "answer_count": 12, "answers": {"id": 511059, "body": "Have you tried the  __name__  attribute  of the class? ie  type(x).__name__  will give you the name of the class, which I think is what you want.   >>> import itertools >>> x = itertools.count(0) >>> type(x).__name__ 'count'    If you're still using Python 2, note that the above method works with  new-style classes  only (in Python 3+ all classes are \"new-style\" classes). Your code might use some old-style classes. The following works for both:   x.__class__.__name__", "score": 2620}}
{"question": "How to read a file line-by-line into a list?", "tags": ["python", "string", "file", "readlines"], "link": "https://stackoverflow.com/questions/3277503/how-to-read-a-file-line-by-line-into-a-list", "answer_count": 28, "answers": {"id": 3277515, "body": "See  Input and Ouput :   with open('filename') as f:     lines = f.readlines()    or with stripping the newline character:   with open('filename') as f:     lines = [line.rstrip('\\n') for line in f]", "score": 1238}}
{"question": "How to read a file line-by-line into a list?", "tags": ["python", "string", "file", "readlines"], "link": "https://stackoverflow.com/questions/3277503/how-to-read-a-file-line-by-line-into-a-list", "answer_count": 28, "answers": {"id": 3277516, "body": "This code will read the entire file into memory and remove all whitespace characters (newlines and spaces) from the end of each line:   with open(filename) as file:     lines = [line.rstrip() for line in file]    If you're working with a large file, then you should instead read and process it line-by-line:   with open(filename) as file:     for line in file:         print(line.rstrip())    In Python 3.8 and up you can use a while loop with the  walrus operator  like so:   with open(filename) as file:     while line := file.readline():         print(line.rstrip())    Depending on what you plan to do with your file and how it was encoded, you may also want to manually set the  access mode  and character encoding:   with open(filename, 'r', encoding='UTF-8') as file:     while line := file.readline():         print(line.rstrip())", "score": 3040}}
{"question": "How to prettyprint a JSON file?", "tags": ["python", "json", "formatting", "pretty-print"], "link": "https://stackoverflow.com/questions/12943819/how-to-prettyprint-a-json-file", "answer_count": 15, "answers": {"id": 32228333, "body": "You can do this on the command line:   python3 -m json.tool some.json    (as already mentioned in the commentaries to the question, thanks to @Kai Petzke for the python3 suggestion).   Actually python is not my favourite tool as far as json processing on the command line is concerned. For simple pretty printing is ok, but if you want to manipulate the json it can become overcomplicated. You'd soon need to write a separate script-file, you could end up with maps whose keys are u\"some-key\" (python unicode), which makes selecting fields more difficult and doesn't really go in the direction of pretty-printing.   You can also use  jq :   jq . some.json    and you get colors as a bonus (and way easier extendability).   Addendum: There is some confusion in the comments about using jq to process large JSON files on the one hand, and having a very large jq program on the other.  For pretty-printing a file consisting of a single large JSON entity, the practical limitation is RAM.  For pretty-printing a 2GB file consisting of a single array of real-world data, the \"maximum resident set size\" required for pretty-printing was 5GB (whether using jq 1.5 or 1.6). Note also that jq can be used from within python after  pip install jq .", "score": 494}}
{"question": "How to prettyprint a JSON file?", "tags": ["python", "json", "formatting", "pretty-print"], "link": "https://stackoverflow.com/questions/12943819/how-to-prettyprint-a-json-file", "answer_count": 15, "answers": {"id": 12944035, "body": "Use the  indent=  parameter of  json.dump()  or  json.dumps()  to specify how many spaces to indent by:   >>> import json >>> your_json = '[\"foo\", {\"bar\": [\"baz\", null, 1.0, 2]}]' >>> parsed = json.loads(your_json) >>> print(json.dumps(parsed, indent=4)) [     \"foo\",     {         \"bar\": [             \"baz\",             null,             1.0,             2         ]     } ]    To parse a file, use  json.load() :   with open('filename.txt', 'r') as handle:     parsed = json.load(handle)", "score": 3062}}
{"question": "How do I split the definition of a long string over multiple lines?", "tags": ["python", "string", "multiline", "multilinestring"], "link": "https://stackoverflow.com/questions/10660435/how-do-i-split-the-definition-of-a-long-string-over-multiple-lines", "answer_count": 31, "answers": {"id": 10660477, "body": "If you don't want a multiline string, but just have a long single line string, you can use parentheses. Just make sure you don't include commas between the string segments (then it will be a tuple).   query = ('SELECT   action.descr as \"action\", '          'role.id as role_id,'          'role.descr as role'          ' FROM '          'public.role_action_def,'          'public.role,'          'public.record_def, '          'public.action'          ' WHERE role.id = role_action_def.role_id AND'          ' record_def.id = role_action_def.def_id AND'          ' action.id = role_action_def.action_id AND'          ' role_action_def.account_id = '+account_id+' AND'          ' record_def.account_id='+account_id+' AND'          ' def_id='+def_id)    In a SQL statement like what you're constructing, multiline strings would also be fine. But if the extra white space a multiline string would contain would be a problem, then this would be a good way to achieve what you want.   As noted in the comments,  concatenating SQL queries in this way is a SQL injection security risk waiting to happen , so use your database's parameterized queries feature to prevent this. However, I'm leaving the answer as-is otherwise as it directly answers the question asked.", "score": 345}}
{"question": "How do I split the definition of a long string over multiple lines?", "tags": ["python", "string", "multiline", "multilinestring"], "link": "https://stackoverflow.com/questions/10660435/how-do-i-split-the-definition-of-a-long-string-over-multiple-lines", "answer_count": 31, "answers": {"id": 10660443, "body": "Are you talking about multi-line strings? Easy, use triple quotes to start and end them.   s = \"\"\" this is a very         long string if I had the         energy to type more and more ...\"\"\"    You can use single quotes too (3 of them of course at start and end) and treat the resulting string  s  just like any other string.   NOTE : Just as with any string, anything between the starting and ending quotes becomes part of the string, so this example has a leading blank (as pointed out by @root45). This string will also contain both blanks and newlines.   I.e.,:   ' this is a very\\n        long string if I had the\\n        energy to type more and more ...'    Finally, one can also construct long lines in Python like this:    s = (\"this is a very\"       \"long string too\"       \"for sure ...\"      )    which will  not  include any extra blanks or newlines (this is a deliberate example showing what the effect of skipping blanks will result in):   'this is a verylong string toofor sure ...'    No commas required, simply place the strings to be joined together into a pair of parenthesis and be sure to account for any needed blanks and newlines.", "score": 3246}}
{"question": "Installing specific package version with pip", "tags": ["python", "mysql", "pip", "pypi", "mysql-python"], "link": "https://stackoverflow.com/questions/5226311/installing-specific-package-version-with-pip", "answer_count": 12, "answers": {"id": 36399566, "body": "One way, as suggested in  this post , is to mention version in  pip  as:   pip install -Iv MySQL_python==1.2.2    i.e. Use  ==  and mention the version number to install only that version.  -I, --ignore-installed  ignores already installed packages.", "score": 211}}
{"question": "Installing specific package version with pip", "tags": ["python", "mysql", "pip", "pypi", "mysql-python"], "link": "https://stackoverflow.com/questions/5226311/installing-specific-package-version-with-pip", "answer_count": 12, "answers": {"id": 33812968, "body": "You can even use a version range with  pip install  command. Something like this:   pip install 'stevedore>=1.3.0,<1.4.0'    And if the package is already installed and you want to downgrade it add  --force-reinstall  like this:   pip install 'stevedore>=1.3.0,<1.4.0' --force-reinstall", "score": 767}}
{"question": "Installing specific package version with pip", "tags": ["python", "mysql", "pip", "pypi", "mysql-python"], "link": "https://stackoverflow.com/questions/5226311/installing-specific-package-version-with-pip", "answer_count": 12, "answers": {"id": 5226504, "body": "TL;DR :   Update as of 2022-12-28 :   pip install --force-reinstall -v   For example:  pip install --force-reinstall -v \"MySQL_python==1.2.2\"   What these options mean:     --force-reinstall  is an option to reinstall all packages even if they are already up-to-date.   -v  is for verbose. You can combine for even more verbosity (i.e.  -vv ) up to 3 times (e.g.  --force-reinstall -vvv ).     Thanks to  @Peter  for highlighting this (and it seems that the context of the question has broadened given the time when the question was first asked!),  the documentation for Python  discusses a caveat with using  -I , in that it can break your installation if it was installed with a different package manager or if if your package is/was a different version.     Original answer:     pip install -Iv  (i.e.  pip install -Iv MySQL_python==1.2.2 )       What these options mean:     -I  stands for  --ignore-installed  which will ignore the installed packages, overwriting them.   -v  is for verbose. You can combine for even more verbosity (i.e.  -vv ) up to 3 times (e.g.  -Ivvv ).     For more information, see  pip install --help   First, I see two issues with what you're trying to do. Since you already have an installed version, you should either uninstall the current existing driver or use  pip install -I MySQL_python==1.2.2   However, you'll soon find out that this doesn't work. If you look at pip's installation log, or if you do a  pip install -Iv MySQL_python==1.2.2  you'll find that the PyPI URL link does not work for MySQL_python v1.2.2. You can verify this here:  http://pypi.python.org/pypi/MySQL-python/1.2.2   The download link 404s and the fallback URL links are re-directing infinitely due to sourceforge.net's recent upgrade and PyPI's stale URL.   So to properly install the driver, you can follow these steps:   pip uninstall MySQL_python pip install -Iv http://sourceforge.net/projects/mysql-python/files/mysql-python/1.2.2/MySQL-python-1.2.2.tar.gz/download", "score": 1591}}
{"question": "Static methods in Python?", "tags": ["python", "static-methods"], "link": "https://stackoverflow.com/questions/735975/static-methods-in-python", "answer_count": 12, "answers": {"id": 735976, "body": "Yes, check out the  staticmethod  decorator:   >>> class C: ...     @staticmethod ...     def hello(): ...             print \"Hello World\" ... >>> C.hello() Hello World", "score": 93}}
{"question": "Static methods in Python?", "tags": ["python", "static-methods"], "link": "https://stackoverflow.com/questions/735975/static-methods-in-python", "answer_count": 12, "answers": {"id": 10206355, "body": "I think that  Steven is actually right . To answer the original question, then, in order to set up a class method, simply assume that the first argument is not going to be a calling instance, and then make sure that you only call the method from the class.   (Note that this answer refers to Python 3.x. In Python 2.x you'll get a  TypeError  for calling the method on the class itself.)   For example:   class Dog:     count = 0 # this is a class variable     dogs = [] # this is a class variable      def __init__(self, name):         self.name = name #self.name is an instance variable         Dog.count += 1         Dog.dogs.append(name)      def bark(self, n): # this is an instance method         print(\"{} says: {}\".format(self.name, \"woof! \" * n))      def rollCall(n): #this is implicitly a class method (see comments below)         print(\"There are {} dogs.\".format(Dog.count))         if n >= len(Dog.dogs) or n < 0:             print(\"They are:\")             for dog in Dog.dogs:                 print(\"  {}\".format(dog))         else:             print(\"The dog indexed at {} is {}.\".format(n, Dog.dogs[n]))  fido = Dog(\"Fido\") fido.bark(3) Dog.rollCall(-1) rex = Dog(\"Rex\") Dog.rollCall(0)    In this code, the \"rollCall\" method assumes that the first argument is not an instance (as it would be if it were called by an instance instead of a class). As long as \"rollCall\" is called from the class rather than an instance, the code will work fine. If we try to call \"rollCall\" from an instance, e.g.:   rex.rollCall(-1)    however, it would cause an exception to be raised because it would send two arguments: itself and -1, and \"rollCall\" is only defined to accept one argument.   Incidentally, rex.rollCall() would send the correct number of arguments, but would also cause an exception to be raised because now n would be representing a Dog instance (i.e., rex) when the function expects n to be numerical.   This is where the decoration comes in: If we precede the \"rollCall\" method with   @staticmethod    then, by explicitly stating that the method is static, we can even call it from an instance. Now,    rex.rollCall(-1)    would work. The insertion of @staticmethod before a method definition, then, stops an instance from sending itself as an argument.   You can verify this by trying the following code with and without the @staticmethod line commented out.   class Dog:     count = 0 # this is a class variable     dogs = [] # this is a class variable      def __init__(self, name):         self.name = name #self.name is an instance variable         Dog.count += 1         Dog.dogs.append(name)      def bark(self, n): # this is an instance method         print(\"{} says: {}\".format(self.name, \"woof! \" * n))      @staticmethod     def rollCall(n):         print(\"There are {} dogs.\".format(Dog.count))         if n >= len(Dog.dogs) or n < 0:             print(\"They are:\")             for dog in Dog.dogs:                 print(\"  {}\".format(dog))         else:             print(\"The dog indexed at {} is {}.\".format(n, Dog.dogs[n]))   fido = Dog(\"Fido\") fido.bark(3) Dog.rollCall(-1) rex = Dog(\"Rex\") Dog.rollCall(0) rex.rollCall(-1)", "score": 259}}
{"question": "Static methods in Python?", "tags": ["python", "static-methods"], "link": "https://stackoverflow.com/questions/735975/static-methods-in-python", "answer_count": 12, "answers": {"id": 735978, "body": "Yep, using the  staticmethod  decorator:   class MyClass(object):     @staticmethod     def the_static_method(x):         print(x)  MyClass.the_static_method(2)  # outputs 2    Note that some code might use the old method of defining a static method, using  staticmethod  as a function rather than a decorator. This should only be used if you have to support ancient versions of Python (2.2 and 2.3):   class MyClass(object):     def the_static_method(x):         print(x)     the_static_method = staticmethod(the_static_method)  MyClass.the_static_method(2)  # outputs 2    This is entirely identical to the first example (using  @staticmethod ), just not using the nice decorator syntax.   Finally, use  staticmethod  sparingly! There are very few situations where static-methods are necessary in Python, and I've seen them used many times where a separate \"top-level\" function would have been clearer.     The following is verbatim from the documentation: :     A static method does not receive an implicit first argument. To declare a static method, use this idiom:   class C:     @staticmethod     def f(arg1, arg2, ...): ...    The @staticmethod form is a function  decorator  \u2013 see the description of function definitions in  Function definitions  for details.   It can be called either on the class (such as  C.f() ) or on an instance (such as  C().f() ). The instance is ignored except for its class.   Static methods in Python are similar to those found in Java or C++. For a more advanced concept, see  classmethod() .   For more information on static methods, consult the documentation on the standard type hierarchy in  The standard type hierarchy .   New in version 2.2.   Changed in version 2.4: Function decorator syntax added.", "score": 2327}}
{"question": "How can I remove a trailing newline?", "tags": ["python", "newline", "trailing"], "link": "https://stackoverflow.com/questions/275018/how-can-i-remove-a-trailing-newline", "answer_count": 28, "answers": {"id": 275401, "body": "The canonical way to strip end-of-line (EOL) characters is to use the string rstrip() method removing any trailing \\r or \\n.  Here are examples for Mac, Windows, and Unix EOL characters.   >>> 'Mac EOL\\r'.rstrip('\\r\\n') 'Mac EOL' >>> 'Windows EOL\\r\\n'.rstrip('\\r\\n') 'Windows EOL' >>> 'Unix EOL\\n'.rstrip('\\r\\n') 'Unix EOL'    Using '\\r\\n' as the parameter to rstrip means that it will strip out any trailing combination of '\\r' or '\\n'.  That's why it works in all three cases above.   This nuance matters in rare cases.  For example, I once had to process a text file which contained an HL7 message.  The HL7 standard requires a trailing '\\r' as its EOL character.  The Windows machine on which I was using this message had appended its own '\\r\\n' EOL character.  Therefore, the end of each line looked like '\\r\\r\\n'.  Using rstrip('\\r\\n') would have taken off the entire '\\r\\r\\n' which is not what I wanted.  In that case, I simply sliced off the last two characters instead.   Note that unlike Perl's  chomp  function, this will strip all specified characters at the end of the string, not just one:   >>> \"Hello\\n\\n\\n\".rstrip(\"\\n\") \"Hello\"", "score": 175}}
{"question": "How can I remove a trailing newline?", "tags": ["python", "newline", "trailing"], "link": "https://stackoverflow.com/questions/275018/how-can-i-remove-a-trailing-newline", "answer_count": 28, "answers": {"id": 275659, "body": "And I would say the \"pythonic\" way to get lines without trailing newline characters is splitlines().   >>> text = \"line 1\\nline 2\\r\\nline 3\\nline 4\" >>> text.splitlines() ['line 1', 'line 2', 'line 3', 'line 4']", "score": 186}}
{"question": "How can I remove a trailing newline?", "tags": ["python", "newline", "trailing"], "link": "https://stackoverflow.com/questions/275018/how-can-i-remove-a-trailing-newline", "answer_count": 28, "answers": {"id": 275025, "body": "Try the method  rstrip()  (see doc  Python 2  and  Python 3 )   >>> 'test string\\n'.rstrip() 'test string'    Python's  rstrip()  method strips  all  kinds of trailing whitespace by default, not just one newline as Perl does with  chomp .   >>> 'test string \\n \\r\\n\\n\\r \\n\\n'.rstrip() 'test string'    To strip only newlines:   >>> 'test string \\n \\r\\n\\n\\r \\n\\n'.rstrip('\\n') 'test string \\n \\r\\n\\n\\r '    In addition to  rstrip() , there are also the methods  strip()  and  lstrip() . Here is an example with the three of them:   >>> s = \"   \\n\\r\\n  \\n  abc   def \\n\\r\\n  \\n  \" >>> s.strip() 'abc   def' >>> s.lstrip() 'abc   def \\n\\r\\n  \\n  ' >>> s.rstrip() '   \\n\\r\\n  \\n  abc   def'", "score": 2328}}
{"question": "Is there a way to run Python on Android?", "tags": ["android", "python", "jython", "ase", "android-scripting"], "link": "https://stackoverflow.com/questions/101754/is-there-a-way-to-run-python-on-android", "answer_count": 23, "answers": {"id": 973765, "body": "Yes! :  Android Scripting Environment   An example  via Matt Cutts  via SL4A -- \"here\u2019s a barcode scanner written in six lines of Python code:   import android droid = android.Android() code = droid.scanBarcode() isbn = int(code['result']['SCAN_RESULT']) url = \"http://books.google.com?q=%d\" % isbn droid.startActivity('android.intent.action.VIEW', url)", "score": 231}}
{"question": "Is there a way to run Python on Android?", "tags": ["android", "python", "jython", "ase", "android-scripting"], "link": "https://stackoverflow.com/questions/101754/is-there-a-way-to-run-python-on-android", "answer_count": 23, "answers": {"id": 973786, "body": "There is also the new  Android Scripting Environment  (ASE/SL4A) project. It looks awesome, and it has some integration with native Android components.    Note: no longer under \"active development\", but some forks may be.", "score": 366}}
{"question": "Is there a way to run Python on Android?", "tags": ["android", "python", "jython", "ase", "android-scripting"], "link": "https://stackoverflow.com/questions/101754/is-there-a-way-to-run-python-on-android", "answer_count": 23, "answers": {"id": 8189603, "body": "One way is to use  Kivy :     Open source Python library for rapid development of applications   that make use of innovative user interfaces, such as multi-touch apps.         Kivy runs on Linux, Windows, OS X, Android and iOS. You can run the same [python] code on all supported platforms.     Kivy Showcase app", "score": 1190}}
{"question": "How do I append to a file?", "tags": ["python", "file", "append"], "link": "https://stackoverflow.com/questions/4706499/how-do-i-append-to-a-file", "answer_count": 12, "answers": {"id": 26833243, "body": "I always do this,   f = open('filename.txt', 'a') f.write(\"stuff\") f.close()    It's simple, but very useful.", "score": 59}}
{"question": "How do I append to a file?", "tags": ["python", "file", "append"], "link": "https://stackoverflow.com/questions/4706499/how-do-i-append-to-a-file", "answer_count": 12, "answers": {"id": 4706519, "body": "You need to open the file in append mode, by setting \"a\" or \"ab\" as the mode. See  open() .   When you open with \"a\" mode, the write position will  always  be at the end of the file (an append). You can open with \"a+\" to allow reading, seek backwards and read (but all writes will still be at the end of the file!).   Example:   >>> with open('test1','wb') as f:         f.write('test') >>> with open('test1','ab') as f:         f.write('koko') >>> with open('test1','rb') as f:         f.read() 'testkoko'    Note : Using 'a' is not the same as opening with 'w' and seeking to the end of the file - consider what might happen if another program opened the file and started writing between the seek and the write. On some operating systems, opening the file with 'a' guarantees that all your following writes will be appended atomically to the end of the file (even as the file grows by other writes).     A few more details about how the \"a\" mode operates ( tested on Linux only ). Even if you seek back, every write will append to the end of the file:   >>> f = open('test','a+') # Not using 'with' just to simplify the example REPL session >>> f.write('hi') >>> f.seek(0) >>> f.read() 'hi' >>> f.seek(0) >>> f.write('bye') # Will still append despite the seek(0)! >>> f.seek(0) >>> f.read() 'hibye'    In fact, the  fopen   manpage  states:     Opening a file in append mode (a as the first character of mode)   causes all subsequent write operations to this stream to occur at   end-of-file, as if preceded the call:   fseek(stream, 0, SEEK_END);        Old simplified answer (not using  with ):   Example: ( in a real program  use  with  to close the file  - see  the documentation )   >>> open(\"test\",\"wb\").write(\"test\") >>> open(\"test\",\"a+b\").write(\"koko\") >>> open(\"test\",\"rb\").read() 'testkoko'", "score": 273}}
{"question": "How do I append to a file?", "tags": ["python", "file", "append"], "link": "https://stackoverflow.com/questions/4706499/how-do-i-append-to-a-file", "answer_count": 12, "answers": {"id": 4706520, "body": "Set the mode in  open()  to  \"a\"  (append) instead of  \"w\"  (write):   with open(\"test.txt\", \"a\") as myfile:     myfile.write(\"appended text\")    The  documentation  lists all the available modes.", "score": 3166}}
{"question": "Why is it string.join(list) instead of list.join(string)?", "tags": ["python", "string", "list"], "link": "https://stackoverflow.com/questions/493819/why-is-it-string-joinlist-instead-of-list-joinstring", "answer_count": 10, "answers": {"id": 493884, "body": "I agree that it's counterintuitive at first, but there's a good reason. Join can't be a method of a list because:     it must work for different iterables too (tuples, generators, etc.)    it must have different behavior between different types of strings.     There are actually two join methods (Python 3.0):   >>> b\"\".join   >>> \"\".join      If join was a method of a list, then it would have to inspect its arguments to decide which one of them to call. And you can't join byte and str together, so the way they have it now makes sense.", "score": 83}}
{"question": "Why is it string.join(list) instead of list.join(string)?", "tags": ["python", "string", "list"], "link": "https://stackoverflow.com/questions/493819/why-is-it-string-joinlist-instead-of-list-joinstring", "answer_count": 10, "answers": {"id": 12662361, "body": "This was discussed in the  String methods... finally  thread in the Python-Dev achive, and was accepted by Guido. This thread began in Jun 1999, and  str.join  was included in Python 1.6 which was released in Sep 2000 (and supported Unicode). Python 2.0 (supported  str  methods including  join ) was released in Oct 2000.     There were four options proposed in this thread:    separator.join(items)   items.join(separator)   items.reduce(separator)   join  as a built-in function       Guido wanted to support not only  list s and  tuple s, but all sequences/iterables.   items.reduce(separator)  is difficult for newcomers.   items.join(separator)  introduces unexpected dependency from sequences to str/unicode.   join()  as a free-standing built-in function would support only specific data types. So using a built-in namespace is not good. If  join()  were to support many data types, creating an optimized implementation would be difficult: if implemented using the  __add__  method then it would be O(n\u00b2).   The separator string ( separator ) should not be omitted. Explicit is better than implicit.     Here are some additional thoughts (my own, and my friend's):     Unicode support was coming, but it was not final. At that time UTF-8 was the most likely about to replace UCS-2/-4. To calculate total buffer length for UTF-8 strings, the method needs to know the character encoding.   At that time, Python had already decided on a common sequence interface rule where a user could create a sequence-like (iterable) class. But Python didn't support extending built-in types until 2.2. At that time it was difficult to provide basic  iterable  class (which is mentioned in another comment).     Guido's decision is recorded in a  historical mail , deciding on  separator.join(items) :     Funny, but it does seem right!  Barry, go for it...    --Guido van Rossum", "score": 474}}
{"question": "Why is it string.join(list) instead of list.join(string)?", "tags": ["python", "string", "list"], "link": "https://stackoverflow.com/questions/493819/why-is-it-string-joinlist-instead-of-list-joinstring", "answer_count": 10, "answers": {"id": 493842, "body": "It's because any iterable can be joined (e.g, list, tuple, dict, set), but its contents and the \"joiner\"  must be  strings.   For example:   '_'.join(['welcome', 'to', 'stack', 'overflow']) '_'.join(('welcome', 'to', 'stack', 'overflow'))    'welcome_to_stack_overflow'    Using something other than strings will raise the following error:     TypeError: sequence item 0: expected str instance, int found", "score": 1466}}
{"question": "How do I measure elapsed time in Python?", "tags": ["python", "performance", "measure", "timeit"], "link": "https://stackoverflow.com/questions/7370801/how-do-i-measure-elapsed-time-in-python", "answer_count": 41, "answers": {"id": 21455138, "body": "Python 3 only:   Since  time.clock()   is deprecated as of Python 3.3 , you will want to use  time.perf_counter()  for system-wide timing, or  time.process_time()  for process-wide timing, just the way you used to use  time.clock() :   import time  t = time.process_time() #do some stuff elapsed_time = time.process_time() - t    The new function  process_time  will not include time elapsed during sleep.", "score": 258}}
{"question": "How do I measure elapsed time in Python?", "tags": ["python", "performance", "measure", "timeit"], "link": "https://stackoverflow.com/questions/7370801/how-do-i-measure-elapsed-time-in-python", "answer_count": 41, "answers": {"id": 25823885, "body": "Use  timeit.default_timer  instead of  timeit.timeit . The former provides the best clock available on your platform and version of Python automatically:   from timeit import default_timer as timer  start = timer() # ... end = timer() print(end - start) # Time in seconds, e.g. 5.38091952400282    timeit.default_timer  is assigned to time.time() or time.clock() depending on OS. On Python 3.3+  default_timer  is  time.perf_counter()  on all platforms. See  Python - time.clock() vs. time.time() - accuracy?   See also:     Optimizing code   How to optimize for speed", "score": 1213}}
{"question": "How do I measure elapsed time in Python?", "tags": ["python", "performance", "measure", "timeit"], "link": "https://stackoverflow.com/questions/7370801/how-do-i-measure-elapsed-time-in-python", "answer_count": 41, "answers": {"id": 7370824, "body": "Use  time.time()  to measure the elapsed wall-clock time between two points:   import time  start = time.time() print(\"hello\") end = time.time() print(end - start)    This gives the execution time in seconds.     Another option since Python 3.3 might be to use  perf_counter  or  process_time , depending on your requirements. Before 3.3 it was recommended to use  time.clock  (thanks  Amber ). However, it is currently deprecated:     On Unix, return the current processor time as a floating point number expressed in seconds. The precision, and in fact the very definition of the meaning of \u201cprocessor time\u201d, depends on that of the C function of the same name.   On Windows, this function returns wall-clock seconds elapsed since the first call to this function, as a floating point number, based on the Win32 function  QueryPerformanceCounter() . The resolution is typically better than one microsecond.   Deprecated since version 3.3 : The behaviour of this function depends on the platform:  use  perf_counter()  or  process_time()  instead , depending on your requirements, to have a well defined behaviour.", "score": 2605}}
{"question": "Why is reading lines from stdin much slower in C++ than Python?", "tags": ["python", "c++", "benchmarking", "iostream", "getline"], "link": "https://stackoverflow.com/questions/9371238/why-is-reading-lines-from-stdin-much-slower-in-c-than-python", "answer_count": 11, "answers": {"id": 43825662, "body": "I'm a few years behind here, but:   In 'Edit 4/5/6' of the original post, you are using the construction:   $ /usr/bin/time cat big_file | program_to_benchmark    This is wrong in a couple of different ways:     You're actually timing the execution of  cat , not your benchmark.  The 'user' and 'sys' CPU usage displayed by  time  are those of  cat , not your benchmarked program.  Even worse, the 'real' time is also not necessarily accurate. Depending on the implementation of  cat  and of pipelines in your local OS, it is possible that  cat  writes a final giant buffer and exits long before the reader process finishes its work.   Use of  cat  is unnecessary and in fact counterproductive; you're adding moving parts.  If you were on a sufficiently old system (i.e. with a single CPU and -- in certain generations of computers -- I/O faster than CPU) -- the mere fact that  cat  was running could substantially color the results.  You are also subject to whatever input and output buffering and other processing  cat  may do.  (This would likely earn you a  'Useless Use Of Cat'  award if I were Randal Schwartz.     A better construction would be:   $ /usr/bin/time program_to_benchmark < big_file    In this statement it is the  shell  which opens big_file, passing it to your program (well, actually to  time  which then executes your program as a subprocess) as an already-open file descriptor. 100% of the file reading is strictly the responsibility of the program you're trying to benchmark.  This gets you a real reading of its performance without spurious complications.   I will mention two possible, but actually wrong, 'fixes' which could also be considered (but I 'number' them differently as these are not things which were wrong in the original post):   A. You could 'fix' this by timing only your program:   $ cat big_file | /usr/bin/time program_to_benchmark    B. or by timing the entire pipeline:   $ /usr/bin/time sh -c 'cat big_file | program_to_benchmark'    These are wrong for the same reasons as #2: they're still using  cat  unnecessarily. I mention them for a few reasons:     they're more 'natural' for people who aren't entirely comfortable with the I/O redirection facilities of the POSIX shell   there may be cases where  cat   is  needed (e.g.: the file to be read requires some sort of privilege to access, and you do not want to grant that privilege to the program to be benchmarked:  sudo cat /dev/sda | /usr/bin/time my_compression_test --no-output )   in practice , on modern machines, the added  cat  in the pipeline is probably of no real consequence.     But I say that last thing with some hesitation. If we examine the last result in 'Edit 5' --   $ /usr/bin/time cat temp_big_file | wc -l 0.01user 1.34system 0:01.83elapsed 74%CPU ...    -- this claims that  cat  consumed 74% of the CPU during the test; and indeed 1.34/1.83 is approximately 74%.  Perhaps a run of:   $ /usr/bin/time wc -l < temp_big_file    would have taken only the remaining .49 seconds!  Probably not:  cat  here had to pay for the  read()  system calls (or equivalent) which transferred the file from 'disk' (actually buffer cache), as well as the pipe writes to deliver them to  wc .  The correct test would still have had to do those  read()  calls; only the write-to-pipe and read-from-pipe calls would have been saved, and those should be pretty cheap.   Still, I predict you would be able to measure the difference between  cat file | wc -l  and  wc -l < file  and find a noticeable (2-digit percentage) difference. Each of the slower tests will have paid a similar penalty in absolute time; which would however amount to a smaller fraction of its larger total time.   In fact I did some quick tests with a 1.5 gigabyte file of garbage, on a Linux 3.13 (Ubuntu 14.04) system, obtaining these results (these are actually 'best of 3' results; after priming the cache, of course):    $ time wc -l < /tmp/junk real 0.280s user 0.156s sys 0.124s (total cpu 0.280s) $ time cat /tmp/junk | wc -l real 0.407s user 0.157s sys 0.618s (total cpu 0.775s) $ time sh -c 'cat /tmp/junk | wc -l' real 0.411s user 0.118s sys 0.660s (total cpu 0.778s)    Notice that the two pipeline results claim to have taken more CPU time (user+sys) than real wall-clock time. This is because I'm using the shell (bash)'s built-in 'time' command, which is cognizant of the pipeline; and I'm on a multi-core machine where separate processes in a pipeline can use separate cores, accumulating CPU time faster than realtime.  Using  /usr/bin/time  I see smaller CPU time than realtime -- showing that it can only time the single pipeline element passed to it on its command line. Also, the shell's output gives milliseconds while  /usr/bin/time  only gives hundredths of a second.   So at the efficiency level of  wc -l , the  cat  makes a huge difference: 409 / 283 = 1.453 or 45.3% more realtime, and 775 / 280 = 2.768, or a whopping 177% more CPU used!  On my random it-was-there-at-the-time test box.   I should add that there is at least one other significant difference between these styles of testing, and I can't say whether it is a benefit or fault; you have to decide this yourself:   When you run  cat big_file | /usr/bin/time my_program , your program is receiving input from a pipe, at precisely the pace sent by  cat , and in chunks no larger than written by  cat .   When you run  /usr/bin/time my_program < big_file , your program receives an open file descriptor to the actual file.  Your program --  or  in many cases the I/O libraries of the language in which it was written -- may take different actions when presented with a file descriptor referencing a regular file.  It may use  mmap(2)  to map the input file into its address space, instead of using explicit  read(2)  system calls.  These differences could have a far larger effect on your benchmark results than the small cost of running the  cat  binary.   Of course it is an interesting benchmark result if the same program performs significantly differently between the two cases. It shows that, indeed, the program or its I/O libraries  are  doing something interesting, like using  mmap() . So in practice it might be good to run the benchmarks both ways; perhaps discounting the  cat  result by some small factor to \"forgive\" the cost of running  cat  itself.", "score": 218}}
{"question": "Why is reading lines from stdin much slower in C++ than Python?", "tags": ["python", "c++", "benchmarking", "iostream", "getline"], "link": "https://stackoverflow.com/questions/9371238/why-is-reading-lines-from-stdin-much-slower-in-c-than-python", "answer_count": 11, "answers": {"id": 9657502, "body": "Just out of curiosity I've taken a look at what happens under the hood, and I've used  dtruss/strace  on each test.   C++   ./a.out < in Saw 6512403 lines in 8 seconds.  Crunch speed: 814050    syscalls  sudo dtruss -c ./a.out < in   CALL                                        COUNT __mac_syscall                                   1   open                                            6 pread                                           8 mprotect                                       17 mmap                                           22 stat64                                         30 read_nocancel                               25958    Python   ./a.py < in Read 6512402 lines in 1 seconds. LPS: 6512402    syscalls  sudo dtruss -c ./a.py < in   CALL                                        COUNT __mac_syscall                                   1   open                                            5 pread                                           8 mprotect                                       17 mmap                                           21 stat64                                         29", "score": 224}}
{"question": "Why is reading lines from stdin much slower in C++ than Python?", "tags": ["python", "c++", "benchmarking", "iostream", "getline"], "link": "https://stackoverflow.com/questions/9371238/why-is-reading-lines-from-stdin-much-slower-in-c-than-python", "answer_count": 11, "answers": {"id": 9371717, "body": "tl;dr: Because of different default settings in C++ requiring more system calls.   By default,  cin  is synchronized with stdio, which causes it to avoid any input buffering.  If you add this to the top of your main, you should see much better performance:   std::ios_base::sync_with_stdio(false);    Normally, when an input stream is buffered, instead of reading one character at a time, the stream will be read in larger chunks.  This reduces the number of system calls, which are typically relatively expensive.  However, since the  FILE*  based  stdio  and  iostreams  often have separate implementations and therefore separate buffers, this could lead to a problem if both were used together.  For example:   int myvalue1; cin >> myvalue1; int myvalue2; scanf(\"%d\",&myvalue2);    If more input was read by  cin  than it actually needed, then the second integer value wouldn't be available for the  scanf  function, which has its own independent buffer.  This would lead to unexpected results.   To avoid this, by default, streams are synchronized with  stdio .  One common way to achieve this is to have  cin  read each character one at a time as needed using  stdio  functions.  Unfortunately, this introduces a lot of overhead.  For small amounts of input, this isn't a big problem, but when you are reading millions of lines, the performance penalty is significant.   Fortunately, the library designers decided that you should also be able to disable this feature to get improved performance if you knew what you were doing, so they provided the  sync_with_stdio  method. From this link (emphasis added):     If the synchronization is turned off, the C++ standard streams are allowed to buffer their I/O independently,  which may be considerably faster in some cases .", "score": 1956}}
{"question": "How to check if the string is empty in Python?", "tags": ["python", "string", "boolean", "is-empty", "comparison-operators"], "link": "https://stackoverflow.com/questions/9573244/how-to-check-if-the-string-is-empty-in-python", "answer_count": 20, "answers": {"id": 9573278, "body": "The most elegant way would probably be to simply check if its true or falsy, e.g.:   if not my_string:    However, you may want to strip white space because:    >>> bool(\"\")  False  >>> bool(\"   \")  True  >>> bool(\"   \".strip())  False    You should probably be a bit more explicit in this however, unless you know for sure that this string has passed some kind of validation and is a string that can be tested this way.", "score": 358}}
{"question": "How to check if the string is empty in Python?", "tags": ["python", "string", "boolean", "is-empty", "comparison-operators"], "link": "https://stackoverflow.com/questions/9573244/how-to-check-if-the-string-is-empty-in-python", "answer_count": 20, "answers": {"id": 9573283, "body": "From  PEP 8 , in the  \u201cProgramming Recommendations\u201d section :     For sequences, (strings, lists, tuples), use the fact that empty sequences are false.     So you should use:   if not some_string:    or:   if some_string:    Just to clarify, sequences are  evaluated  to  False  or  True  in a Boolean context if they are empty or not. They are  not equal  to  False  or  True .", "score": 564}}
{"question": "How to check if the string is empty in Python?", "tags": ["python", "string", "boolean", "is-empty", "comparison-operators"], "link": "https://stackoverflow.com/questions/9573244/how-to-check-if-the-string-is-empty-in-python", "answer_count": 20, "answers": {"id": 9573259, "body": "Empty strings are \"falsy\" ( python 2  or  python 3  reference), which means they are considered false in a Boolean context, so you can just do this:   if not myString:    This is the preferred way if you know that your variable is a string.  If your variable could also be some other type then you should use:   if myString == \"\":    See the documentation on  Truth Value Testing  for other values that are false in Boolean contexts.", "score": 3147}}
{"question": "Determine the type of an object?", "tags": ["python", "dictionary", "types", "typeof"], "link": "https://stackoverflow.com/questions/2225038/determine-the-type-of-an-object", "answer_count": 15, "answers": {"id": 2225081, "body": "It might be more Pythonic to use a  try ... except  block. That way, if you have a class which quacks like a list, or quacks like a dict, it will behave properly regardless of what its type  really  is.   To clarify, the preferred method of \"telling the difference\" between variable types is with something called  duck typing : as long as the methods (and return types) that a variable responds to are what your subroutine expects, treat it like what you expect it to be. For example, if you have a class that overloads the bracket operators with  getattr  and  setattr , but uses some funny internal scheme, it would be appropriate for it to behave as a dictionary if that's what it's trying to emulate.   The other problem with the  type(A) is type(B)  checking is that if  A  is a subclass of  B , it evaluates to  false  when, programmatically, you would hope it would be  true . If an object is a subclass of a list, it should work like a list: checking the type as presented in the other answer will prevent this. ( isinstance  will work, however).", "score": 45}}
{"question": "Determine the type of an object?", "tags": ["python", "dictionary", "types", "typeof"], "link": "https://stackoverflow.com/questions/2225038/determine-the-type-of-an-object", "answer_count": 15, "answers": {"id": 2225055, "body": "Use  type() :   >>> a = [] >>> type(a)   >>> f = () >>> type(f)", "score": 233}}
{"question": "Determine the type of an object?", "tags": ["python", "dictionary", "types", "typeof"], "link": "https://stackoverflow.com/questions/2225038/determine-the-type-of-an-object", "answer_count": 15, "answers": {"id": 2225066, "body": "There are two built-in functions that help you identify the type of an object. You can use  type()   if you need the exact type of an object, and  isinstance()  to  check  an object\u2019s type against something. Usually, you want to use  isinstance()  most of the times since it is very robust and also supports type inheritance.     To get the actual type of an object, you use the built-in  type()  function. Passing an object as the only parameter will return the type object of that object:   >>> type([]) is list True >>> type({}) is dict True >>> type('') is str True >>> type(0) is int True    This of course also works for custom types:   >>> class Test1 (object):         pass >>> class Test2 (Test1):         pass >>> a = Test1() >>> b = Test2() >>> type(a) is Test1 True >>> type(b) is Test2 True    Note that  type()  will only return the immediate type of the object, but won\u2019t be able to tell you about type inheritance.   >>> type(b) is Test1 False    To cover that, you should use the  isinstance  function. This of course also works for built-in types:   >>> isinstance(b, Test1) True >>> isinstance(b, Test2) True >>> isinstance(a, Test1) True >>> isinstance(a, Test2) False >>> isinstance([], list) True >>> isinstance({}, dict) True    isinstance()  is usually the preferred way to ensure the type of an object because it will also accept derived types. So unless you actually need the type object (for whatever reason), using  isinstance()  is preferred over  type() .   The second parameter of  isinstance()  also accepts a tuple of types, so it\u2019s possible to check for multiple types at once.  isinstance  will then return true, if the object is of any of those types:   >>> isinstance([], (tuple, list, set)) True", "score": 2387}}
{"question": "How do I count the occurrences of a list item?", "tags": ["python", "list", "count"], "link": "https://stackoverflow.com/questions/2600191/how-do-i-count-the-occurrences-of-a-list-item", "answer_count": 29, "answers": {"id": 23909767, "body": "Counting the occurrences of one item in a list   For counting the occurrences of just one list item you can use  count()   >>> l = [\"a\",\"b\",\"b\"] >>> l.count(\"a\") 1 >>> l.count(\"b\") 2    Counting the occurrences of  all  items in a list is also known as \"tallying\" a list, or creating a tally counter.   Counting all items with count()   To count the occurrences of items in  l  one can simply use a list comprehension and the  count()  method   [[x,l.count(x)] for x in set(l)]    (or similarly with a dictionary  dict((x,l.count(x)) for x in set(l)) )   Example:    >>> l = [\"a\",\"b\",\"b\"] >>> [[x,l.count(x)] for x in set(l)] [['a', 1], ['b', 2]] >>> dict((x,l.count(x)) for x in set(l)) {'a': 1, 'b': 2}    Counting all items with Counter()   Alternatively, there's the faster  Counter  class from the  collections  library   Counter(l)    Example:   >>> l = [\"a\",\"b\",\"b\"] >>> from collections import Counter >>> Counter(l) Counter({'b': 2, 'a': 1})    How much faster is Counter?   I checked how much faster  Counter  is for tallying lists. I tried both methods out with a few values of  n  and it appears that  Counter  is faster by a constant factor of approximately 2.   Here is the script I used:   from __future__ import print_function import timeit  t1=timeit.Timer('Counter(l)', \\                 'import random;import string;from collections import Counter;n=1000;l=[random.choice(string.ascii_letters) for x in range(n)]'                 )  t2=timeit.Timer('[[x,l.count(x)] for x in set(l)]',                 'import random;import string;n=1000;l=[random.choice(string.ascii_letters) for x in range(n)]'                 )  print(\"Counter(): \", t1.repeat(repeat=3,number=10000)) print(\"count():   \", t2.repeat(repeat=3,number=10000)    And the output:   Counter():  [0.46062711701961234, 0.4022796869976446, 0.3974247490405105] count():    [7.779430688009597, 7.962715800967999, 8.420845870045014]", "score": 373}}
{"question": "How do I count the occurrences of a list item?", "tags": ["python", "list", "count"], "link": "https://stackoverflow.com/questions/2600191/how-do-i-count-the-occurrences-of-a-list-item", "answer_count": 29, "answers": {"id": 5829377, "body": "Use  Counter  if you are using Python 2.7 or 3.x and you want the number of occurrences for each element:   >>> from collections import Counter >>> z = ['blue', 'red', 'blue', 'yellow', 'blue', 'red'] >>> Counter(z) Counter({'blue': 3, 'red': 2, 'yellow': 1})", "score": 2445}}
{"question": "How do I count the occurrences of a list item?", "tags": ["python", "list", "count"], "link": "https://stackoverflow.com/questions/2600191/how-do-i-count-the-occurrences-of-a-list-item", "answer_count": 29, "answers": {"id": 2600208, "body": "If you only want a single item's count, use the  count  method:   >>> [1, 2, 3, 4, 1, 4, 1].count(1) 3      Important: this is very slow if you are counting  multiple  different items   Each  count  call goes over the entire list of  n  elements. Calling  count  in a loop  n  times means  n * n  total checks, which can be catastrophic for performance.   If you want to count multiple items, use  Counter , which only does  n  total checks.", "score": 2555}}
{"question": "What is the difference between venv, pyvenv, pyenv, virtualenv, virtualenvwrapper, pipenv, etc?", "tags": ["python", "virtualenv", "virtualenvwrapper", "pyenv", "python-venv"], "link": "https://stackoverflow.com/questions/41573587/what-is-the-difference-between-venv-pyvenv-pyenv-virtualenv-virtualenvwrappe", "answer_count": 8, "answers": {"id": 59923461, "body": "UPDATE 2020-08-25:   Added below \" Conclusion \" paragraph   I've went down the  pipenv  rabbit hole ( it's a deep and dark hole indeed... ) and  since the last answer is over 2 years ago , felt it was useful to update the discussion with the latest developments on the Python virtual envelopes topic I've found.   DISCLAIMER:   This answer is  NOT  about continuing the raging debate about the merits of  pipenv   versus   venv  as envelope solutions-  I make no endorsement of either . It's about  PyPA  endorsing conflicting standards and how future development of  virtualenv  promises to negate making an  either/or  choice between them at all. I focused on these two tools precisely because they are the anointed ones by  PyPA .     venv   As the OP notes,  venv  is a tool for virtualizing environments.  NOT  a third party solution, but native tool.  PyPA  endorses  venv  for creating  VIRTUAL ENVELOPES : \" Changed in version 3.5: The use of venv is now recommended for creating virtual environments \".   pipenv   pipenv - like  venv  - can be used to create virtual envelopes but additionally rolls-in package management and  vulnerability checking  functionality. Instead of using  requirements.txt ,  pipenv  delivers package management via  Pipfile .  As  PyPA  endorses pipenv for  PACKAGE MANAGEMENT , that would seem to imply  pipfile  is to supplant  requirements.txt .   HOWEVER :  pipenv  uses  virtualenv  as its tool for creating virtual envelopes,  NOT   venv  which is endorsed by  PyPA  as the go-to tool for creating virtual envelopes.   Conflicting Standards:   So if settling on a virtual envelope solution wasn't difficult enough, we now have  PyPA  endorsing two different tools which use different virtual envelope solutions. The raging Github debate on  venv vs virtualenv  which highlights this conflict can be found  here .   Conflict Resolution:   The Github debate referenced in above link has steered  virtualenv  development in the direction of accommodating  venv  in  future releases :     prefer built-in venv: if the target python has venv we'll create the environment using that (and then perform subsequent operations on that to facilitate other guarantees we offer)       Conclusion:   So it looks like there will be some future convergence between the two rival virtual envelope solutions, but as of now  pipenv - which uses  virtualenv  - varies materially from  venv .   Given  the problems  pipenv  solves  and the fact that  PyPA  has given its blessing, it  appears  to have a bright future. And if  virtualenv  delivers on its proposed development objectives, choosing a virtual envelope solution should no longer be a case of either  pipenv  OR  venv .   Update 2020-08-25:   An oft repeated criticism of  Pipenv  I saw when producing this analysis was that it was not actively maintained. Indeed, what's the point of using a solution whose future could be seen questionable due to lack of continuous development? After a dry spell of about 18 months,  Pipenv  is once again being actively developed. Indeed, large and material updates have since been  released .", "score": 129}}
{"question": "What is the difference between venv, pyvenv, pyenv, virtualenv, virtualenvwrapper, pipenv, etc?", "tags": ["python", "virtualenv", "virtualenvwrapper", "pyenv", "python-venv"], "link": "https://stackoverflow.com/questions/41573587/what-is-the-difference-between-venv-pyvenv-pyenv-virtualenv-virtualenvwrappe", "answer_count": 8, "answers": {"id": 47559925, "body": "I would just avoid the use of  virtualenv  after Python3.3+ and instead use the standard shipped library  venv . To create a new virtual environment you would type:   $ python3 -m venv        virtualenv  tries to copy the Python binary into the virtual environment's bin directory. However it does not update library file links embedded into that binary, so if you build Python from source into a non-system directory with relative path names, the Python binary breaks. Since this is how you make a copy distributable Python, it is a big flaw. BTW to inspect embedded library file links on OS X, use  otool . For example from within your virtual environment, type:   $ otool -L bin/python python:     @executable_path/../Python (compatibility version 3.4.0, current version 3.4.0)     /usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1238.0.0)    Consequently I would avoid  virtualenvwrapper  and  pipenv .  pyvenv  is deprecated.  pyenv  seems to be used often where  virtualenv  is used but I would stay away from it also since I think  venv  also does what  pyenv  is built for.   venv  creates virtual environments in the shell that are  fresh  and  sandboxed , with  user-installable libraries , and it's  multi-python safe .   Fresh : because virtual environments only start with the standard libraries that ship with python, you have to install any other libraries all over again with  pip install  while the virtual environment is active.   Sandboxed : because none of these new library installs are visible outside the virtual environment, so you can delete the whole environment and start again without worrying about impacting your base python install.   User-installable libraries : because the virtual environment's target folder is created without  sudo  in some directory you already own, so you won't need  sudo  permissions to install libraries into it.   multi-python safe : because when virtual environments activate, the shell only sees the python version (3.4, 3.5 etc.) that was used to build that virtual environment.   pyenv  is similar to  venv  in that it lets you manage multiple python environments. However with  pyenv  you can't conveniently rollback library installs to some start state and you will likely need  admin  privileges at some point to update libraries. So I think it is also best to use  venv .   In the last couple of years I have found many problems in build systems (emacs packages, python standalone application builders, installers...) that ultimately come down to issues with  virtualenv . I think python will be a better platform when we eliminate this additional option and only use  venv .   EDIT: Tweet of the BDFL,     I use venv (in the stdlib) and a bunch of shell aliases to quickly switch.   \u2014 Guido van Rossum (@gvanrossum)  October 22, 2020", "score": 630}}
{"question": "What is the difference between venv, pyvenv, pyenv, virtualenv, virtualenvwrapper, pipenv, etc?", "tags": ["python", "virtualenv", "virtualenvwrapper", "pyenv", "python-venv"], "link": "https://stackoverflow.com/questions/41573587/what-is-the-difference-between-venv-pyvenv-pyenv-virtualenv-virtualenvwrappe", "answer_count": 8, "answers": {"id": 41573588, "body": "This is my personal recommendation for beginners:  start by learning  virtualenv  and  pip , tools which work with both Python 2 and 3 and in a variety of situations, and pick up other tools once you start needing them.   Now on to answer the question: what is the difference between these similarly named things: venv, virtualenv, etc?   PyPI packages not in the standard library:     virtualenv  is a very popular tool that creates isolated Python environments for Python libraries. If you're not familiar with this tool, I highly recommend learning it, as it is a very useful tool.   It works by installing a bunch of files in a directory (eg:  env/ ), and then modifying the  PATH  environment variable to prefix it with a custom  bin  directory (eg:  env/bin/ ). An exact copy of the  python  or  python3  binary is placed in this directory, but Python is programmed to look for libraries relative to its path first, in the environment directory. It's not part of Python's standard library, but is officially blessed by the PyPA (Python Packaging Authority). Once activated, you can install packages in the virtual environment using  pip .     pyenv  is used to isolate Python versions. For example, you may want to test your code against Python 2.7, 3.6, 3.7 and 3.8, so you'll need a way to switch between them. Once activated, it prefixes the  PATH  environment variable with  ~/.pyenv/shims , where there are special files matching the Python commands ( python ,  pip ). These are not copies of the Python-shipped commands; they are special scripts that decide on the fly which version of Python to run based on the  PYENV_VERSION  environment variable, or the  .python-version  file, or the  ~/.pyenv/version  file.  pyenv  also makes the process of downloading and installing multiple Python versions easier, using the command  pyenv install .     pyenv-virtualenv  is a plugin for  pyenv  by the same author as  pyenv , to allow you to use  pyenv  and  virtualenv  at the same time conveniently. However, if you're using Python 3.3 or later,  pyenv-virtualenv  will try to run  python -m venv  if it is available, instead of  virtualenv . You can use  virtualenv  and  pyenv  together without  pyenv-virtualenv , if you don't want the convenience features.     virtualenvwrapper  is a set of extensions to  virtualenv  (see  docs ). It gives you commands like  mkvirtualenv ,  lssitepackages , and especially  workon  for switching between different  virtualenv  directories. This tool is especially useful if you want multiple  virtualenv  directories.     pyenv-virtualenvwrapper  is a plugin for  pyenv  by the same author as  pyenv , to conveniently integrate  virtualenvwrapper  into  pyenv .     pipenv  aims to combine  Pipfile ,  pip  and  virtualenv  into one command on the command-line. The  virtualenv  directory typically gets placed in  ~/.local/share/virtualenvs/XXX , with  XXX  being a hash of the path of the project directory. This is different from  virtualenv , where the directory is typically in the current working directory.  pipenv  is meant to be used when developing Python applications (as opposed to libraries). There are alternatives to  pipenv , such as  poetry , which I won't list here since this question is only about the packages that are similarly named.       Standard library:     pyvenv  (not to be confused with  pyenv  in the previous section) is a script shipped with Python 3.3 to 3.7. It was  removed from Python 3.8  as it had problems (not to mention the confusing name). Running  python3 -m venv  has exactly the same effect as  pyvenv .     venv  is a package shipped with Python 3, which you can run using  python3 -m venv  (although for some reason some distros separate it out into a separate distro package, such as  python3-venv  on Ubuntu/Debian). It serves the same purpose as  virtualenv , but only has a subset of its features ( see a comparison here ).  virtualenv  continues to be more popular than  venv , especially since the former supports both Python 2 and 3.", "score": 2437}}
{"question": "How can I determine a Python variable&#39;s type?", "tags": ["python", "types"], "link": "https://stackoverflow.com/questions/402504/how-can-i-determine-a-python-variables-type", "answer_count": 22, "answers": {"id": 32885953, "body": "It is so simple. You do it like this.   print(type(variable_name))", "score": 214}}
{"question": "How can I determine a Python variable&#39;s type?", "tags": ["python", "types"], "link": "https://stackoverflow.com/questions/402504/how-can-i-determine-a-python-variables-type", "answer_count": 22, "answers": {"id": 402507, "body": "You may be looking for the  type()   built-in function .   See the examples below, but there's no \"unsigned\" type in Python just like Java.   Positive integer:   >>> v = 10 >>> type(v)      Large  positive integer:   >>> v = 100000000000000 >>> type(v)      Negative integer:   >>> v = -10 >>> type(v)      Literal sequence of characters:   >>> v = 'hi' >>> type(v)      Floating point number:   >>> v = 3.14159 >>> type(v)", "score": 530}}
{"question": "How can I determine a Python variable&#39;s type?", "tags": ["python", "types"], "link": "https://stackoverflow.com/questions/402504/how-can-i-determine-a-python-variables-type", "answer_count": 22, "answers": {"id": 402704, "body": "Use the  type()  built-in function:   >>> i = 123 >>> type(i)   >>> type(i) is int True >>> i = 123.456 >>> type(i)   >>> type(i) is float True    To check if a variable is of a given type, use  isinstance :   >>> i = 123 >>> isinstance(i, int) True >>> isinstance(i, (float, str, set, dict)) False    Note that Python doesn't have the same types as C/C++, which appears to be your question.", "score": 2094}}
{"question": "Delete an element from a dictionary", "tags": ["python", "dictionary", "del"], "link": "https://stackoverflow.com/questions/5844672/delete-an-element-from-a-dictionary", "answer_count": 20, "answers": {"id": 5844700, "body": "I think your solution is best way to do it. But if you want another solution, you can create a new dictionary with using the keys from old dictionary without including your specified key, like this:   >>> a {0: 'zero', 1: 'one', 2: 'two', 3: 'three'} >>> {i:a[i] for i in a if i!=0} {1: 'one', 2: 'two', 3: 'three'}", "score": 119}}
{"question": "Delete an element from a dictionary", "tags": ["python", "dictionary", "del"], "link": "https://stackoverflow.com/questions/5844672/delete-an-element-from-a-dictionary", "answer_count": 20, "answers": {"id": 22564121, "body": "pop  mutates the dictionary.    >>> lol = {\"hello\": \"gdbye\"}  >>> lol.pop(\"hello\")      'gdbye'  >>> lol      {}    If you want to keep the original you could just copy it.", "score": 529}}
{"question": "Delete an element from a dictionary", "tags": ["python", "dictionary", "del"], "link": "https://stackoverflow.com/questions/5844672/delete-an-element-from-a-dictionary", "answer_count": 20, "answers": {"id": 5844692, "body": "The  del  statement  removes an element:   del d[key]    Note that this mutates the existing dictionary, so the contents of the dictionary changes for anybody else who has a reference to the same instance. To return a  new  dictionary, make a copy of the dictionary:   def removekey(d, key):     r = dict(d)     del r[key]     return r    The  dict()  constructor makes a  shallow copy . To make a deep copy, see the  copy  module .     Note that making a copy for every dict  del /assignment/etc. means you're going from constant time to linear time, and also using linear space. For small dicts, this is not a problem. But if you're planning to make lots of copies of large dicts, you probably want a different data structure, like a HAMT (as described in  this answer ).", "score": 2567}}
{"question": "How do I pad a string with zeros?", "tags": ["python", "string", "zero-padding"], "link": "https://stackoverflow.com/questions/339007/how-do-i-pad-a-string-with-zeros", "answer_count": 19, "answers": {"id": 24386708, "body": "For Python 3.6+ using  f-strings :   >>> i = 1 >>> f\"{i:0>2}\"  # Works for both numbers and strings. '01' >>> f\"{i:02}\"  # Works only for numbers. '01'    For  Python 2.6  to Python 3.5:   >>> \"{:0>2}\".format(\"1\")  # Works for both numbers and strings. '01' >>> \"{:02}\".format(1)  # Works only for numbers. '01'    Those  standard format specifiers  are  [[fill]align][minimumwidth]  and  [0][minimumwidth] .", "score": 258}}
{"question": "How do I pad a string with zeros?", "tags": ["python", "string", "zero-padding"], "link": "https://stackoverflow.com/questions/339007/how-do-i-pad-a-string-with-zeros", "answer_count": 19, "answers": {"id": 339024, "body": "Just use the  rjust  method of the string object.   This example creates a 10-character length string, padding as necessary:   >>> s = 'test' >>> s.rjust(10, '0') >>> '000000test'", "score": 502}}
{"question": "How do I pad a string with zeros?", "tags": ["python", "string", "zero-padding"], "link": "https://stackoverflow.com/questions/339007/how-do-i-pad-a-string-with-zeros", "answer_count": 19, "answers": {"id": 339013, "body": "To pad strings:   >>> n = '4' >>> print(n.zfill(3)) 004    To pad numbers:   >>> n = 4 >>> print(f'{n:03}') # Preferred method, python >= 3.6 004 >>> print('%03d' % n) 004 >>> print(format(n, '03')) # python >= 2.6 004 >>> print('{0:03d}'.format(n))  # python >= 2.6 + python 3 004 >>> print('{foo:03d}'.format(foo=n))  # python >= 2.6 + python 3 004 >>> print('{:03d}'.format(n))  # python >= 2.7 + python3 004    String formatting documentation .", "score": 3485}}
{"question": "How do I get the number of elements in a list (length of a list) in Python?", "tags": ["python", "list"], "link": "https://stackoverflow.com/questions/1712227/how-do-i-get-the-number-of-elements-in-a-list-length-of-a-list-in-python", "answer_count": 11, "answers": {"id": 16114025, "body": "While this may not be useful due to the fact that it'd make a lot more sense as being \"out of the box\" functionality, a fairly simple hack would be to build a class with a  length  property:   class slist(list):     @property     def length(self):         return len(self)    You can use it like so:   >>> l = slist(range(10)) >>> l.length 10 >>> print l [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]    Essentially, it's exactly identical to a list object, with the added benefit of having an OOP-friendly  length  property.   As always, your mileage may vary.", "score": 80}}
{"question": "How do I get the number of elements in a list (length of a list) in Python?", "tags": ["python", "list"], "link": "https://stackoverflow.com/questions/1712227/how-do-i-get-the-number-of-elements-in-a-list-length-of-a-list-in-python", "answer_count": 11, "answers": {"id": 27137427, "body": "How do I get the length of a list?     To find the number of elements in a list, use the builtin function  len :   items = [] items.append(\"apple\") items.append(\"orange\") items.append(\"banana\")    And now:   len(items)    returns 3.   Explanation   Everything in Python is an object, including lists. All objects have a header of some sort in the C implementation.   Lists and other similar builtin objects with a \"size\" in Python, in particular, have an attribute called  ob_size , where the number of elements in the object is cached. So checking the number of objects in a list is very fast.   But if you're checking if list size is zero or not, don't use  len  - instead, put the list in a boolean context -  it is treated as False if empty, and True if non-empty .   From the  docs   len(s)     Return the length (the number of items) of an object. The argument may be a sequence (such as a string, bytes, tuple, list, or range) or a collection (such as a dictionary, set, or frozen set).     len  is implemented with  __len__ , from the data model  docs :   object.__len__(self)     Called to implement the built-in function  len() . Should return the length of the object, an integer >= 0. Also, an object that doesn\u2019t define a  __nonzero__()  [in Python 2 or  __bool__()  in Python 3] method and whose  __len__()  method returns zero is considered to be false in a Boolean context.     And we can also see that  __len__  is a method of lists:   items.__len__()    returns 3.   Builtin types you can get the  len  (length) of   And in fact we see we can get this information for all of the described types:   >>> all(hasattr(cls, '__len__') for cls in (str, bytes, tuple, list,                                              range, dict, set, frozenset)) True    Do not use  len  to test for an empty or nonempty list   To test for a specific length, of course, simply test for equality:   if len(items) == required_length:     ...    But there's a special case for testing for a zero length list or the inverse. In that case, do not test for equality.   Also, do not do:   if len(items):      ...    Instead, simply do:   if items:     # Then we have some items, not empty!     ...    or   if not items: # Then we have an empty list!     ...    I  explain why here  but in short,  if items  or  if not items  is more readable and performant than other alternatives.", "score": 318}}
{"question": "How do I get the number of elements in a list (length of a list) in Python?", "tags": ["python", "list"], "link": "https://stackoverflow.com/questions/1712227/how-do-i-get-the-number-of-elements-in-a-list-length-of-a-list-in-python", "answer_count": 11, "answers": {"id": 1712236, "body": "The  len()  function can be used with several different types in Python - both built-in types and library types. For example:   >>> len([1, 2, 3]) 3", "score": 2973}}
{"question": "Delete a column from a Pandas DataFrame", "tags": ["python", "pandas", "dataframe", "del"], "link": "https://stackoverflow.com/questions/13411544/delete-a-column-from-a-pandas-dataframe", "answer_count": 23, "answers": {"id": 22596982, "body": "Use:   columns = ['Col1', 'Col2', ...] df.drop(columns, inplace=True, axis=1)    This will delete one or more columns in-place. Note that  inplace=True  was added in pandas v0.13 and won't work on older versions. You'd have to assign the result back in that case:   df = df.drop(columns, axis=1)", "score": 318}}
{"question": "Delete a column from a Pandas DataFrame", "tags": ["python", "pandas", "dataframe", "del"], "link": "https://stackoverflow.com/questions/13411544/delete-a-column-from-a-pandas-dataframe", "answer_count": 23, "answers": {"id": 13485766, "body": "As you've guessed, the right syntax is    del df['column_name']    It's difficult to make  del df.column_name  work simply as the result of syntactic limitations in Python.  del df[name]  gets translated to  df.__delitem__(name)  under the covers by Python.", "score": 1332}}
{"question": "Delete a column from a Pandas DataFrame", "tags": ["python", "pandas", "dataframe", "del"], "link": "https://stackoverflow.com/questions/13411544/delete-a-column-from-a-pandas-dataframe", "answer_count": 23, "answers": {"id": 18145399, "body": "The best way to do this in Pandas is to use  drop :   df = df.drop('column_name', axis=1)    where  1  is the  axis  number ( 0  for rows and  1  for columns.)   Or, the  drop()  method accepts  index / columns  keywords as an alternative to specifying the axis. So we can now just do:   df = df.drop(columns=['column_nameA', 'column_nameB'])      This was  introduced in v0.21.0  (October 27, 2017)     To delete the column without having to reassign  df  you can do:   df.drop('column_name', axis=1, inplace=True)    Finally, to drop by column  number  instead of by column  label , try this to delete, e.g. the 1st, 2nd and 4th columns:   df = df.drop(df.columns[[0, 1, 3]], axis=1)  # df.columns is zero-based pd.Index    Also working with \"text\" syntax for the columns:   df.drop(['column_nameA', 'column_nameB'], axis=1, inplace=True)", "score": 3717}}
{"question": "How to remove an element from a list by index", "tags": ["python", "list", "indexing"], "link": "https://stackoverflow.com/questions/627435/how-to-remove-an-element-from-a-list-by-index", "answer_count": 18, "answers": {"id": 24352671, "body": "Like others mentioned pop and del are  the  efficient ways to remove an item of given index. Yet just for the sake of completion (since the same thing can be done via many ways in Python):   Using slices (this does not do in place removal of item from original list):   (Also this will be the least efficient method when working with Python list, but this could be useful (but not efficient, I reiterate) when working with user defined objects that do not support pop, yet do define a  __getitem__  ):   >>> a = [1, 2, 3, 4, 5, 6] >>> index = 3 # Only positive index  >>> a = a[:index] + a[index+1 :] # a is now [1, 2, 3, 5, 6]    Note:  Please note that this method does not modify the list in place like  pop  and  del . It instead makes two copies of lists (one from the start until the index but without it ( a[:index] ) and one after the index till the last element ( a[index+1:] )) and creates a new list object by adding both. This is then reassigned to the list variable ( a ). The old list object is hence dereferenced and hence garbage collected (provided the original list object is not referenced by any variable other than a).   This makes this method very inefficient and it can also produce undesirable side effects (especially when other variables point to the original list object which remains un-modified).   Thanks to @MarkDickinson for pointing this out ...   This  Stack Overflow answer explains the concept of slicing.   Also note that this works only with positive indices.   While using with objects, the  __getitem__  method must have been defined and more importantly the  __add__  method must have been defined to return an object containing items from both the operands.   In essence, this works with any object whose class definition is like:   class foo(object):     def __init__(self, items):         self.items = items      def __getitem__(self, index):         return foo(self.items[index])      def __add__(self, right):         return foo( self.items + right.items )    This works with  list  which defines  __getitem__  and  __add__  methods.   Comparison of the three ways in terms of efficiency:   Assume the following is predefined:   a = range(10) index = 3    The  del object[index]  method:   By far the most efficient method. It works will all objects that define a  __del__  method.   The disassembly is as follows:   Code:   def del_method():     global a     global index     del a[index]    Disassembly:    10    0 LOAD_GLOBAL     0 (a)        3 LOAD_GLOBAL     1 (index)        6 DELETE_SUBSCR   # This is the line that deletes the item        7 LOAD_CONST      0 (None)       10 RETURN_VALUE None    pop  method:   It is less efficient than the del method and is used when you need to get the deleted item.   Code:   def pop_method():     global a     global index     a.pop(index)    Disassembly:    17     0 LOAD_GLOBAL     0 (a)         3 LOAD_ATTR       1 (pop)         6 LOAD_GLOBAL     2 (index)         9 CALL_FUNCTION   1        12 POP_TOP        13 LOAD_CONST      0 (None)        16 RETURN_VALUE    The slice and add method.   The least efficient.   Code:   def slice_method():     global a     global index     a = a[:index] + a[index+1:]    Disassembly:    24     0 LOAD_GLOBAL    0 (a)         3 LOAD_GLOBAL    1 (index)         6 SLICE+2         7 LOAD_GLOBAL    0 (a)        10 LOAD_GLOBAL    1 (index)        13 LOAD_CONST     1 (1)        16 BINARY_ADD        17 SLICE+1        18 BINARY_ADD        19 STORE_GLOBAL   0 (a)        22 LOAD_CONST     0 (None)        25 RETURN_VALUE None    Note: In all three disassembles ignore the last two lines which basically are  return None . Also the first two lines are loading the global values  a  and  index .", "score": 197}}
{"question": "How to remove an element from a list by index", "tags": ["python", "list", "indexing"], "link": "https://stackoverflow.com/questions/627435/how-to-remove-an-element-from-a-list-by-index", "answer_count": 18, "answers": {"id": 627441, "body": "You probably want  pop :   a = ['a', 'b', 'c', 'd'] a.pop(1)  # now a is ['a', 'c', 'd']    By default,  pop  without any arguments removes the last item:   a = ['a', 'b', 'c', 'd'] a.pop()  # now a is ['a', 'b', 'c']", "score": 958}}
{"question": "How to remove an element from a list by index", "tags": ["python", "list", "indexing"], "link": "https://stackoverflow.com/questions/627435/how-to-remove-an-element-from-a-list-by-index", "answer_count": 18, "answers": {"id": 627453, "body": "Use  del  and specify the index of the element you want to delete:   >>> a = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] >>> del a[-1] >>> a [0, 1, 2, 3, 4, 5, 6, 7, 8]    Also supports slices:   >>> del a[2:4] >>> a [0, 1, 4, 5, 6, 7, 8, 9]    Here  is the section from the tutorial.", "score": 2602}}
{"question": "How can I randomly select (choose) an item from a list (get a random element)?", "tags": ["python", "list", "random"], "link": "https://stackoverflow.com/questions/306400/how-can-i-randomly-select-choose-an-item-from-a-list-get-a-random-element", "answer_count": 18, "answers": {"id": 12373205, "body": "If you also need the index, use  random.randrange   from random import randrange random_index = randrange(len(foo)) print(foo[random_index])", "score": 194}}
{"question": "How can I randomly select (choose) an item from a list (get a random element)?", "tags": ["python", "list", "random"], "link": "https://stackoverflow.com/questions/306400/how-can-i-randomly-select-choose-an-item-from-a-list-get-a-random-element", "answer_count": 18, "answers": {"id": 30488952, "body": "If you want to randomly select more than one item from a list, or select an item from a set, I'd recommend using  random.sample  instead.   import random group_of_items = {'a', 'b', 'c', 'd', 'e'}  # a sequence or set will work here. num_to_select = 2                           # set the number to select here. list_of_random_items = random.sample(group_of_items, num_to_select) first_random_item = list_of_random_items[0] second_random_item = list_of_random_items[1]     If you're only pulling a single item from a list though, choice is less clunky, as using sample would have the syntax  random.sample(some_list, 1)[0]  instead of  random.choice(some_list) .   Unfortunately though, choice only works for a single output from sequences (such as lists or tuples).  Though  random.choice(tuple(some_set))  may be an option for getting a single item from a set.   EDIT: Using Secrets   As many have pointed out, if you require more secure pseudorandom samples, you should use the secrets module:   import secrets                              # imports secure module. secure_random = secrets.SystemRandom()      # creates a secure random object. group_of_items = {'a', 'b', 'c', 'd', 'e'}  # a sequence or set will work here. num_to_select = 2                           # set the number to select here. list_of_random_items = secure_random.sample(group_of_items, num_to_select) first_random_item = list_of_random_items[0] second_random_item = list_of_random_items[1]    EDIT: Pythonic One-Liner   If you want a more pythonic one-liner for selecting multiple items, you can use unpacking.   import random first_random_item, second_random_item = random.sample({'a', 'b', 'c', 'd', 'e'}, 2)", "score": 300}}
{"question": "How can I randomly select (choose) an item from a list (get a random element)?", "tags": ["python", "list", "random"], "link": "https://stackoverflow.com/questions/306400/how-can-i-randomly-select-choose-an-item-from-a-list-get-a-random-element", "answer_count": 18, "answers": {"id": 306417, "body": "Use  random.choice() :   import random  foo = ['a', 'b', 'c', 'd', 'e'] print(random.choice(foo))    For  cryptographically secure  random choices (e.g., for generating a passphrase from a wordlist), use  secrets.choice() :   import secrets  foo = ['battery', 'correct', 'horse', 'staple'] print(secrets.choice(foo))    secrets  is new in Python 3.6. On older versions of Python you can use the  random.SystemRandom  class:   import random  secure_random = random.SystemRandom() print(secure_random.choice(foo))", "score": 3522}}
{"question": "Limiting floats to two decimal points", "tags": ["python", "floating-point", "rounding", "precision"], "link": "https://stackoverflow.com/questions/455612/limiting-floats-to-two-decimal-points", "answer_count": 36, "answers": {"id": 41407651, "body": "The built-in  round()  works just fine in Python 2.7 or later.   Example:   >>> round(14.22222223, 2) 14.22    Check out  the documentation .", "score": 447}}
{"question": "Limiting floats to two decimal points", "tags": ["python", "floating-point", "rounding", "precision"], "link": "https://stackoverflow.com/questions/455612/limiting-floats-to-two-decimal-points", "answer_count": 36, "answers": {"id": 6539677, "body": "There are new format specifications,  String Format Specification Mini-Language :   You can do the same as:   \"{:.2f}\".format(13.949999999999999)    Note 1:  the above returns a string. In order to get as float, simply wrap with  float(...) :   float(\"{:.2f}\".format(13.949999999999999))    Note 2:  wrapping with  float()  doesn't change anything:   >>> x = 13.949999999999999999 >>> x 13.95 >>> g = float(\"{:.2f}\".format(x)) >>> g 13.95 >>> x == g True >>> h = round(x, 2) >>> h 13.95 >>> x == h True", "score": 831}}
{"question": "Limiting floats to two decimal points", "tags": ["python", "floating-point", "rounding", "precision"], "link": "https://stackoverflow.com/questions/455612/limiting-floats-to-two-decimal-points", "answer_count": 36, "answers": {"id": 455634, "body": "You are running into the  old problem  with floating point numbers that not all numbers can be represented exactly. The command line is just showing you the full floating point form from memory.   With floating point representation, your rounded version is the same number. Since computers are binary, they store floating point numbers as an integer and then divide it by a power of two so 13.95 will be represented in a similar fashion to 125650429603636838/(2**53).   Double precision numbers have 53 bits (16 digits) of precision and regular floats have 24 bits (8 digits) of precision. The  floating point type in Python uses double precision  to store the values.   For example,   >>> 125650429603636838/(2**53) 13.949999999999999  >>> 234042163/(2**24) 13.949999988079071  >>> a = 13.946 >>> print(a) 13.946 >>> print(\"%.2f\" % a) 13.95 >>> round(a,2) 13.949999999999999 >>> print(\"%.2f\" % round(a, 2)) 13.95 >>> print(\"{:.2f}\".format(a)) 13.95 >>> print(\"{:.2f}\".format(round(a, 2))) 13.95 >>> print(\"{:.15f}\".format(round(a, 2))) 13.949999999999999    If you are after only two decimal places (to display a currency value, for example), then you have a couple of better choices:     Use integers and store values in cents, not dollars and then divide by 100 to convert to dollars.   Or use a fixed point number like  decimal .", "score": 2321}}
{"question": "Calling a function of a module by using its name (a string)", "tags": ["python", "object", "reflection"], "link": "https://stackoverflow.com/questions/3061/calling-a-function-of-a-module-by-using-its-name-a-string", "answer_count": 19, "answers": {"id": 4605, "body": "Based on  Patrick's solution , to get the module dynamically as well, import it using:   module = __import__('foo') func = getattr(module, 'bar') func()", "score": 485}}
{"question": "Calling a function of a module by using its name (a string)", "tags": ["python", "object", "reflection"], "link": "https://stackoverflow.com/questions/3061/calling-a-function-of-a-module-by-using-its-name-a-string", "answer_count": 19, "answers": {"id": 834451, "body": "Using  locals() , which returns a dictionary with the current local symbol table:   locals()[\"myfunction\"]()      Using  globals() , which returns a dictionary with the global symbol table:   globals()[\"myfunction\"]()", "score": 794}}
{"question": "Calling a function of a module by using its name (a string)", "tags": ["python", "object", "reflection"], "link": "https://stackoverflow.com/questions/3061/calling-a-function-of-a-module-by-using-its-name-a-string", "answer_count": 19, "answers": {"id": 3071, "body": "Given a module  foo  with method  bar :   import foo bar = getattr(foo, 'bar') result = bar()    getattr  can similarly be used on class instance bound methods, module-level methods, class methods... the list goes on.", "score": 2971}}
{"question": "How to print without a newline or space", "tags": ["python", "trailing-newline"], "link": "https://stackoverflow.com/questions/493386/how-to-print-without-a-newline-or-space", "answer_count": 30, "answers": {"id": 493500, "body": "Note: The title of this question used to be something like \"How to printf in Python\"   Since people may come here looking for it based on the title, Python also supports printf-style substitution:   >>> strings = [ \"one\", \"two\", \"three\" ] >>> >>> for i in xrange(3): ...     print \"Item %d: %s\" % (i, strings[i]) ... Item 0: one Item 1: two Item 2: three    And, you can handily multiply string values:   >>> print \".\" * 10 ..........", "score": 177}}
{"question": "How to print without a newline or space", "tags": ["python", "trailing-newline"], "link": "https://stackoverflow.com/questions/493386/how-to-print-without-a-newline-or-space", "answer_count": 30, "answers": {"id": 11685717, "body": "For Python 2 and earlier, it should be as simple as described in  Re: How does one print without a CR?  by  Guido van Rossum  (paraphrased):     Is it possible to print something, but not automatically have a carriage return appended to it?     Yes, append a comma after the last argument to print. For instance, this loop prints the numbers 0..9 on a line separated by spaces. Note the parameterless \"print\" that adds the final newline:   >>> for i in range(10): ...     print i, ... else: ...     print ... 0 1 2 3 4 5 6 7 8 9 >>>", "score": 301}}
{"question": "How to print without a newline or space", "tags": ["python", "trailing-newline"], "link": "https://stackoverflow.com/questions/493386/how-to-print-without-a-newline-or-space", "answer_count": 30, "answers": {"id": 493399, "body": "In Python 3, you can use the  sep=  and  end=  parameters of the  print  function:   To not add a newline to the end of the string:   print('.', end='')    To not add a space between all the function arguments you want to print:   print('a', 'b', 'c', sep='')    You can pass any string to either parameter, and you can use both parameters at the same time.   If you are having trouble with buffering, you can flush the output by adding  flush=True  keyword argument:   print('.', end='', flush=True)    Python 2.6 and 2.7   From Python 2.6 you can either import the  print  function from Python 3 using the  __future__  module :   from __future__ import print_function    which allows you to use the Python 3 solution above.   However, note that the  flush  keyword is not available in the version of the  print  function imported from  __future__  in Python 2; it only works in Python 3, more specifically 3.3 and later. In earlier versions you'll still need to flush manually with a call to  sys.stdout.flush() . You'll also have to rewrite all other print statements in the file where you do this import.   Or you can use  sys.stdout.write()   import sys sys.stdout.write('.')    You may also need to call   sys.stdout.flush()    to ensure  stdout  is flushed immediately.", "score": 3393}}
{"question": "How can I check if an object has an attribute?", "tags": ["python", "class", "object", "attributes", "attributeerror"], "link": "https://stackoverflow.com/questions/610883/how-can-i-check-if-an-object-has-an-attribute", "answer_count": 16, "answers": {"id": 611708, "body": "You can use  hasattr()  or catch  AttributeError , but if you really just want the value of the attribute with a default if it isn't there, the best option is just to use  getattr() :   getattr(a, 'property', 'default value')", "score": 686}}
{"question": "How can I check if an object has an attribute?", "tags": ["python", "class", "object", "attributes", "attributeerror"], "link": "https://stackoverflow.com/questions/610883/how-can-i-check-if-an-object-has-an-attribute", "answer_count": 16, "answers": {"id": 610923, "body": "As  Jarret Hardie answered ,  hasattr  will do the trick.  I would like to add, though, that many in the Python community recommend a strategy of \"easier to ask for forgiveness than permission\" (EAFP) rather than \"look before you leap\" (LBYL).  See these references:   EAFP vs LBYL (was Re: A little disappointed so far)   EAFP vs. LBYL @Code Like a Pythonista: Idiomatic Python   That is,   try:     doStuff(a.property) except AttributeError:     otherStuff()    ... is preferred to:   if hasattr(a, 'property'):     doStuff(a.property) else:     otherStuff()", "score": 834}}
{"question": "How can I check if an object has an attribute?", "tags": ["python", "class", "object", "attributes", "attributeerror"], "link": "https://stackoverflow.com/questions/610883/how-can-i-check-if-an-object-has-an-attribute", "answer_count": 16, "answers": {"id": 610893, "body": "Try  hasattr() :   if hasattr(a, 'property'):     a.property    See  zweiterlinde's answer , which offers good advice about asking forgiveness! It is a very Pythonic approach!   The general practice in Python is that, if the property is likely to be there most of the time, simply call it and either let the exception propagate, or trap it with a try/except block. This will likely be faster than  hasattr . If the property is likely to not be there most of the time, or you're not sure, using  hasattr  will probably be faster than repeatedly falling into an exception block.", "score": 3508}}
{"question": "How do I lowercase a string in Python?", "tags": ["python", "string", "uppercase", "lowercase"], "link": "https://stackoverflow.com/questions/6797984/how-do-i-lowercase-a-string-in-python", "answer_count": 10, "answers": {"id": 26175350, "body": "With Python 2, this doesn't work for non-English words in UTF-8. In this case  decode('utf-8')  can help:   >>> s='\u041a\u0438\u043b\u043e\u043c\u0435\u0442\u0440' >>> print s.lower() \u041a\u0438\u043b\u043e\u043c\u0435\u0442\u0440 >>> print s.decode('utf-8').lower() \u043a\u0438\u043b\u043e\u043c\u0435\u0442\u0440", "score": 211}}
{"question": "How do I lowercase a string in Python?", "tags": ["python", "string", "uppercase", "lowercase"], "link": "https://stackoverflow.com/questions/6797984/how-do-i-lowercase-a-string-in-python", "answer_count": 10, "answers": {"id": 31599276, "body": "The canonical Pythonic way of doing this is   >>> 'Kilometers'.lower() 'kilometers'    However, if the purpose is to do case insensitive matching, you should use case-folding:   >>> 'Kilometers'.casefold() 'kilometers'    Here's why:   >>> \"Ma\u00dfe\".casefold() 'masse' >>> \"Ma\u00dfe\".lower() 'ma\u00dfe' >>> \"MASSE\" == \"Ma\u00dfe\" False >>> \"MASSE\".lower() == \"Ma\u00dfe\".lower() False >>> \"MASSE\".casefold() == \"Ma\u00dfe\".casefold() True    This is a str method in Python 3, but in Python 2, you'll want to look at the PyICU or py2casefold -  several answers address this here .   Unicode Python 3   Python 3  handles plain string literals as unicode:   >>> string = '\u041a\u0438\u043b\u043e\u043c\u0435\u0442\u0440' >>> string '\u041a\u0438\u043b\u043e\u043c\u0435\u0442\u0440' >>> string.lower() '\u043a\u0438\u043b\u043e\u043c\u0435\u0442\u0440'    Python 2, plain string literals are bytes   In Python 2, the below, pasted into a shell, encodes the literal as a string of bytes, using  utf-8 .   And  lower  doesn't map any changes that bytes would be aware of, so we get the same string.   >>> string = '\u041a\u0438\u043b\u043e\u043c\u0435\u0442\u0440' >>> string '\\xd0\\x9a\\xd0\\xb8\\xd0\\xbb\\xd0\\xbe\\xd0\\xbc\\xd0\\xb5\\xd1\\x82\\xd1\\x80' >>> string.lower() '\\xd0\\x9a\\xd0\\xb8\\xd0\\xbb\\xd0\\xbe\\xd0\\xbc\\xd0\\xb5\\xd1\\x82\\xd1\\x80' >>> print string.lower() \u041a\u0438\u043b\u043e\u043c\u0435\u0442\u0440    In scripts, Python will object to non-ascii (as of Python 2.5, and warning in Python 2.4) bytes being in a string with no encoding given, since the intended coding would be ambiguous. For more on that, see the Unicode how-to in the  docs  and  PEP 263   Use Unicode literals, not  str  literals   So we need a  unicode  string to handle this conversion, accomplished easily with a unicode string literal, which disambiguates with a  u  prefix (and note the  u  prefix also works in Python 3):   >>> unicode_literal = u'\u041a\u0438\u043b\u043e\u043c\u0435\u0442\u0440' >>> print(unicode_literal.lower()) \u043a\u0438\u043b\u043e\u043c\u0435\u0442\u0440    Note that the bytes are completely different from the  str  bytes - the escape character is  '\\u'  followed by the 2-byte width, or 16 bit representation of these  unicode  letters:   >>> unicode_literal u'\\u041a\\u0438\\u043b\\u043e\\u043c\\u0435\\u0442\\u0440' >>> unicode_literal.lower() u'\\u043a\\u0438\\u043b\\u043e\\u043c\\u0435\\u0442\\u0440'    Now if we only have it in the form of a  str , we need to convert it to  unicode . Python's Unicode type is a universal encoding format that has many  advantages  relative to most other encodings. We can either use the  unicode  constructor or  str.decode  method with the codec to convert the  str  to  unicode :   >>> unicode_from_string = unicode(string, 'utf-8') # \"encoding\" unicode from string >>> print(unicode_from_string.lower()) \u043a\u0438\u043b\u043e\u043c\u0435\u0442\u0440 >>> string_to_unicode = string.decode('utf-8')  >>> print(string_to_unicode.lower()) \u043a\u0438\u043b\u043e\u043c\u0435\u0442\u0440 >>> unicode_from_string == string_to_unicode == unicode_literal True    Both methods convert to the unicode type - and same as the unicode_literal.   Best Practice, use Unicode   It is recommended that you always  work with text in Unicode .     Software should only work with Unicode strings internally, converting to a particular encoding on output.     Can encode back when necessary   However, to get the lowercase back in type  str , encode the python string to  utf-8  again:   >>> print string \u041a\u0438\u043b\u043e\u043c\u0435\u0442\u0440 >>> string '\\xd0\\x9a\\xd0\\xb8\\xd0\\xbb\\xd0\\xbe\\xd0\\xbc\\xd0\\xb5\\xd1\\x82\\xd1\\x80' >>> string.decode('utf-8') u'\\u041a\\u0438\\u043b\\u043e\\u043c\\u0435\\u0442\\u0440' >>> string.decode('utf-8').lower() u'\\u043a\\u0438\\u043b\\u043e\\u043c\\u0435\\u0442\\u0440' >>> string.decode('utf-8').lower().encode('utf-8') '\\xd0\\xba\\xd0\\xb8\\xd0\\xbb\\xd0\\xbe\\xd0\\xbc\\xd0\\xb5\\xd1\\x82\\xd1\\x80' >>> print string.decode('utf-8').lower().encode('utf-8') \u043a\u0438\u043b\u043e\u043c\u0435\u0442\u0440    So in Python 2, Unicode can encode into Python strings, and Python strings can decode into the Unicode type.", "score": 441}}
{"question": "How do I lowercase a string in Python?", "tags": ["python", "string", "uppercase", "lowercase"], "link": "https://stackoverflow.com/questions/6797984/how-do-i-lowercase-a-string-in-python", "answer_count": 10, "answers": {"id": 6797990, "body": "Use  str.lower() :   \"Kilometer\".lower()", "score": 3649}}
{"question": "Class (static) variables and methods", "tags": ["python", "class", "static", "class-variables"], "link": "https://stackoverflow.com/questions/68645/class-static-variables-and-methods", "answer_count": 27, "answers": {"id": 27568860, "body": "Static and Class Methods   As the other answers have noted, static and class methods are easily accomplished using the built-in decorators:   class Test(object):      # regular instance method:     def my_method(self):         pass      # class method:     @classmethod     def my_class_method(cls):         pass      # static method:     @staticmethod     def my_static_method():         pass    As usual, the first argument to  my_method()  is bound to the class instance object. In contrast, the first argument to  my_class_method()  is  bound to the class object itself  (e.g., in this case,  Test ). For  my_static_method() , none of the arguments are bound, and having arguments at all is optional.   \"Static Variables\"   However, implementing \"static variables\" (well,  mutable  static variables, anyway, if that's not a contradiction in terms...) is not as straight forward. As millerdev  pointed out in his answer , the problem is that Python's class attributes are not truly \"static variables\". Consider:   class Test(object):     i = 3  # This is a class attribute  x = Test() x.i = 12   # Attempt to change the value of the class attribute using x instance assert x.i == Test.i  # ERROR assert Test.i == 3    # Test.i was not affected assert x.i == 12      # x.i is a different object than Test.i    This is because the line  x.i = 12  has added a new instance attribute  i  to  x  instead of changing the value of the  Test  class  i  attribute.   Partial  expected static variable behavior, i.e., syncing of the attribute between multiple instances (but  not  with the class itself; see \"gotcha\" below), can be achieved by turning the class attribute into a property:   class Test(object):      _i = 3      @property     def i(self):         return type(self)._i      @i.setter     def i(self,val):         type(self)._i = val  ## ALTERNATIVE IMPLEMENTATION - FUNCTIONALLY EQUIVALENT TO ABOVE ## ## (except with separate methods for getting and setting i) ##  class Test(object):      _i = 3      def get_i(self):         return type(self)._i      def set_i(self,val):         type(self)._i = val      i = property(get_i, set_i)    Now you can do:   x1 = Test() x2 = Test() x1.i = 50 assert x2.i == x1.i  # no error assert x2.i == 50    # the property is synced    The static variable will now remain in sync  between all class instances .   (NOTE: That is, unless a class instance decides to define its own version of  _i ! But if someone decides to do THAT, they deserve what they get, don't they???)   Note that technically speaking,  i  is still not a 'static variable' at all; it is a  property , which is a special type of descriptor. However, the  property  behavior is now equivalent to a (mutable) static variable synced across all class instances.   Immutable \"Static Variables\"   For immutable static variable behavior, simply omit the  property  setter:   class Test(object):      _i = 3      @property     def i(self):         return type(self)._i  ## ALTERNATIVE IMPLEMENTATION - FUNCTIONALLY EQUIVALENT TO ABOVE ## ## (except with separate methods for getting i) ##  class Test(object):      _i = 3      def get_i(self):         return type(self)._i      i = property(get_i)    Now attempting to set the instance  i  attribute will return an  AttributeError :   x = Test() assert x.i == 3  # success x.i = 12         # ERROR    One Gotcha to be Aware of   Note that the above methods only work with  instances  of your class - they will  not  work  when using the class itself . So for example:   x = Test() assert x.i == Test.i  # ERROR  # x.i and Test.i are two different objects: type(Test.i)  # class 'property' type(x.i)     # class 'int'    The line  assert Test.i == x.i  produces an error, because the  i  attribute of  Test  and  x  are two different objects.   Many people will find this surprising. However, it should not be. If we go back and inspect our  Test  class definition (the second version), we take note of this line:       i = property(get_i)     Clearly, the member  i  of  Test  must be a  property  object, which is the type of object returned from the  property  function.   If you find the above confusing, you are most likely still thinking about it from the perspective of other languages (e.g. Java or c++). You should go study the  property  object, about the order in which Python attributes are returned, the descriptor protocol, and the method resolution order (MRO).   I present a solution to the above 'gotcha' below; however I would suggest - strenuously - that you do not try to do something like the following until - at minimum - you thoroughly understand why  assert Test.i = x.i  causes an error.   REAL, ACTUAL  Static Variables -  Test.i == x.i   I present the (Python 3) solution below for informational purposes only. I am not endorsing it as a \"good solution\". I have my doubts as to whether emulating the static variable behavior of other languages in Python is ever actually necessary. However, regardless as to whether it is actually useful, the below should help further understanding of how Python works.   UPDATE: this attempt  is really pretty awful ; if you insist on doing something like this (hint: please don't; Python is a very elegant language and shoe-horning it into behaving like another language is just not necessary), use the code in  Ethan Furman's answer  instead.   Emulating static variable behavior of other languages using a metaclass   A metaclass is the class of a class. The default metaclass for all classes in Python (i.e., the \"new style\" classes post Python 2.3 I believe) is  type . For example:   type(int)  # class 'type' type(str)  # class 'type' class Test(): pass type(Test) # class 'type'    However, you can define your own metaclass like this:   class MyMeta(type): pass    And apply it to your own class like this (Python 3 only):   class MyClass(metaclass = MyMeta):     pass  type(MyClass)  # class MyMeta    Below is a metaclass I have created which attempts to emulate \"static variable\" behavior of other languages. It basically works by replacing the default getter, setter, and deleter with versions which check to see if the attribute being requested is a \"static variable\".   A catalog of the \"static variables\" is stored in the  StaticVarMeta.statics  attribute. All attribute requests are initially attempted to be resolved using a substitute resolution order. I have dubbed this the \"static resolution order\", or \"SRO\". This is done by looking for the requested attribute in the set of \"static variables\" for a given class (or its parent classes). If the attribute does not appear in the \"SRO\", the class will fall back on the default attribute get/set/delete behavior (i.e., \"MRO\").   from functools import wraps  class StaticVarsMeta(type):     '''A metaclass for creating classes that emulate the \"static variable\" behavior     of other languages. I do not advise actually using this for anything!!!          Behavior is intended to be similar to classes that use __slots__. However, \"normal\"     attributes and __statics___ can coexist (unlike with __slots__).           Example usage:                   class MyBaseClass(metaclass = StaticVarsMeta):             __statics__ = {'a','b','c'}             i = 0  # regular attribute             a = 1  # static var defined (optional)                      class MyParentClass(MyBaseClass):             __statics__ = {'d','e','f'}             j = 2              # regular attribute             d, e, f = 3, 4, 5  # Static vars             a, b, c = 6, 7, 8  # Static vars (inherited from MyBaseClass, defined/re-defined here)                      class MyChildClass(MyParentClass):             __statics__ = {'a','b','c'}             j = 2  # regular attribute (redefines j from MyParentClass)             d, e, f = 9, 10, 11   # Static vars (inherited from MyParentClass, redefined here)             a, b, c = 12, 13, 14  # Static vars (overriding previous definition in MyParentClass here)'''     statics = {}     def __new__(mcls, name, bases, namespace):         # Get the class object         cls = super().__new__(mcls, name, bases, namespace)         # Establish the \"statics resolution order\"         cls.__sro__ = tuple(c for c in cls.__mro__ if isinstance(c,mcls))                                  # Replace class getter, setter, and deleter for instance attributes         cls.__getattribute__ = StaticVarsMeta.__inst_getattribute__(cls, cls.__getattribute__)         cls.__setattr__ = StaticVarsMeta.__inst_setattr__(cls, cls.__setattr__)         cls.__delattr__ = StaticVarsMeta.__inst_delattr__(cls, cls.__delattr__)         # Store the list of static variables for the class object         # This list is permanent and cannot be changed, similar to __slots__         try:             mcls.statics[cls] = getattr(cls,'__statics__')         except AttributeError:             mcls.statics[cls] = namespace['__statics__'] = set() # No static vars provided         # Check and make sure the statics var names are strings         if any(not isinstance(static,str) for static in mcls.statics[cls]):             typ = dict(zip((not isinstance(static,str) for static in mcls.statics[cls]), map(type,mcls.statics[cls])))[True].__name__             raise TypeError('__statics__ items must be strings, not {0}'.format(typ))         # Move any previously existing, not overridden statics to the static var parent class(es)         if len(cls.__sro__) > 1:             for attr,value in namespace.items():                 if attr not in StaticVarsMeta.statics[cls] and attr != ['__statics__']:                     for c in cls.__sro__[1:]:                         if attr in StaticVarsMeta.statics[c]:                             setattr(c,attr,value)                             delattr(cls,attr)         return cls     def __inst_getattribute__(self, orig_getattribute):         '''Replaces the class __getattribute__'''         @wraps(orig_getattribute)         def wrapper(self, attr):             if StaticVarsMeta.is_static(type(self),attr):                 return StaticVarsMeta.__getstatic__(type(self),attr)             else:                 return orig_getattribute(self, attr)         return wrapper     def __inst_setattr__(self, orig_setattribute):         '''Replaces the class __setattr__'''         @wraps(orig_setattribute)         def wrapper(self, attr, value):             if StaticVarsMeta.is_static(type(self),attr):                 StaticVarsMeta.__setstatic__(type(self),attr, value)             else:                 orig_setattribute(self, attr, value)         return wrapper     def __inst_delattr__(self, orig_delattribute):         '''Replaces the class __delattr__'''         @wraps(orig_delattribute)         def wrapper(self, attr):             if StaticVarsMeta.is_static(type(self),attr):                 StaticVarsMeta.__delstatic__(type(self),attr)             else:                 orig_delattribute(self, attr)         return wrapper     def __getstatic__(cls,attr):         '''Static variable getter'''         for c in cls.__sro__:             if attr in StaticVarsMeta.statics[c]:                 try:                     return getattr(c,attr)                 except AttributeError:                     pass         raise AttributeError(cls.__name__ + \" object has no attribute '{0}'\".format(attr))     def __setstatic__(cls,attr,value):         '''Static variable setter'''         for c in cls.__sro__:             if attr in StaticVarsMeta.statics[c]:                 setattr(c,attr,value)                 break     def __delstatic__(cls,attr):         '''Static variable deleter'''         for c in cls.__sro__:             if attr in StaticVarsMeta.statics[c]:                 try:                     delattr(c,attr)                     break                 except AttributeError:                     pass         raise AttributeError(cls.__name__ + \" object has no attribute '{0}'\".format(attr))     def __delattr__(cls,attr):         '''Prevent __sro__ attribute from deletion'''         if attr == '__sro__':             raise AttributeError('readonly attribute')         super().__delattr__(attr)     def is_static(cls,attr):         '''Returns True if an attribute is a static variable of any class in the __sro__'''         if any(attr in StaticVarsMeta.statics[c] for c in cls.__sro__):             return True         return False", "score": 299}}
{"question": "Class (static) variables and methods", "tags": ["python", "class", "static", "class-variables"], "link": "https://stackoverflow.com/questions/68645/class-static-variables-and-methods", "answer_count": 27, "answers": {"id": 69067, "body": "@Blair Conrad said static variables declared inside the class definition, but not inside a method are class or \"static\" variables:   >>> class Test(object): ...     i = 3 ... >>> Test.i 3    There are a few gotcha's here. Carrying on from the example above:   >>> t = Test() >>> t.i     # \"static\" variable accessed via instance 3 >>> t.i = 5 # but if we assign to the instance ... >>> Test.i  # we have not changed the \"static\" variable 3 >>> t.i     # we have overwritten Test.i on t by creating a new attribute t.i 5 >>> Test.i = 6 # to change the \"static\" variable we do it by assigning to the class >>> t.i 5 >>> Test.i 6 >>> u = Test() >>> u.i 6           # changes to t do not affect new instances of Test  # Namespaces are one honking great idea -- let's do more of those! >>> Test.__dict__ {'i': 6, ...} >>> t.__dict__ {'i': 5} >>> u.__dict__ {}    Notice how the instance variable  t.i  got out of sync with the \"static\" class variable when the attribute  i  was set directly on  t . This is because  i  was re-bound within the  t  namespace, which is distinct from the  Test  namespace. If you want to change the value of a \"static\" variable, you must change it within the scope (or object) where it was originally defined. I put \"static\" in quotes because Python does not really have static variables in the sense that C++ and Java do.   Although it doesn't say anything specific about static variables or methods, the  Python tutorial  has some relevant information on  classes and class objects .    @Steve Johnson also answered regarding static methods, also documented under \"Built-in Functions\" in the Python Library Reference.   class Test(object):     @staticmethod     def f(arg1, arg2, ...):         ...    @beid also mentioned classmethod, which is similar to staticmethod. A classmethod's first argument is the class object. Example:   class Test(object):     i = 3 # class (or static) variable     @classmethod     def g(cls, arg):         # here we can use 'cls' instead of the class name (Test)         if arg > cls.i:             cls.i = arg # would be the same as Test.i = arg1", "score": 783}}
{"question": "Class (static) variables and methods", "tags": ["python", "class", "static", "class-variables"], "link": "https://stackoverflow.com/questions/68645/class-static-variables-and-methods", "answer_count": 27, "answers": {"id": 68672, "body": "Variables declared inside the class definition, but not inside a method are class or static variables:   >>> class MyClass: ...     i = 3 ... >>> MyClass.i 3     As @ millerdev  points out, this creates a class-level  i  variable, but this is distinct from any instance-level  i  variable, so you could have   >>> m = MyClass() >>> m.i = 4 >>> MyClass.i, m.i >>> (3, 4)    This is different from C++ and Java, but not so different from C#, where a static member can't be accessed using a reference to an instance.   See  what the Python tutorial has to say on the subject of classes and class objects .   @Steve Johnson has already answered regarding  static methods , also documented under  \"Built-in Functions\" in the Python Library Reference .   class C:     @staticmethod     def f(arg1, arg2, ...): ...    @beidy recommends  classmethod s over staticmethod, as the method then receives the class type as the first argument.", "score": 2392}}
{"question": "Importing files from different folder", "tags": ["python", "importerror", "python-import"], "link": "https://stackoverflow.com/questions/4383571/importing-files-from-different-folder", "answer_count": 41, "answers": {"id": 40612922, "body": "When modules are in parallel locations, as in the question:   application/app2/some_folder/some_file.py application/app2/another_folder/another_file.py    This shorthand makes one module visible to the other:   import sys sys.path.append('../')", "score": 264}}
{"question": "Importing files from different folder", "tags": ["python", "importerror", "python-import"], "link": "https://stackoverflow.com/questions/4383571/importing-files-from-different-folder", "answer_count": 41, "answers": {"id": 21995949, "body": "There is nothing wrong with:   from application.app.folder.file import func_name    Just make sure  folder  also contains an  __init__.py . This allows it to be included as a package. I am not sure why the other answers talk about  PYTHONPATH .", "score": 1404}}
{"question": "Importing files from different folder", "tags": ["python", "importerror", "python-import"], "link": "https://stackoverflow.com/questions/4383571/importing-files-from-different-folder", "answer_count": 41, "answers": {"id": 4383597, "body": "Note: This answer was intended for a very specific question. For most programmers coming here from a search engine, this is not the answer you are looking for. Typically you would structure your files into packages (see other answers) instead of modifying the search path.     By default, you can't. When importing a file, Python only searches the directory that the entry-point script is running from and  sys.path  which includes locations such as the package installation directory (it's actually  a little more complex  than this, but this covers most cases).   However, you can add to the Python path at runtime:   # some_file.py import sys # caution: path[0] is reserved for script path (or '' in REPL) sys.path.insert(1, '/path/to/application/app/folder')  import file", "score": 2338}}
{"question": "Check if a given key already exists in a dictionary", "tags": ["python", "dictionary"], "link": "https://stackoverflow.com/questions/1602934/check-if-a-given-key-already-exists-in-a-dictionary", "answer_count": 16, "answers": {"id": 1602990, "body": "You can test for the presence of a key in a dictionary, using the  in  keyword:   d = {'a': 1, 'b': 2} 'a' in d # <== evaluates to True 'c' in d # <== evaluates to False    A common use for checking the existence of a key in a dictionary before mutating it is to default-initialize the value (e.g. if your values are lists, for example, and you want to ensure that there is an empty list to which you can append when inserting the first value for a key). In cases such as those, you may find the  collections.defaultdict()  type to be of interest.   In older code, you may also find some uses of  has_key() , a deprecated method for checking the existence of keys in dictionaries (just use  key_name in dict_name , instead).", "score": 299}}
{"question": "Check if a given key already exists in a dictionary", "tags": ["python", "dictionary"], "link": "https://stackoverflow.com/questions/1602934/check-if-a-given-key-already-exists-in-a-dictionary", "answer_count": 16, "answers": {"id": 1602944, "body": "Use  key in my_dict  directly instead of  key in my_dict.keys() :   if 'key1' in my_dict:     print(\"blah\") else:     print(\"boo\")    That will be much  faster  as it uses the dictionary's O(1) hashing as opposed to doing an O(n) linear search on a list of keys.", "score": 1932}}
{"question": "Check if a given key already exists in a dictionary", "tags": ["python", "dictionary"], "link": "https://stackoverflow.com/questions/1602934/check-if-a-given-key-already-exists-in-a-dictionary", "answer_count": 16, "answers": {"id": 1602964, "body": "in  tests for the existence of a key in a  dict :   d = {\"key1\": 10, \"key2\": 23}  if \"key1\" in d:     print(\"this will execute\")  if \"nonexistent key\" in d:     print(\"this will not\")      Use  dict.get()  to provide a default value when the key does not exist:   d = {}  for i in range(100):     key = i % 10     d[key] = d.get(key, 0) + 1      To provide a default value for  every  key, either use  dict.setdefault()  on each assignment:   d = {}  for i in range(100):     d[i % 10] = d.setdefault(i % 10, 0) + 1        ...or better, use  defaultdict  from the  collections  module:   from collections import defaultdict  d = defaultdict(int)  for i in range(100):     d[i % 10] += 1", "score": 5669}}
{"question": "How do I escape curly-brace ({}) characters characters in a string while using .format?", "tags": ["python", "string", "format", "string-formatting", "curly-braces"], "link": "https://stackoverflow.com/questions/5466451/how-do-i-escape-curly-brace-characters-characters-in-a-string-while-using", "answer_count": 24, "answers": {"id": 5466473, "body": "You escape it by doubling the braces.   Eg:   x = \"{{ Hello }} {0}\" print(x.format(42))", "score": 121}}
{"question": "How do I escape curly-brace ({}) characters characters in a string while using .format?", "tags": ["python", "string", "format", "string-formatting", "curly-braces"], "link": "https://stackoverflow.com/questions/5466451/how-do-i-escape-curly-brace-characters-characters-in-a-string-while-using", "answer_count": 24, "answers": {"id": 47923703, "body": "Python 3.6+ (2017)   In the recent versions of Python one would use  f-strings  (see also  PEP498 ).   With f-strings one should use double  {{  or  }}   n = 42   print(f\" {{Hello}} {n} \")    produces the desired    {Hello} 42    If you need to resolve an expression in the brackets instead of using literal text you'll need three sets of brackets:   hello = \"HELLO\" print(f\"{{{hello.lower()}}}\")    produces   {hello}", "score": 309}}
{"question": "How do I escape curly-brace ({}) characters characters in a string while using .format?", "tags": ["python", "string", "format", "string-formatting", "curly-braces"], "link": "https://stackoverflow.com/questions/5466451/how-do-i-escape-curly-brace-characters-characters-in-a-string-while-using", "answer_count": 24, "answers": {"id": 5466478, "body": "You need to double the  {{  and  }} :   >>> x = \" {{ Hello }} {0} \" >>> print(x.format(42)) ' { Hello } 42 '    Here's the relevant part of the  Python documentation for format string syntax :     Format strings contain \u201creplacement fields\u201d surrounded by curly braces  {} . Anything that is not contained in braces is considered literal text, which is copied unchanged to the output. If you need to include a brace character in the literal text, it can be escaped by doubling:  {{  and  }} .", "score": 3532}}
{"question": "How do I get a substring of a string in Python?", "tags": ["python", "string", "substring"], "link": "https://stackoverflow.com/questions/663171/how-do-i-get-a-substring-of-a-string-in-python", "answer_count": 16, "answers": {"id": 11808384, "body": "Substr() normally (i.e. PHP and Perl) works this way:    s = Substr(s, beginning, LENGTH)    So the parameters are  beginning  and  LENGTH .   But Python's behaviour is different; it expects beginning and one after END (!).  This is difficult to spot by beginners.  So the correct replacement for Substr(s, beginning, LENGTH) is   s = s[ beginning : beginning + LENGTH]", "score": 174}}
{"question": "How do I get a substring of a string in Python?", "tags": ["python", "string", "substring"], "link": "https://stackoverflow.com/questions/663171/how-do-i-get-a-substring-of-a-string-in-python", "answer_count": 16, "answers": {"id": 9780082, "body": "Just for completeness as nobody else has mentioned it.  The third parameter to an array slice is a step.  So reversing a string is as simple as:   some_string[::-1]    Or selecting alternate characters would be:   \"H-e-l-l-o- -W-o-r-l-d\"[::2] # outputs \"Hello World\"    The ability to step forwards and backwards through the string maintains consistency with being able to array slice from the start or end.", "score": 500}}
{"question": "How do I get a substring of a string in Python?", "tags": ["python", "string", "substring"], "link": "https://stackoverflow.com/questions/663171/how-do-i-get-a-substring-of-a-string-in-python", "answer_count": 16, "answers": {"id": 663175, "body": ">>> x = \"Hello World!\" >>> x[2:] 'llo World!' >>> x[:2] 'He' >>> x[:-2] 'Hello Worl' >>> x[-2:] 'd!' >>> x[2:-2] 'llo Worl'    Python calls this concept \"slicing\" and it works on more than just strings. Take a look  here  for a comprehensive introduction.", "score": 3804}}
{"question": "How do I parse a string to a float or int?", "tags": ["python", "parsing", "floating-point", "type-conversion", "integer"], "link": "https://stackoverflow.com/questions/379906/how-do-i-parse-a-string-to-a-float-or-int", "answer_count": 34, "answers": {"id": 379966, "body": "def num(s):     try:         return int(s)     except ValueError:         return float(s)", "score": 583}}
{"question": "How do I parse a string to a float or int?", "tags": ["python", "parsing", "floating-point", "type-conversion", "integer"], "link": "https://stackoverflow.com/questions/379906/how-do-i-parse-a-string-to-a-float-or-int", "answer_count": 34, "answers": {"id": 20929983, "body": "Python2 method to check if a string is a float:   def is_float(value):   if value is None:       return False   try:       float(value)       return True   except:       return False    For the Python3 version of is_float see:  Checking if a string can be converted to float in Python   A longer and more accurate name for this function could be:  is_convertible_to_float(value)   What is, and is not a float in  Python  may surprise you:   The below unit tests were done using python2.  Check it that Python3 has different behavior for what strings are convertable to float.  One confounding difference is that any number of interior underscores are now allowed:   (float(\"1_3.4\") == float(13.4))  is True   val                   is_float(val) Note --------------------  ----------   -------------------------------- \"\"                    False        Blank string \"127\"                 True         Passed string True                  True         Pure sweet Truth \"True\"                False        Vile contemptible lie False                 True         So false it becomes true \"123.456\"             True         Decimal \"      -127    \"      True         Spaces trimmed \"\\t\\n12\\r\\n\"          True         whitespace ignored \"NaN\"                 True         Not a number \"NaNanananaBATMAN\"    False        I am Batman \"-iNF\"                True         Negative infinity \"123.E4\"              True         Exponential notation \".1\"                  True         mantissa only \"1_2_3.4\"             False        Underscores not allowed \"12 34\"               False        Spaces not allowed on interior \"1,234\"               False        Commas gtfo u'\\x30'               True         Unicode is fine. \"NULL\"                False        Null is not special 0x3fade               True         Hexadecimal \"6e7777777777777\"     True         Shrunk to infinity \"1.797693e+308\"       True         This is max value \"infinity\"            True         Same as inf \"infinityandBEYOND\"   False        Extra characters wreck it \"12.34.56\"            False        Only one dot allowed u'\u56db'                 False        Japanese '4' is not a float. \"#56\"                 False        Pound sign \"56%\"                 False        Percent of what? \"0E0\"                 True         Exponential, move dot 0 places 0**0                  True         0___0  Exponentiation \"-5e-5\"               True         Raise to a negative number \"+1e1\"                True         Plus is OK with exponent \"+1e1^5\"              False        Fancy exponent not interpreted \"+1e1.3\"              False        No decimals in exponent \"-+1\"                 False        Make up your mind \"(1)\"                 False        Parenthesis is bad    You think you know what numbers are? You are not so good as you think! Not big surprise.   Don't use this code on life-critical software!   Catching broad exceptions this way, killing canaries and gobbling the exception creates a tiny chance that a valid float as string will return false.  The  float(...)  line of code can failed for any of a thousand reasons that have nothing to do with the contents of the string.  But if you're writing life-critical software in a duck-typing prototype language like Python, then you've got much larger problems.", "score": 604}}
{"question": "How do I parse a string to a float or int?", "tags": ["python", "parsing", "floating-point", "type-conversion", "integer"], "link": "https://stackoverflow.com/questions/379906/how-do-i-parse-a-string-to-a-float-or-int", "answer_count": 34, "answers": {"id": 379910, "body": ">>> a = \"545.2222\" >>> float(a) 545.22220000000004 >>> int(float(a)) 545", "score": 3106}}
{"question": "How do I install pip on Windows?", "tags": ["python", "windows", "pip"], "link": "https://stackoverflow.com/questions/4750806/how-do-i-install-pip-on-windows", "answer_count": 40, "answers": {"id": 9038397, "body": "2014 UPDATE:   1) If you have installed Python 3.4 or later, pip is included with Python and should already be working on your system.   2) If you are running a version below Python 3.4 or if pip was not installed with Python 3.4 for some reason, then you'd probably use pip's official installation script  get-pip.py . The pip installer now grabs setuptools for you, and works regardless of architecture (32-bit or 64-bit).   The installation  instructions are detailed here  and involve:     To install or upgrade pip, securely download  get-pip.py .   Then run the following (which may require administrator access):     python get-pip.py      To upgrade an existing setuptools (or distribute), run  pip install -U setuptools     I'll leave the two sets of old instructions below for posterity.   OLD Answers:   For Windows editions of the  64 bit  variety - 64-bit Windows + Python used to require a separate installation method due to ez_setup, but I've tested the new distribute method on 64-bit Windows running 32-bit Python and 64-bit Python, and you can now use the same method for all versions of Windows/Python 2.7X:   OLD Method 2  using  distribute :     Download  distribute  - I threw mine in  C:\\Python27\\Scripts  (feel free to create a  Scripts  directory if it doesn't exist.   Open up a command prompt (on Windows you should check out  conemu2  if you don't use  PowerShell ) and change ( cd ) to the directory you've downloaded  distribute_setup.py  to.   Run distribute_setup:  python distribute_setup.py  (This will not work if your python installation directory is not added to your path -  go here for help )   Change the current directory to the  Scripts  directory for your Python installation ( C:\\Python27\\Scripts ) or add that directory, as well as the Python base installation directory to your %PATH% environment variable.   Install pip using the newly installed setuptools:  easy_install pip     The last step will not work unless you're either in the directory  easy_install.exe  is located in (C:\\Python27\\Scripts would be the default for Python 2.7), or you have that directory added to your path.   OLD Method 1  using ez_setup:   from the setuptools page  --     Download  ez_setup.py  and run it; it will download the appropriate .egg file and install it for you. (Currently, the provided .exe installer does not support 64-bit versions of Python for Windows, due to a distutils installer compatibility issue.     After this, you may continue with:     Add  c:\\Python2x\\Scripts  to the Windows path (replace the  x  in  Python2x  with the actual version number you have installed)   Open a new (!) DOS prompt. From there run  easy_install pip", "score": 225}}
{"question": "How do I install pip on Windows?", "tags": ["python", "windows", "pip"], "link": "https://stackoverflow.com/questions/4750806/how-do-i-install-pip-on-windows", "answer_count": 40, "answers": {"id": 4921215, "body": "--  Outdated  -- use distribute, not setuptools as described here. --  --  Outdated #2  -- use setuptools as distribute is deprecated.   As you mentioned pip doesn't include an independent installer, but you can install it with its predecessor easy_install.   So:     Download the last pip version from here:  http://pypi.python.org/pypi/pip#downloads   Uncompress it   Download the last easy installer for Windows: ( download the .exe at the bottom of  http://pypi.python.org/pypi/setuptools  ). Install it.   copy the uncompressed pip folder  content  into  C:\\Python2x\\  folder (don't copy the whole folder into it, just the content), because python command doesn't work outside  C:\\Python2x  folder and then run:   python setup.py install   Add your python  C:\\Python2x\\Scripts  to the path     You are done.    Now you can use  pip install package  to easily install packages as in Linux :)", "score": 310}}
{"question": "How do I install pip on Windows?", "tags": ["python", "windows", "pip"], "link": "https://stackoverflow.com/questions/4750806/how-do-i-install-pip-on-windows", "answer_count": 40, "answers": {"id": 12476379, "body": "Python 3.4+ and 2.7.9+   Good news!  Python 3.4  (released March 2014) and  Python 2.7.9  (released December 2014) ship with Pip. This is the best feature of any Python release. It makes the community's wealth of libraries accessible to everyone. Newbies are no longer excluded from using community libraries by the prohibitive difficulty of setup. In shipping with a package manager, Python joins  Ruby ,  Node.js ,  Haskell ,  Perl ,  Go \u2014almost every other contemporary language with a majority open-source community. Thank you, Python.   If you do find that pip is not available, simply run  ensurepip .     On Windows:   py -3 -m ensurepip      Otherwise:   python3 -m ensurepip        Of course, that doesn't mean Python packaging is problem solved. The experience remains frustrating. I discuss this  in the Stack Overflow question  Does Python have a package/module management system? .   Python 3 \u2264 3.3 and 2 \u2264 2.7.8   Flying in the face of its  'batteries included'  motto, Python ships without a package manager. To make matters worse, Pip was\u2014until recently\u2014ironically difficult to install.   Official instructions   Per  https://pip.pypa.io/en/stable/installing/#do-i-need-to-install-pip :   Download  get-pip.py , being careful to save it as a  .py  file rather than  .txt . Then, run it from the command prompt:   python get-pip.py    You possibly need an administrator command prompt to do this. Follow  Start a Command Prompt as an Administrator  (Microsoft TechNet).   This installs the pip package, which (in Windows) contains ...\\Scripts\\pip.exe that path must be in PATH environment variable to use pip from the command line (see the second part of 'Alternative Instructions' for adding it to your PATH,   Alternative instructions   The official documentation tells users to install Pip and each of its dependencies from source. That's tedious for the experienced and prohibitively difficult for newbies.   For our sake, Christoph Gohlke prepares Windows installers ( .msi ) for popular Python packages. He builds installers for all Python versions, both 32 and 64 bit. You need to:     Install setuptools   Install pip     For me, this installed Pip at  C:\\Python27\\Scripts\\pip.exe . Find  pip.exe  on your computer, then add its folder (for example,  C:\\Python27\\Scripts ) to your path (Start / Edit environment variables). Now you should be able to run  pip  from the command line. Try installing a package:   pip install httpie    There you go (hopefully)! Solutions for common problems are given below:   Proxy problems   If you work in an office, you might be behind an HTTP proxy. If so, set the environment variables  http_proxy  and  https_proxy . Most Python applications (and other free software) respect these. Example syntax:   http://proxy_url:port http://username:password@proxy_url:port    If you're really unlucky, your proxy might be a Microsoft  NTLM  proxy. Free software can't cope. The only solution is to install a free software friendly proxy that forwards to the nasty proxy.  http://cntlm.sourceforge.net/   Unable to find vcvarsall.bat   Python modules can be partly written in C or C++. Pip tries to compile from source. If you don't have a C/C++ compiler installed and configured, you'll see this cryptic error message.     Error: Unable to find vcvarsall.bat     You can fix that by  installing a C++ compiler  such as  MinGW  or  Visual C++ . Microsoft actually ships one specifically for use with Python. Or try  Microsoft Visual C++ Compiler for Python 2.7 .   Often though it's easier to check  Christoph's site  for your package.", "score": 1960}}
{"question": "How to leave/exit/deactivate a Python virtualenv", "tags": ["python", "virtualenv", "exit", "virtualenvwrapper"], "link": "https://stackoverflow.com/questions/990754/how-to-leave-exit-deactivate-a-python-virtualenv", "answer_count": 16, "answers": {"id": 33932473, "body": "To activate a Python virtual environment:   $ cd ~/python-venv/ $ ./bin/activate    To deactivate:   $ deactivate", "score": 81}}
{"question": "How to leave/exit/deactivate a Python virtualenv", "tags": ["python", "virtualenv", "exit", "virtualenvwrapper"], "link": "https://stackoverflow.com/questions/990754/how-to-leave-exit-deactivate-a-python-virtualenv", "answer_count": 16, "answers": {"id": 29586756, "body": "Use:   $ deactivate     If this doesn't work, try    $ source deactivate    Anyone who knows how  Bash  source  works will think that's odd, but some wrappers/workflows around virtualenv implement it as a complement/counterpart to  source activate . Your mileage may vary.", "score": 177}}
{"question": "How to leave/exit/deactivate a Python virtualenv", "tags": ["python", "virtualenv", "exit", "virtualenvwrapper"], "link": "https://stackoverflow.com/questions/990754/how-to-leave-exit-deactivate-a-python-virtualenv", "answer_count": 16, "answers": {"id": 990779, "body": "Usually, activating a virtualenv gives you a shell function named:   $ deactivate    which puts things back to normal.   I have just looked specifically again at the code for  virtualenvwrapper , and, yes, it too supports  deactivate  as the way to escape from all virtualenvs.   If you are trying to leave an  Anaconda  environment, the command depends upon your version of  conda . Recent versions (like 4.6) install a  conda  function directly in your shell, in which case you run:   conda deactivate    Older conda versions instead implement deactivation using a stand-alone script:   source deactivate", "score": 4456}}
{"question": "How do I get the last element of a list?", "tags": ["python", "list", "indexing"], "link": "https://stackoverflow.com/questions/930397/how-do-i-get-the-last-element-of-a-list", "answer_count": 26, "answers": {"id": 930759, "body": "You can also do:   last_elem = alist.pop()    It depends on what you want to do with your list because the  pop()  method will delete the last element.", "score": 136}}
{"question": "How do I get the last element of a list?", "tags": ["python", "list", "indexing"], "link": "https://stackoverflow.com/questions/930397/how-do-i-get-the-last-element-of-a-list", "answer_count": 26, "answers": {"id": 4139773, "body": "If your  str()  or  list()  objects might end up being empty as so:  astr = ''  or  alist = [] , then you might want to use  alist[-1:]  instead of  alist[-1]  for object \"sameness\".   The significance of this is:   alist = [] alist[-1]   # will generate an IndexError exception whereas  alist[-1:]  # will return an empty list astr = '' astr[-1]    # will generate an IndexError exception whereas astr[-1:]   # will return an empty str    Where the distinction being made is that returning an empty list object or empty str object is more \"last element\"-like then an exception object.", "score": 333}}
{"question": "How do I get the last element of a list?", "tags": ["python", "list", "indexing"], "link": "https://stackoverflow.com/questions/930397/how-do-i-get-the-last-element-of-a-list", "answer_count": 26, "answers": {"id": 930398, "body": "some_list[-1]  is the shortest and most Pythonic.   In fact, you can do much more with this syntax. The  some_list[-n]  syntax gets the nth-to-last element. So  some_list[-1]  gets the last element,  some_list[-2]  gets the second to last, etc, all the way down to  some_list[-len(some_list)] , which gives you the first element.   You can also set list elements in this way. For instance:   >>> some_list = [1, 2, 3] >>> some_list[-1] = 5 # Set the last element >>> some_list[-2] = 3 # Set the second to last element >>> some_list [1, 3, 5]    Note that getting a list item by index will raise an  IndexError  if the expected item doesn't exist. This means that  some_list[-1]  will raise an exception if  some_list  is empty, because an empty list can't have a last element.", "score": 4003}}
{"question": "How to sort a list of dictionaries by a value of the dictionary in Python?", "tags": ["python", "list", "sorting", "dictionary", "data-structures"], "link": "https://stackoverflow.com/questions/72899/how-to-sort-a-list-of-dictionaries-by-a-value-of-the-dictionary-in-python", "answer_count": 22, "answers": {"id": 73044, "body": "my_list = [{'name':'Homer', 'age':39}, {'name':'Bart', 'age':10}]  my_list.sort(lambda x,y : cmp(x['name'], y['name']))    my_list  will now be what you want.   Or better:   Since Python 2.4, there's a  key  argument is both more efficient and neater:   my_list = sorted(my_list, key=lambda k: k['name'])    ...the lambda is, IMO, easier to understand than  operator.itemgetter , but your mileage may vary.", "score": 125}}
{"question": "How to sort a list of dictionaries by a value of the dictionary in Python?", "tags": ["python", "list", "sorting", "dictionary", "data-structures"], "link": "https://stackoverflow.com/questions/72899/how-to-sort-a-list-of-dictionaries-by-a-value-of-the-dictionary-in-python", "answer_count": 22, "answers": {"id": 73465, "body": "import operator    To sort the list of dictionaries by key='name':   list_of_dicts.sort(key=operator.itemgetter('name'))    To sort the list of dictionaries by key='age':   list_of_dicts.sort(key=operator.itemgetter('age'))", "score": 247}}
{"question": "How to sort a list of dictionaries by a value of the dictionary in Python?", "tags": ["python", "list", "sorting", "dictionary", "data-structures"], "link": "https://stackoverflow.com/questions/72899/how-to-sort-a-list-of-dictionaries-by-a-value-of-the-dictionary-in-python", "answer_count": 22, "answers": {"id": 73050, "body": "The  sorted()  function takes a  key=  parameter   newlist = sorted(list_to_be_sorted, key=lambda d: d['name'])    Alternatively, you can use  operator.itemgetter  instead of defining the function yourself   from operator import itemgetter newlist = sorted(list_to_be_sorted, key=itemgetter('name'))    For completeness, add  reverse=True  to sort in descending order   newlist = sorted(list_to_be_sorted, key=itemgetter('name'), reverse=True)", "score": 3735}}
{"question": "How to upgrade all Python packages with pip", "tags": ["python", "pip", "pypi"], "link": "https://stackoverflow.com/questions/2720014/how-to-upgrade-all-python-packages-with-pip", "answer_count": 63, "answers": {"id": 5839291, "body": "You can use the following Python code. Unlike  pip freeze , this will not print warnings and FIXME errors.  For pip < 10.0.1   import pip from subprocess import call  packages = [dist.project_name for dist in pip.get_installed_distributions()] call(\"pip install --upgrade \" + ' '.join(packages), shell=True)    For pip >= 10.0.1   import pkg_resources from subprocess import call  packages = [dist.project_name for dist in pkg_resources.working_set] call(\"pip install --upgrade \" + ' '.join(packages), shell=True)", "score": 843}}
{"question": "How to upgrade all Python packages with pip", "tags": ["python", "pip", "pypi"], "link": "https://stackoverflow.com/questions/2720014/how-to-upgrade-all-python-packages-with-pip", "answer_count": 63, "answers": {"id": 16269635, "body": "To upgrade all local packages, you can install  pip-review :   $ pip install pip-review    After that, you can either upgrade the packages interactively:   $ pip-review --local --interactive    Or automatically:   $ pip-review --local --auto      pip-review  is a fork of  pip-tools . See  pip-tools  issue  mentioned by  @knedlsepp .  pip-review  package works but  pip-tools  package no longer works.  pip-review  is looking for a new maintainer.   pip-review  works on Windows  since version 0.5 .", "score": 947}}
{"question": "How to upgrade all Python packages with pip", "tags": ["python", "pip", "pypi"], "link": "https://stackoverflow.com/questions/2720014/how-to-upgrade-all-python-packages-with-pip", "answer_count": 63, "answers": {"id": 3452888, "body": "There isn't a built-in flag yet. Starting with pip version 22.3, the  --outdated  and  --format=freeze  have become  mutually exclusive . Use Python, to parse the JSON output:   pip --disable-pip-version-check list --outdated --format=json | python -c \"import json, sys; print('\\n'.join([x['name'] for x in json.load(sys.stdin)]))\" | xargs -n1 pip install -U    If you are using  pip<22.3  you can use:   pip list --outdated --format=freeze | grep -v '^\\-e' | cut -d = -f 1  | xargs -n1 pip install -U    For older versions of  pip :   pip freeze --local | grep -v '^\\-e' | cut -d = -f 1  | xargs -n1 pip install -U        The  grep  is to skip editable (\"-e\") package definitions, as suggested by  @jawache . (Yes, you could replace  grep + cut  with  sed  or  awk  or  perl  or...).     The  -n1  flag for  xargs  prevents stopping everything if updating one package fails (thanks  @andsens ).         Note:  there are infinite potential variations for this. I'm trying to keep this answer short and simple, but please do suggest variations in the comments!", "score": 2932}}
{"question": "How can I remove a key from a Python dictionary?", "tags": ["python", "dictionary", "unset"], "link": "https://stackoverflow.com/questions/11277432/how-can-i-remove-a-key-from-a-python-dictionary", "answer_count": 11, "answers": {"id": 15206537, "body": "It took me some time to figure out what exactly  my_dict.pop(\"key\", None)  is doing. So I'll add this as an answer to save others googling time:     pop(key[, default])   If  key  is in the dictionary, remove it and return its value, else return  default . If  default  is not given and  key  is not in the dictionary, a  KeyError  is raised.     Documentation", "score": 206}}
{"question": "How can I remove a key from a Python dictionary?", "tags": ["python", "dictionary", "unset"], "link": "https://stackoverflow.com/questions/11277432/how-can-i-remove-a-key-from-a-python-dictionary", "answer_count": 11, "answers": {"id": 11277484, "body": "Specifically to answer \"is there a one line way of doing this?\"   if 'key' in my_dict: del my_dict['key']    ...well, you  asked  ;-)   You should consider, though, that this way of deleting an object from a  dict  is  not atomic \u2014it is possible that  'key'  may be in  my_dict  during the  if  statement, but may be deleted before  del  is executed, in which case  del  will fail with a  KeyError .  Given this, it would be safest to either  use  dict.pop  or something along the lines of   try:     del my_dict['key'] except KeyError:     pass    which, of course, is definitely  not  a one-liner.", "score": 475}}
{"question": "How can I remove a key from a Python dictionary?", "tags": ["python", "dictionary", "unset"], "link": "https://stackoverflow.com/questions/11277432/how-can-i-remove-a-key-from-a-python-dictionary", "answer_count": 11, "answers": {"id": 11277439, "body": "To delete a key regardless of whether it is in the dictionary, use the two-argument form of  dict.pop() :   my_dict.pop('key', None)    This will return  my_dict[key]  if  key  exists in the dictionary, and  None  otherwise. If the second parameter is not specified (i.e.  my_dict.pop('key') ) and  key  does not exist, a  KeyError  is raised.   To delete a key that is guaranteed to exist, you can also use   del my_dict['key']    This will raise a  KeyError  if the key is not in the dictionary.", "score": 4769}}
{"question": "Convert string &quot;Jun 1 2005 1:33PM&quot; into datetime", "tags": ["python", "datetime", "type-conversion"], "link": "https://stackoverflow.com/questions/466345/convert-string-jun-1-2005-133pm-into-datetime", "answer_count": 26, "answers": {"id": 466366, "body": "Check out  strptime  in the  time  module.  It is the inverse of  strftime .   $ python >>> import time >>> my_time = time.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p') time.struct_time(tm_year=2005, tm_mon=6, tm_mday=1,                  tm_hour=13, tm_min=33, tm_sec=0,                  tm_wday=2, tm_yday=152, tm_isdst=-1)  timestamp = time.mktime(my_time) # convert time object to datetime from datetime import datetime my_datetime = datetime.fromtimestamp(timestamp) # convert time object to date from datetime import date my_date = date.fromtimestamp(timestamp)", "score": 516}}
{"question": "Convert string &quot;Jun 1 2005 1:33PM&quot; into datetime", "tags": ["python", "datetime", "type-conversion"], "link": "https://stackoverflow.com/questions/466345/convert-string-jun-1-2005-133pm-into-datetime", "answer_count": 26, "answers": {"id": 470303, "body": "Use the third-party  dateutil  library:   from dateutil import parser parser.parse(\"Aug 28 1999 12:00AM\")  # datetime.datetime(1999, 8, 28, 0, 0)    It can handle most date formats and is more convenient than  strptime  since it usually guesses the correct format. It is also very useful for writing tests, where readability is more important than performance.   Install it with:   pip install python-dateutil", "score": 1082}}
{"question": "Convert string &quot;Jun 1 2005 1:33PM&quot; into datetime", "tags": ["python", "datetime", "type-conversion"], "link": "https://stackoverflow.com/questions/466345/convert-string-jun-1-2005-133pm-into-datetime", "answer_count": 26, "answers": {"id": 466376, "body": "datetime.strptime  parses an input string in the user-specified format into a  timezone-naive   datetime  object:   >>> from datetime import datetime >>> datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p') datetime.datetime(2005, 6, 1, 13, 33)    To obtain a  date  object using an existing  datetime  object, convert it using  .date() :   >>> datetime.strptime('Jun 1 2005', '%b %d %Y').date() date(2005, 6, 1)      Links:     strptime  docs:  Python 2 ,  Python 3     strptime / strftime  format string docs:  Python 2 ,  Python 3     strftime.org  format string cheatsheet       Notes:     strptime  = \"string parse time\"   strftime  = \"string format time\"", "score": 4463}}
{"question": "Renaming column names in Pandas", "tags": ["python", "pandas", "replace", "dataframe", "rename"], "link": "https://stackoverflow.com/questions/11346283/renaming-column-names-in-pandas", "answer_count": 33, "answers": {"id": 16667215, "body": "The  rename  method can take a  function , for example:   In [11]: df.columns Out[11]: Index([u'$a', u'$b', u'$c', u'$d', u'$e'], dtype=object)  In [12]: df.rename(columns=lambda x: x[1:], inplace=True)  In [13]: df.columns Out[13]: Index([u'a', u'b', u'c', u'd', u'e'], dtype=object)", "score": 523}}
{"question": "Renaming column names in Pandas", "tags": ["python", "pandas", "replace", "dataframe", "rename"], "link": "https://stackoverflow.com/questions/11346283/renaming-column-names-in-pandas", "answer_count": 33, "answers": {"id": 11346337, "body": "Just assign it to the  .columns  attribute:   >>> df = pd.DataFrame({'$a':[1,2], '$b': [10,20]}) >>> df    $a  $b 0   1  10 1   2  20  >>> df.columns = ['a', 'b'] >>> df    a   b 0  1  10 1  2  20", "score": 2566}}
{"question": "Renaming column names in Pandas", "tags": ["python", "pandas", "replace", "dataframe", "rename"], "link": "https://stackoverflow.com/questions/11346283/renaming-column-names-in-pandas", "answer_count": 33, "answers": {"id": 11354850, "body": "Rename Specific Columns   Use the  df.rename()  function and refer the columns to be renamed. Not all the columns have to be renamed:   df = df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'})  # Or rename the existing DataFrame (rather than creating a copy)  df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)    Minimal Code Example   df = pd.DataFrame('x', index=range(3), columns=list('abcde')) df     a  b  c  d  e 0  x  x  x  x  x 1  x  x  x  x  x 2  x  x  x  x  x    The following methods all work and produce the same output:   df2 = df.rename({'a': 'X', 'b': 'Y'}, axis=1) df2 = df.rename({'a': 'X', 'b': 'Y'}, axis='columns') df2 = df.rename(columns={'a': 'X', 'b': 'Y'})   df2     X  Y  c  d  e 0  x  x  x  x  x 1  x  x  x  x  x 2  x  x  x  x  x    Remember to assign the result back, as the modification is not-inplace. Alternatively, specify  inplace=True :   df.rename({'a': 'X', 'b': 'Y'}, axis=1, inplace=True) df     X  Y  c  d  e 0  x  x  x  x  x 1  x  x  x  x  x 2  x  x  x  x  x      You can specify  errors='raise'  to raise errors if an invalid column-to-rename is specified.     Reassign Column Headers   Use  df.set_axis()  with  axis=1 .   df2 = df.set_axis(['V', 'W', 'X', 'Y', 'Z'], axis=1) df2     V  W  X  Y  Z 0  x  x  x  x  x 1  x  x  x  x  x 2  x  x  x  x  x    Headers can be assigned directly:   df.columns = ['V', 'W', 'X', 'Y', 'Z'] df     V  W  X  Y  Z 0  x  x  x  x  x 1  x  x  x  x  x 2  x  x  x  x  x", "score": 4688}}
{"question": "Why is &quot;1000000000000000 in range(1000000000000001)&quot; so fast in Python 3?", "tags": ["python", "performance", "python-3.x", "range", "python-internals"], "link": "https://stackoverflow.com/questions/30081275/why-is-1000000000000000-in-range1000000000000001-so-fast-in-python-3", "answer_count": 12, "answers": {"id": 30081470, "body": "Use the  source , Luke!   In CPython,  range(...).__contains__  (a method wrapper) will eventually delegate to a simple calculation which checks if the value can possibly be in the range.  The reason for the speed here is we're using  mathematical reasoning about the bounds, rather than a direct iteration of the range object .  To explain the logic used:     Check that the number is between  start  and  stop , and   Check that the stride value doesn't \"step over\" our number.     For example,  994  is in  range(4, 1000, 2)  because:     4 <= 994 < 1000 , and   (994 - 4) % 2 == 0 .     The full C code is included below, which is a bit more verbose because of memory management and reference counting details, but the basic idea is there:   static int range_contains_long(rangeobject *r, PyObject *ob) {     int cmp1, cmp2, cmp3;     PyObject *tmp1 = NULL;     PyObject *tmp2 = NULL;     PyObject *zero = NULL;     int result = -1;      zero = PyLong_FromLong(0);     if (zero == NULL) /* MemoryError in int(0) */         goto end;      /* Check if the value can possibly be in the range. */      cmp1 = PyObject_RichCompareBool(r->step, zero, Py_GT);     if (cmp1 == -1)         goto end;     if (cmp1 == 1) { /* positive steps: start <= ob < stop */         cmp2 = PyObject_RichCompareBool(r->start, ob, Py_LE);         cmp3 = PyObject_RichCompareBool(ob, r->stop, Py_LT);     }     else { /* negative steps: stop < ob <= start */         cmp2 = PyObject_RichCompareBool(ob, r->start, Py_LE);         cmp3 = PyObject_RichCompareBool(r->stop, ob, Py_LT);     }      if (cmp2 == -1 || cmp3 == -1) /* TypeError */         goto end;     if (cmp2 == 0 || cmp3 == 0) { /* ob outside of range */         result = 0;         goto end;     }      /* Check that the stride does not invalidate ob's membership. */     tmp1 = PyNumber_Subtract(ob, r->start);     if (tmp1 == NULL)         goto end;     tmp2 = PyNumber_Remainder(tmp1, r->step);     if (tmp2 == NULL)         goto end;     /* result = ((int(ob) - start) % step) == 0 */     result = PyObject_RichCompareBool(tmp2, zero, Py_EQ);   end:     Py_XDECREF(tmp1);     Py_XDECREF(tmp2);     Py_XDECREF(zero);     return result; }  static int range_contains(rangeobject *r, PyObject *ob) {     if (PyLong_CheckExact(ob) || PyBool_Check(ob))         return range_contains_long(r, ob);      return (int)_PySequence_IterSearch((PyObject*)r, ob,                                        PY_ITERSEARCH_CONTAINS); }    The \"meat\" of the idea is mentioned in the comment lines:   /* positive steps: start <= ob < stop */ /* negative steps: stop < ob <= start */ /* result = ((int(ob) - start) % step) == 0 */     As a final note - look at the  range_contains  function at the bottom of the code snippet.  If the exact type check fails then we don't use the clever algorithm described, instead falling back to a dumb iteration search of the range using  _PySequence_IterSearch !  You can check this behaviour in the interpreter (I'm using v3.5.0 here):   >>> x, r = 1000000000000000, range(1000000000000001) >>> class MyInt(int): ...     pass ...  >>> x_ = MyInt(x) >>> x in r  # calculates immediately :)  True >>> x_ in r  # iterates for ages.. :(  ^\\Quit (core dumped)", "score": 528}}
{"question": "Why is &quot;1000000000000000 in range(1000000000000001)&quot; so fast in Python 3?", "tags": ["python", "performance", "python-3.x", "range", "python-internals"], "link": "https://stackoverflow.com/questions/30081275/why-is-1000000000000000-in-range1000000000000001-so-fast-in-python-3", "answer_count": 12, "answers": {"id": 30081894, "body": "The fundamental misunderstanding here is in thinking that  range  is a generator. It's not. In fact, it's not any kind of iterator.   You can tell this pretty easily:   >>> a = range(5) >>> print(list(a)) [0, 1, 2, 3, 4] >>> print(list(a)) [0, 1, 2, 3, 4]    If it were a generator, iterating it once would exhaust it:   >>> b = my_crappy_range(5) >>> print(list(b)) [0, 1, 2, 3, 4] >>> print(list(b)) []    What  range  actually is, is a sequence, just like a list. You can even test this:   >>> import collections.abc >>> isinstance(a, collections.abc.Sequence) True    This means it has to follow all the rules of being a sequence:   >>> a[3]         # indexable 3 >>> len(a)       # sized 5 >>> 3 in a       # membership True >>> reversed(a)  # reversible   >>> a.index(3)   # implements 'index' 3 >>> a.count(3)   # implements 'count' 1      The difference between a  range  and a  list  is that a  range  is a  lazy  or  dynamic  sequence; it doesn't remember all of its values, it just remembers its  start ,  stop , and  step , and creates the values on demand on  __getitem__ .   (As a side note, if you  print(iter(a)) , you'll notice that  range  uses the same  listiterator  type as  list . How does that work? A  listiterator  doesn't use anything special about  list  except for the fact that it provides a C implementation of  __getitem__ , so it works fine for  range  too.)     Now, there's nothing that says that  Sequence.__contains__  has to be constant time\u2014in fact, for obvious examples of sequences like  list , it isn't. But there's nothing that says it  can't  be. And it's easier to implement  range.__contains__  to just check it mathematically ( (val - start) % step , but with some extra complexity to deal with negative steps) than to actually generate and test all the values, so why  shouldn't  it do it the better way?   But there doesn't seem to be anything in the language that  guarantees  this will happen. As Ashwini Chaudhari points out, if you give it a non-integral value, instead of converting to integer and doing the mathematical test, it will fall back to iterating all the values and comparing them one by one. And just because CPython 3.2+ and PyPy 3.x versions happen to contain this optimization, and it's an obvious good idea and easy to do, there's no reason that IronPython or NewKickAssPython 3.x couldn't leave it out. (And in fact, CPython 3.0-3.1  didn't  include it.)     If  range  actually were a generator, like  my_crappy_range , then it wouldn't make sense to test  __contains__  this way, or at least the way it makes sense wouldn't be obvious. If you'd already iterated the first 3 values, is  1  still  in  the generator? Should testing for  1  cause it to iterate and consume all the values up to  1  (or up to the first value  >= 1 )?", "score": 1235}}
{"question": "Why is &quot;1000000000000000 in range(1000000000000001)&quot; so fast in Python 3?", "tags": ["python", "performance", "python-3.x", "range", "python-internals"], "link": "https://stackoverflow.com/questions/30081275/why-is-1000000000000000-in-range1000000000000001-so-fast-in-python-3", "answer_count": 12, "answers": {"id": 30081318, "body": "The Python 3  range()  object doesn't produce numbers immediately; it is a smart  sequence object  that produces numbers  on demand . All it contains is your start, stop and step values, then as you iterate over the object the next integer is calculated each iteration.   The object also implements the  object.__contains__  hook , and  calculates  if your number is part of its range. Calculating is a (near) constant time operation  * . There is never a need to scan through all possible integers in the range.   From the  range()  object documentation :     The advantage of the  range  type over a regular  list  or  tuple  is that a range object will always take the same (small) amount of memory, no matter the size of the range it represents (as it only stores the  start ,  stop  and  step  values, calculating individual items and subranges as needed).     So at a minimum, your  range()  object would do:   class my_range:     def __init__(self, start, stop=None, step=1, /):         if stop is None:             start, stop = 0, start         self.start, self.stop, self.step = start, stop, step         if step < 0:             lo, hi, step = stop, start, -step         else:             lo, hi = start, stop         self.length = 0 if lo > hi else ((hi - lo - 1) // step) + 1      def __iter__(self):         current = self.start         if self.step < 0:             while current > self.stop:                 yield current                 current += self.step         else:             while current < self.stop:                 yield current                 current += self.step      def __len__(self):         return self.length      def __getitem__(self, i):         if i < 0:             i += self.length         if 0 <= i < self.length:             return self.start + i * self.step         raise IndexError('my_range object index out of range')      def __contains__(self, num):         if self.step < 0:             if not (self.stop < num <= self.start):                 return False         else:             if not (self.start <= num < self.stop):                 return False         return (num - self.start) % self.step == 0    This is still missing several things that a real  range()  supports (such as the  .index()  or  .count()  methods, hashing, equality testing, or slicing), but should give you an idea.   I also simplified the  __contains__  implementation to only focus on integer tests; if you give a real  range()  object a non-integer value (including subclasses of  int ), a slow scan is initiated to see if there is a match, just as if you use a containment test against a list of all the contained values. This was done to continue to support other numeric types that just happen to support equality testing with integers but are not expected to support integer arithmetic as well. See the original  Python issue  that implemented the containment test.     *  Near  constant time because Python integers are unbounded and so math operations also grow in time as N grows, making this a O(log N) operation. Since it\u2019s all executed in optimised C code and Python stores integer values in 30-bit chunks, you\u2019d run out of memory before you saw any performance impact due to the size of the integers involved here.", "score": 3131}}
{"question": "Find the current directory and file&#39;s directory", "tags": ["python", "directory"], "link": "https://stackoverflow.com/questions/5137497/find-the-current-directory-and-files-directory", "answer_count": 13, "answers": {"id": 13720875, "body": "You may find this useful as a reference:   import os  print(\"Path at terminal when executing this file\") print(os.getcwd() + \"\\n\")  print(\"This file path, relative to os.getcwd()\") print(__file__ + \"\\n\")  print(\"This file full path (following symlinks)\") full_path = os.path.realpath(__file__) print(full_path + \"\\n\")  print(\"This file directory and name\") path, filename = os.path.split(full_path) print(path + ' --> ' + filename + \"\\n\")  print(\"This file directory only\") print(os.path.dirname(full_path))", "score": 371}}
{"question": "Find the current directory and file&#39;s directory", "tags": ["python", "directory"], "link": "https://stackoverflow.com/questions/5137497/find-the-current-directory-and-files-directory", "answer_count": 13, "answers": {"id": 5137507, "body": "Current working directory :   os.getcwd()   And the  __file__  attribute  can help you find out where the file you are executing is located. This Stack\u00a0Overflow post explains everything:   How do I get the path of the current executed file in Python?", "score": 392}}
{"question": "Find the current directory and file&#39;s directory", "tags": ["python", "directory"], "link": "https://stackoverflow.com/questions/5137497/find-the-current-directory-and-files-directory", "answer_count": 13, "answers": {"id": 5137509, "body": "To get the full path to the directory a Python file is contained in, write this in that file:   import os  dir_path = os.path.dirname(os.path.realpath(__file__))    (Note that the incantation above won't work if you've already used  os.chdir()  to change your current working directory, since the value of the  __file__  constant is relative to the current working directory and is not changed by an  os.chdir()  call.)     To get the current working directory use    import os cwd = os.getcwd()      Documentation references for the modules, constants and functions used above:     The  os  and  os.path  modules.   The  __file__  constant   os.path.realpath(path)  (returns  \"the canonical path of the specified filename, eliminating any symbolic links encountered in the path\" )   os.path.dirname(path)  (returns  \"the directory name of pathname  path \" )   os.getcwd()  (returns  \"a string representing the current working directory\" )   os.chdir(path)  ( \"change the current working directory to  path \" )", "score": 4806}}
{"question": "What is the difference between Python&#39;s list methods append and extend?", "tags": ["python", "list", "data-structures", "append", "extend"], "link": "https://stackoverflow.com/questions/252703/what-is-the-difference-between-pythons-list-methods-append-and-extend", "answer_count": 20, "answers": {"id": 28119966, "body": "What is the difference between the list methods append and extend?       .append()  adds its argument as a single element to the end of a list. The length of the list itself will increase by one.   .extend()  iterates over its argument adding each element to the list, extending the list. The length of the list will increase by however many elements were in the iterable argument.     .append()   The  .append()  method appends an object to the end of the list.   my_list.append(object)     Whatever the object is, whether a number, a string, another list, or something else, it gets added onto the end of  my_list  as a single entry on the list.   >>> my_list ['foo', 'bar'] >>> my_list.append('baz') >>> my_list ['foo', 'bar', 'baz']    So keep in mind that a list is an object. If you append another list onto a list, the first list will be a single object at the end of the list (which may not be what you want):   >>> another_list = [1, 2, 3] >>> my_list.append(another_list) >>> my_list ['foo', 'bar', 'baz', [1, 2, 3]]                      #^^^^^^^^^--- single item at the end of the list.    .extend()   The  .extend()  method extends a list by appending elements from an iterable:   my_list.extend(iterable)    So with extend, each element of the iterable gets appended onto the list. For example:   >>> my_list ['foo', 'bar'] >>> another_list = [1, 2, 3] >>> my_list.extend(another_list) >>> my_list ['foo', 'bar', 1, 2, 3]    Keep in mind that a string is an iterable, so if you extend a list with a string, you'll append each character as you iterate over the string (which may not be what you want):   >>> my_list.extend('baz') >>> my_list ['foo', 'bar', 1, 2, 3, 'b', 'a', 'z']    Operator Overload,  __add__  ( + ) and  __iadd__  ( += )   Both  +  and  +=  operators are defined for  list . They are semantically similar to extend.   my_list + another_list  creates a third list in memory, so you can return the result of it, but it requires that the second iterable be a list.   my_list += another_list  modifies the list in-place (it  is  the in-place operator, and lists are mutable objects, as we've seen) so it does not create a new list. It also works like extend, in that the second iterable can be any kind of iterable.   Don't get confused -  my_list = my_list + another_list  is not equivalent to  +=  - it gives you a brand new list assigned to my_list.   Time Complexity   Append has ( amortized )  constant time complexity , O(1).   Extend has time complexity, O(k).   Iterating through the multiple calls to  .append()  adds to the complexity, making it equivalent to that of extend, and since extend's iteration is implemented in C, it will always be faster if you intend to append successive items from an iterable onto a list.   Regarding \"amortized\" - from the  list object implementation source :       /* This over-allocates proportional to the list size, making room      * for additional growth.  The over-allocation is mild, but is      * enough to give linear-time amortized behavior over a long      * sequence of appends() in the presence of a poorly-performing      * system realloc().    This means that we get the benefits of a larger than needed memory reallocation up front, but we may pay for it on the next marginal reallocation with an even larger one. Total time for all appends is linear at O(n), and that time allocated per append, becomes O(1).   Performance   You may wonder what is more performant, since append can be used to achieve the same outcome as extend. The following functions do the same thing:   def append(alist, iterable):     for item in iterable:         alist.append(item)          def extend(alist, iterable):     alist.extend(iterable)    So let's time them:   import timeit  >>> min(timeit.repeat(lambda: append([], \"abcdefghijklmnopqrstuvwxyz\"))) 2.867846965789795 >>> min(timeit.repeat(lambda: extend([], \"abcdefghijklmnopqrstuvwxyz\"))) 0.8060121536254883    Addressing a comment on timings   A commenter said:     Perfect answer, I just miss the timing of comparing adding only one element     Do the semantically correct thing. If you want to append all elements in an iterable, use  .extend() . If you're just adding one element, use  .append() .   Ok, so let's create an experiment to see how this works out in time:   def append_one(a_list, element):     a_list.append(element)  def extend_one(a_list, element):     \"\"\"creating a new list is semantically the most direct     way to create an iterable to give to extend\"\"\"     a_list.extend([element])  import timeit    And we see that going out of our way to create an iterable just to use extend is a (minor) waste of time:   >>> min(timeit.repeat(lambda: append_one([], 0))) 0.2082819009956438 >>> min(timeit.repeat(lambda: extend_one([], 0))) 0.2397019260097295    We learn from this that there's nothing gained from using  .extend()  when we have only  one  element to append.   Also, these timings are not that important. I am just showing them to make the point that, in Python, doing the semantically correct thing is doing things the  Right  Way\u2122.   It's conceivable that you might test timings on two comparable operations and get an ambiguous or inverse result. Just focus on doing the semantically correct thing.   Conclusion   We see that  .extend()  is semantically clearer, and that it can run much faster than  .append() ,  when you intend to append each element in an iterable to a list.   If you only have a single element (not in an iterable) to add to the list, use  .append() .", "score": 672}}
{"question": "What is the difference between Python&#39;s list methods append and extend?", "tags": ["python", "list", "data-structures", "append", "extend"], "link": "https://stackoverflow.com/questions/252703/what-is-the-difference-between-pythons-list-methods-append-and-extend", "answer_count": 20, "answers": {"id": 252705, "body": ".append()  adds an element to a list,  whereas  .extend()  concatenates the first list with another list/iterable.   >>> xs = ['A', 'B'] >>> xs ['A', 'B']  >>> xs.append(\"D\") >>> xs ['A', 'B', 'D']  >>> xs.append([\"E\", \"F\"]) >>> xs ['A', 'B', 'D', ['E', 'F']]  >>> xs.insert(2, \"C\") >>> xs ['A', 'B', 'C', 'D', ['E', 'F']]  >>> xs.extend([\"G\", \"H\"]) >>> xs ['A', 'B', 'C', 'D', ['E', 'F'], 'G', 'H']", "score": 741}}
{"question": "What is the difference between Python&#39;s list methods append and extend?", "tags": ["python", "list", "data-structures", "append", "extend"], "link": "https://stackoverflow.com/questions/252703/what-is-the-difference-between-pythons-list-methods-append-and-extend", "answer_count": 20, "answers": {"id": 252711, "body": ".append()  appends a  single object  at the end of the list:   >>> x = [1, 2, 3] >>> x.append([4, 5]) >>> print(x) [1, 2, 3, [4, 5]]    .extend()  appends  multiple objects  that are taken from inside the specified iterable:   >>> x = [1, 2, 3] >>> x.extend([4, 5]) >>> print(x) [1, 2, 3, 4, 5]", "score": 5914}}
{"question": "How do I split a list into equally-sized chunks?", "tags": ["python", "list", "split", "chunks"], "link": "https://stackoverflow.com/questions/312443/how-do-i-split-a-list-into-equally-sized-chunks", "answer_count": 69, "answers": {"id": 16935535, "body": "I know this is kind of old but nobody yet mentioned  numpy.array_split :   import numpy as np  lst = range(50) np.array_split(lst, 5)    Result:   [array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),  array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19]),  array([20, 21, 22, 23, 24, 25, 26, 27, 28, 29]),  array([30, 31, 32, 33, 34, 35, 36, 37, 38, 39]),  array([40, 41, 42, 43, 44, 45, 46, 47, 48, 49])]", "score": 446}}
{"question": "How do I split a list into equally-sized chunks?", "tags": ["python", "list", "split", "chunks"], "link": "https://stackoverflow.com/questions/312443/how-do-i-split-a-list-into-equally-sized-chunks", "answer_count": 69, "answers": {"id": 1751478, "body": "Something super simple:   def chunks(xs, n):     n = max(1, n)     return (xs[i:i+n] for i in range(0, len(xs), n))    For Python 2, use  xrange()  instead of  range() .", "score": 668}}
{"question": "How do I split a list into equally-sized chunks?", "tags": ["python", "list", "split", "chunks"], "link": "https://stackoverflow.com/questions/312443/how-do-i-split-a-list-into-equally-sized-chunks", "answer_count": 69, "answers": {"id": 312464, "body": "Here's a generator that yields evenly-sized chunks:   def chunks(lst, n):     \"\"\"Yield successive n-sized chunks from lst.\"\"\"     for i in range(0, len(lst), n):         yield lst[i:i + n]    import pprint pprint.pprint(list(chunks(range(10, 75), 10))) [[10, 11, 12, 13, 14, 15, 16, 17, 18, 19],  [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],  [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],  [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],  [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],  [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],  [70, 71, 72, 73, 74]]    For Python 2, using  xrange  instead of  range :   def chunks(lst, n):     \"\"\"Yield successive n-sized chunks from lst.\"\"\"     for i in xrange(0, len(lst), n):         yield lst[i:i + n]      Below is a list comprehension one-liner. The method above is preferable, though, since using named functions makes code easier to understand. For Python 3:   [lst[i:i + n] for i in range(0, len(lst), n)]    For Python 2:   [lst[i:i + n] for i in xrange(0, len(lst), n)]", "score": 4484}}
{"question": "How do I make function decorators and chain them together?", "tags": ["python", "function", "decorator", "python-decorators", "chain"], "link": "https://stackoverflow.com/questions/739654/how-do-i-make-function-decorators-and-chain-them-together", "answer_count": 22, "answers": {"id": 739679, "body": "Alternatively, you could write a factory function which return a decorator which wraps the return value of the decorated function in a tag passed to the factory function. For example:   from functools import wraps  def wrap_in_tag(tag):     def factory(func):         @wraps(func)         def decorator():             return '<%(tag)s>%(rv)s ' % (                 {'tag': tag, 'rv': func()})         return decorator     return factory    This enables you to write:   @wrap_in_tag('b') @wrap_in_tag('i') def say():     return 'hello'    or   makebold = wrap_in_tag('b') makeitalic = wrap_in_tag('i')  @makebold @makeitalic def say():     return 'hello'    Personally I would have written the decorator somewhat differently:   from functools import wraps  def wrap_in_tag(tag):     def factory(func):         @wraps(func)         def decorator(val):             return func('<%(tag)s>%(val)s ' %                         {'tag': tag, 'val': val})         return decorator     return factory    which would yield:   @wrap_in_tag('b') @wrap_in_tag('i') def say(val):     return val say('hello')    Don't forget the construction for which decorator syntax is a shorthand:   say = wrap_in_tag('b')(wrap_in_tag('i')(say)))", "score": 162}}
{"question": "How do I make function decorators and chain them together?", "tags": ["python", "function", "decorator", "python-decorators", "chain"], "link": "https://stackoverflow.com/questions/739654/how-do-i-make-function-decorators-and-chain-them-together", "answer_count": 22, "answers": {"id": 739665, "body": "Check out  the documentation  to see how decorators work. Here is what you asked for:   from functools import wraps  def makebold(fn):     @wraps(fn)     def wrapper(*args, **kwargs):         return \" \" + fn(*args, **kwargs) + \" \"     return wrapper  def makeitalic(fn):     @wraps(fn)     def wrapper(*args, **kwargs):         return \" \" + fn(*args, **kwargs) + \" \"     return wrapper  @makebold @makeitalic def hello():     return \"hello world\"  @makebold @makeitalic def log(s):     return s  print hello()        # returns \" hello world \" print hello.__name__ # with functools.wraps() this returns \"hello\" print log('hello')   # returns \" hello \"", "score": 3101}}
{"question": "How do I make function decorators and chain them together?", "tags": ["python", "function", "decorator", "python-decorators", "chain"], "link": "https://stackoverflow.com/questions/739654/how-do-i-make-function-decorators-and-chain-them-together", "answer_count": 22, "answers": {"id": 1594484, "body": "If you are not into long explanations, see  Paolo Bergantino\u2019s answer .   Decorator Basics   Python\u2019s functions are objects   To understand decorators, you must first understand that functions are objects in Python. This has important consequences. Let\u2019s see why with a simple example :   def shout(word=\"yes\"):     return word.capitalize()+\"!\"  print(shout()) # outputs : 'Yes!'  # As an object, you can assign the function to a variable like any other object  scream = shout  # Notice we don't use parentheses: we are not calling the function, # we are putting the function \"shout\" into the variable \"scream\". # It means you can then call \"shout\" from \"scream\":  print(scream()) # outputs : 'Yes!'  # More than that, it means you can remove the old name 'shout', #\u00a0and the function will still be accessible from 'scream'  del shout try:     print(shout()) except NameError as e:     print(e)     #outputs: \"name 'shout' is not defined\"  print(scream()) # outputs: 'Yes!'    Keep this in mind. We\u2019ll circle back to it shortly.   Another interesting property of Python functions is they can be defined inside another function!   def talk():      # You can define a function on the fly in \"talk\" ...     def whisper(word=\"yes\"):         return word.lower()+\"...\"      # ... and use it right away!     print(whisper())  # You call \"talk\", that defines \"whisper\" EVERY TIME you call it, then # \"whisper\" is called in \"talk\".  talk() # outputs:  # \"yes...\"  # But \"whisper\" DOES NOT EXIST outside \"talk\":  try:     print(whisper()) except NameError as e:     print(e)     #outputs : \"name 'whisper' is not defined\"*     #Python's functions are objects    Functions references   Okay, still here? Now the fun part...   You\u2019ve seen that functions are objects. Therefore, functions:     can be assigned to a variable   can be defined in another function     That means that  a function can  return  another function .   def getTalk(kind=\"shout\"):      # We define functions on the fly     def shout(word=\"yes\"):         return word.capitalize()+\"!\"      def whisper(word=\"yes\") :         return word.lower()+\"...\"      # Then we return one of them     if kind == \"shout\":         # We don't use \"()\", we are not calling the function,         # we are returning the function object         return shout       else:         return whisper  # How do you use this strange beast?  # Get the function and assign it to a variable talk = getTalk()        # You can see that \"talk\" is here a function object: print(talk) #outputs :    # The object is the one returned by the function: print(talk()) #outputs : Yes!  # And you can even use it directly if you feel wild: print(getTalk(\"whisper\")()) #outputs : yes...    There\u2019s more!   If you can  return  a function, you can pass one as a parameter:   def doSomethingBefore(func):      print(\"I do something before then I call the function you gave me\")     print(func())  doSomethingBefore(scream) #outputs:  #I do something before then I call the function you gave me #Yes!    Well, you just have everything needed to understand decorators. You see, decorators are \u201cwrappers\u201d, which means that  they let you execute code before and after the function they decorate  without modifying the function itself.   Handcrafted decorators   How you\u2019d do it manually:   # A decorator is a function that expects ANOTHER function as parameter def my_shiny_new_decorator(a_function_to_decorate):      # Inside, the decorator defines a function on the fly: the wrapper.     # This function is going to be wrapped around the original function     # so it can execute code before and after it.     def the_wrapper_around_the_original_function():          # Put here the code you want to be executed BEFORE the original function is called         print(\"Before the function runs\")          # Call the function here (using parentheses)         a_function_to_decorate()          # Put here the code you want to be executed AFTER the original function is called         print(\"After the function runs\")      # At this point, \"a_function_to_decorate\" HAS NEVER BEEN EXECUTED.     # We return the wrapper function we have just created.     # The wrapper contains the function and the code to execute before and after. It\u2019s ready to use!     return the_wrapper_around_the_original_function  # Now imagine you create a function you don't want to ever touch again. def a_stand_alone_function():     print(\"I am a stand alone function, don't you dare modify me\")  a_stand_alone_function()  #outputs: I am a stand alone function, don't you dare modify me  # Well, you can decorate it to extend its behavior. # Just pass it to the decorator, it will wrap it dynamically in  # any code you want and return you a new function ready to be used:  a_stand_alone_function_decorated = my_shiny_new_decorator(a_stand_alone_function) a_stand_alone_function_decorated() #outputs: #Before the function runs #I am a stand alone function, don't you dare modify me #After the function runs    Now, you probably want that every time you call  a_stand_alone_function ,  a_stand_alone_function_decorated  is called instead. That\u2019s easy, just overwrite  a_stand_alone_function  with the function returned by  my_shiny_new_decorator :   a_stand_alone_function = my_shiny_new_decorator(a_stand_alone_function) a_stand_alone_function() #outputs: #Before the function runs #I am a stand alone function, don't you dare modify me #After the function runs  # That\u2019s EXACTLY what decorators do!    Decorators demystified   The previous example, using the decorator syntax:   @my_shiny_new_decorator def another_stand_alone_function():     print(\"Leave me alone\")  another_stand_alone_function()   #outputs:   #Before the function runs #Leave me alone #After the function runs    Yes, that\u2019s all, it\u2019s that simple.  @decorator  is just a shortcut to:   another_stand_alone_function = my_shiny_new_decorator(another_stand_alone_function)    Decorators are just a pythonic variant of the  decorator design pattern . There are several classic design patterns embedded in Python to ease development (like iterators).   Of course, you can accumulate decorators:   def bread(func):     def wrapper():         print(\" \")         func()         print(\"<\\______/>\")     return wrapper  def ingredients(func):     def wrapper():         print(\"#tomatoes#\")         func()         print(\"~salad~\")     return wrapper  def sandwich(food=\"--ham--\"):     print(food)  sandwich() #outputs: --ham-- sandwich = bread(ingredients(sandwich)) sandwich() #outputs: #  # #tomatoes# # --ham-- # ~salad~ #<\\______/>    Using the Python decorator syntax:   @bread @ingredients def sandwich(food=\"--ham--\"):     print(food)  sandwich() #outputs: #  # #tomatoes# # --ham-- # ~salad~ #<\\______/>    The order you set the decorators MATTERS:   @ingredients @bread def strange_sandwich(food=\"--ham--\"):     print(food)  strange_sandwich() #outputs: ##tomatoes# #  # --ham-- #<\\______/> # ~salad~      Now: to answer the question...   As a conclusion, you can easily see how to answer the question:   # The decorator to make it bold def makebold(fn):     # The new function the decorator returns     def wrapper():         # Insertion of some code before and after         return \" \" + fn() + \" \"     return wrapper  # The decorator to make it italic def makeitalic(fn):     # The new function the decorator returns     def wrapper():         # Insertion of some code before and after         return \" \" + fn() + \" \"     return wrapper  @makebold @makeitalic def say():     return \"hello\"  print(say()) #outputs:  hello   # This is the exact equivalent to  def say():     return \"hello\" say = makebold(makeitalic(say))  print(say()) #outputs:  hello     You can now just leave happy, or burn your brain a little bit more and see advanced uses of decorators.     Taking decorators to the next level   Passing arguments to the decorated function   # It\u2019s not black magic, you just have to let the wrapper  # pass the argument:  def a_decorator_passing_arguments(function_to_decorate):     def a_wrapper_accepting_arguments(arg1, arg2):         print(\"I got args! Look: {0}, {1}\".format(arg1, arg2))         function_to_decorate(arg1, arg2)     return a_wrapper_accepting_arguments  # Since when you are calling the function returned by the decorator, you are # calling the wrapper, passing arguments to the wrapper will let it pass them to  # the decorated function  @a_decorator_passing_arguments def print_full_name(first_name, last_name):     print(\"My name is {0} {1}\".format(first_name, last_name))      print_full_name(\"Peter\", \"Venkman\") # outputs: #I got args! Look: Peter Venkman #My name is Peter Venkman    Decorating methods   One nifty thing about Python is that methods and functions are really the same.  The only difference is that methods expect that their first argument is a reference to the current object ( self ).   That means you can build a decorator for methods the same way! Just remember to take  self  into consideration:   def method_friendly_decorator(method_to_decorate):     def wrapper(self, lie):         lie = lie - 3 # very friendly, decrease age even more :-)         return method_to_decorate(self, lie)     return wrapper           class Lucy(object):          def __init__(self):         self.age = 32          @method_friendly_decorator     def sayYourAge(self, lie):         print(\"I am {0}, what did you think?\".format(self.age + lie))          l = Lucy() l.sayYourAge(-3) #outputs: I am 26, what did you think?    If you\u2019re making general-purpose decorator--one you\u2019ll apply to any function or method, no matter its arguments--then just use  *args, **kwargs :   def a_decorator_passing_arbitrary_arguments(function_to_decorate):     # The wrapper accepts any arguments     def a_wrapper_accepting_arbitrary_arguments(*args, **kwargs):         print(\"Do I have args?:\")         print(args)         print(kwargs)         # Then you unpack the arguments, here *args, **kwargs         # If you are not familiar with unpacking, check:         # http://www.saltycrane.com/blog/2008/01/how-to-use-args-and-kwargs-in-python/         function_to_decorate(*args, **kwargs)     return a_wrapper_accepting_arbitrary_arguments  @a_decorator_passing_arbitrary_arguments def function_with_no_argument():     print(\"Python is cool, no argument here.\")  function_with_no_argument() #outputs #Do I have args?: #() #{} #Python is cool, no argument here.  @a_decorator_passing_arbitrary_arguments def function_with_arguments(a, b, c):     print(a, b, c)      function_with_arguments(1,2,3) #outputs #Do I have args?: #(1, 2, 3) #{} #1 2 3    @a_decorator_passing_arbitrary_arguments def function_with_named_arguments(a, b, c, platypus=\"Why not ?\"):     print(\"Do {0}, {1} and {2} like platypus? {3}\".format(a, b, c, platypus))  function_with_named_arguments(\"Bill\", \"Linus\", \"Steve\", platypus=\"Indeed!\") #outputs #Do I have args ? : #('Bill', 'Linus', 'Steve') #{'platypus': 'Indeed!'} #Do Bill, Linus and Steve like platypus? Indeed!  class Mary(object):          def __init__(self):         self.age = 31          @a_decorator_passing_arbitrary_arguments     def sayYourAge(self, lie=-3): # You can now add a default value         print(\"I am {0}, what did you think?\".format(self.age + lie))  m = Mary() m.sayYourAge() #outputs # Do I have args?: #(<__main__.Mary object at 0xb7d303ac>,) #{} #I am 28, what did you think?    Passing arguments to the decorator   Great, now what would you say about passing arguments to the decorator itself?   This can get somewhat twisted, since a decorator must accept a function as an argument. Therefore, you cannot pass the decorated function\u2019s arguments directly to the decorator.   Before rushing to the solution, let\u2019s write a little reminder:   # Decorators are ORDINARY functions def my_decorator(func):     print(\"I am an ordinary function\")     def wrapper():         print(\"I am function returned by the decorator\")         func()     return wrapper  # Therefore, you can call it without any \"@\"  def lazy_function():     print(\"zzzzzzzz\")  decorated_function = my_decorator(lazy_function) #outputs: I am an ordinary function              # It outputs \"I am an ordinary function\", because that\u2019s just what you do: # calling a function. Nothing magic.  @my_decorator def lazy_function():     print(\"zzzzzzzz\")      #outputs: I am an ordinary function    It\u2019s exactly the same. \" my_decorator \" is called. So when you  @my_decorator , you are telling Python to call the function 'labelled by the variable \" my_decorator \"'.   This is important! The label you give can point directly to the decorator\u2014 or not .   Let\u2019s get evil. \u263a   def decorator_maker():          print(\"I make decorators! I am executed only once: \"           \"when you make me create a decorator.\")                  def my_decorator(func):                  print(\"I am a decorator! I am executed only when you decorate a function.\")                         def wrapped():             print(\"I am the wrapper around the decorated function. \"                   \"I am called when you call the decorated function. \"                   \"As the wrapper, I return the RESULT of the decorated function.\")             return func()                  print(\"As the decorator, I return the wrapped function.\")                  return wrapped          print(\"As a decorator maker, I return a decorator\")     return my_decorator              # Let\u2019s create a decorator. It\u2019s just a new function after all. new_decorator = decorator_maker()        #outputs: #I make decorators! I am executed only once: when you make me create a decorator. #As a decorator maker, I return a decorator  # Then we decorate the function              def decorated_function():     print(\"I am the decorated function.\")     decorated_function = new_decorator(decorated_function) #outputs: #I am a decorator! I am executed only when you decorate a function. #As the decorator, I return the wrapped function       # Let\u2019s call the function: decorated_function() #outputs: #I am the wrapper around the decorated function. I am called when you call the decorated function. #As the wrapper, I return the RESULT of the decorated function. #I am the decorated function.    No surprise here.   Let\u2019s do EXACTLY the same thing, but skip all the pesky intermediate variables:   def decorated_function():     print(\"I am the decorated function.\") decorated_function = decorator_maker()(decorated_function) #outputs: #I make decorators! I am executed only once: when you make me create a decorator. #As a decorator maker, I return a decorator #I am a decorator! I am executed only when you decorate a function. #As the decorator, I return the wrapped function.  # Finally: decorated_function()     #outputs: #I am the wrapper around the decorated function. I am called when you call the decorated function. #As the wrapper, I return the RESULT of the decorated function. #I am the decorated function.    Let\u2019s make it  even shorter :   @decorator_maker() def decorated_function():     print(\"I am the decorated function.\") #outputs: #I make decorators! I am executed only once: when you make me create a decorator. #As a decorator maker, I return a decorator #I am a decorator! I am executed only when you decorate a function. #As the decorator, I return the wrapped function.  #Eventually:  decorated_function()     #outputs: #I am the wrapper around the decorated function. I am called when you call the decorated function. #As the wrapper, I return the RESULT of the decorated function. #I am the decorated function.    Hey, did you see that? We used a function call with the \" @ \" syntax! :-)   So, back to decorators with arguments. If we can use functions to generate the decorator on the fly, we can pass arguments to that function, right?   def decorator_maker_with_arguments(decorator_arg1, decorator_arg2):          print(\"I make decorators! And I accept arguments: {0}, {1}\".format(decorator_arg1, decorator_arg2))                  def my_decorator(func):         # The ability to pass arguments here is a gift from closures.         # If you are not comfortable with closures, you can assume it\u2019s ok,         # or read: https://stackoverflow.com/questions/13857/can-you-explain-closures-as-they-relate-to-python         print(\"I am the decorator. Somehow you passed me arguments: {0}, {1}\".format(decorator_arg1, decorator_arg2))                         # Don't confuse decorator arguments and function arguments!         def wrapped(function_arg1, function_arg2) :             print(\"I am the wrapper around the decorated function.\\n\"                   \"I can access all the variables\\n\"                   \"\\t- from the decorator: {0} {1}\\n\"                   \"\\t- from the function call: {2} {3}\\n\"                   \"Then I can pass them to the decorated function\"                   .format(decorator_arg1, decorator_arg2,                           function_arg1, function_arg2))             return func(function_arg1, function_arg2)                  return wrapped          return my_decorator  @decorator_maker_with_arguments(\"Leonard\", \"Sheldon\") def decorated_function_with_arguments(function_arg1, function_arg2):     print(\"I am the decorated function and only knows about my arguments: {0}\"            \" {1}\".format(function_arg1, function_arg2))            decorated_function_with_arguments(\"Rajesh\", \"Howard\") #outputs: #I make decorators! And I accept arguments: Leonard Sheldon #I am the decorator. Somehow you passed me arguments: Leonard Sheldon #I am the wrapper around the decorated function.  #I can access all the variables  #   - from the decorator: Leonard Sheldon  #   - from the function call: Rajesh Howard  #Then I can pass them to the decorated function #I am the decorated function and only knows about my arguments: Rajesh Howard    Here it is: a decorator with arguments. Arguments can be set as variable:   c1 = \"Penny\" c2 = \"Leslie\"  @decorator_maker_with_arguments(\"Leonard\", c1) def decorated_function_with_arguments(function_arg1, function_arg2):     print(\"I am the decorated function and only knows about my arguments:\"            \" {0} {1}\".format(function_arg1, function_arg2))  decorated_function_with_arguments(c2, \"Howard\") #outputs: #I make decorators! And I accept arguments: Leonard Penny #I am the decorator. Somehow you passed me arguments: Leonard Penny #I am the wrapper around the decorated function.  #I can access all the variables  #   - from the decorator: Leonard Penny  #   - from the function call: Leslie Howard  #Then I can pass them to the decorated function #I am the decorated function and only know about my arguments: Leslie Howard    As you can see, you can pass arguments to the decorator like any function using this trick. You can even use  *args, **kwargs  if you wish. But remember decorators are called  only once . Just when Python imports the script. You can't dynamically set the arguments afterwards. When you do \"import x\",  the function is already decorated , so you can't change anything.     Let\u2019s practice: decorating a decorator   Okay, as a bonus, I'll give you a snippet to make any decorator accept generically any argument. After all, in order to accept arguments, we created our decorator using another function.   We wrapped the decorator.   Anything else we saw recently that wrapped function?   Oh yes, decorators!   Let\u2019s have some fun and write a decorator for the decorators:   def decorator_with_args(decorator_to_enhance):     \"\"\"      This function is supposed to be used as a decorator.     It must decorate an other function, that is intended to be used as a decorator.     Take a cup of coffee.     It will allow any decorator to accept an arbitrary number of arguments,     saving you the headache to remember how to do that every time.     \"\"\"          # We use the same trick we did to pass arguments     def decorator_maker(*args, **kwargs):                 # We create on the fly a decorator that accepts only a function         # but keeps the passed arguments from the maker.         def decorator_wrapper(func):                     # We return the result of the original decorator, which, after all,              # IS JUST AN ORDINARY FUNCTION (which returns a function).             # Only pitfall: the decorator must have this specific signature or it won't work:             return decorator_to_enhance(func, *args, **kwargs)                  return decorator_wrapper          return decorator_maker            It can be used as follows:   # You create the function you will use as a decorator. And stick a decorator on it :-) # Don't forget, the signature is \"decorator(func, *args, **kwargs)\" @decorator_with_args  def decorated_decorator(func, *args, **kwargs):      def wrapper(function_arg1, function_arg2):         print(\"Decorated with {0} {1}\".format(args, kwargs))         return func(function_arg1, function_arg2)     return wrapper      # Then you decorate the functions you wish with your brand new decorated decorator.  @decorated_decorator(42, 404, 1024) def decorated_function(function_arg1, function_arg2):     print(\"Hello {0} {1}\".format(function_arg1, function_arg2))  decorated_function(\"Universe and\", \"everything\") #outputs: #Decorated with (42, 404, 1024) {} #Hello Universe and everything  # Whoooot!    I know, the last time you had this feeling, it was after listening a guy saying: \"before understanding recursion, you must first understand recursion\". But now, don't you feel good about mastering this?     Best practices: decorators     Decorators were introduced in Python 2.4, so be sure your code will be run on >= 2.4.   Decorators slow down the function call. Keep that in mind.   You cannot un-decorate a function.  (There  are  hacks to create decorators that can be removed, but nobody uses them.) So once a function is decorated, it\u2019s decorated  for all the code .   Decorators wrap functions, which can make them hard to debug.  (This gets better from Python >= 2.5; see below.)     The  functools  module was introduced in Python 2.5. It includes the function  functools.wraps() , which copies the name, module, and docstring of the decorated function to its wrapper.   (Fun fact:  functools.wraps()  is a decorator! \u263a)   # For debugging, the stacktrace prints you the function __name__ def foo():     print(\"foo\")      print(foo.__name__) #outputs: foo      # With a decorator, it gets messy     def bar(func):     def wrapper():         print(\"bar\")         return func()     return wrapper  @bar def foo():     print(\"foo\")  print(foo.__name__) #outputs: wrapper  # \"functools\" can help for that  import functools  def bar(func):     # We say that \"wrapper\", is wrapping \"func\"     # and the magic begins     @functools.wraps(func)     def wrapper():         print(\"bar\")         return func()     return wrapper  @bar def foo():     print(\"foo\")  print(foo.__name__) #outputs: foo      How can the decorators be useful?   Now the big question:  What can I use decorators for?   Seem cool and powerful, but a practical example would be great. Well, there are 1000 possibilities. Classic uses are extending a function behavior from an external lib (you can't modify it), or for debugging (you don't want to modify it because it\u2019s temporary).   You can use them to extend several functions in a DRY\u2019s way, like so:   def benchmark(func):     \"\"\"     A decorator that prints the time a function takes     to execute.     \"\"\"     import time     def wrapper(*args, **kwargs):         t = time.clock()         res = func(*args, **kwargs)         print(\"{0} {1}\".format(func.__name__, time.clock()-t))         return res     return wrapper   def logging(func):     \"\"\"     A decorator that logs the activity of the script.     (it actually just prints it, but it could be logging!)     \"\"\"     def wrapper(*args, **kwargs):         res = func(*args, **kwargs)         print(\"{0} {1} {2}\".format(func.__name__, args, kwargs))         return res     return wrapper   def counter(func):     \"\"\"     A decorator that counts and prints the number of times a function has been executed     \"\"\"     def wrapper(*args, **kwargs):         wrapper.count = wrapper.count + 1         res = func(*args, **kwargs)         print(\"{0} has been used: {1}x\".format(func.__name__, wrapper.count))         return res     wrapper.count = 0     return wrapper  @counter @benchmark @logging def reverse_string(string):     return str(reversed(string))  print(reverse_string(\"Able was I ere I saw Elba\")) print(reverse_string(\"A man, a plan, a canoe, pasta, heros, rajahs, a coloratura, maps, snipe, percale, macaroni, a gag, a banana bag, a tan, a tag, a banana bag again (or a camel), a crepe, pins, Spam, a rut, a Rolo, cash, a jar, sore hats, a peon, a canal: Panama!\"))  #outputs: #reverse_string ('Able was I ere I saw Elba',) {} #wrapper 0.0 #wrapper has been used: 1x  #ablE was I ere I saw elbA #reverse_string ('A man, a plan, a canoe, pasta, heros, rajahs, a coloratura, maps, snipe, percale, macaroni, a gag, a banana bag, a tan, a tag, a banana bag again (or a camel), a crepe, pins, Spam, a rut, a Rolo, cash, a jar, sore hats, a peon, a canal: Panama!',) {} #wrapper 0.0 #wrapper has been used: 2x #!amanaP :lanac a ,noep a ,stah eros ,raj a ,hsac ,oloR a ,tur a ,mapS ,snip ,eperc a ,)lemac a ro( niaga gab ananab a ,gat a ,nat a ,gab ananab a ,gag a ,inoracam ,elacrep ,epins ,spam ,arutaroloc a ,shajar ,soreh ,atsap ,eonac a ,nalp a ,nam A    Of course the good thing with decorators is that you can use them right away on almost anything without rewriting. DRY, I said:   @counter @benchmark @logging def get_random_futurama_quote():     from urllib import urlopen     result = urlopen(\"http://subfusion.net/cgi-bin/quote.pl?quote=futurama\").read()     try:         value = result.split(\" \")[1].split(\" \")[0]         return value.strip()     except:         return \"No, I'm ... doesn't!\"       print(get_random_futurama_quote()) print(get_random_futurama_quote())  #outputs: #get_random_futurama_quote () {} #wrapper 0.02 #wrapper has been used: 1x #The laws of science be a harsh mistress. #get_random_futurama_quote () {} #wrapper 0.01 #wrapper has been used: 2x #Curse you, merciful Poseidon!    Python itself provides several decorators:  property ,  staticmethod , etc.     Django uses decorators to manage caching and view permissions.   Twisted to fake inlining asynchronous functions calls.     This really is a large playground.", "score": 4872}}
{"question": "How do I check if a list is empty?", "tags": ["python", "list"], "link": "https://stackoverflow.com/questions/53513/how-do-i-check-if-a-list-is-empty", "answer_count": 27, "answers": {"id": 7302987, "body": "I prefer it explicitly:   if len(li) == 0:     print('the list is empty')    This way it's 100% clear that  li  is a sequence (list) and we want to test its size. My problem with  if not li: ...  is that it gives the false impression that  li  is a boolean variable.", "score": 1138}}
{"question": "How do I check if a list is empty?", "tags": ["python", "list"], "link": "https://stackoverflow.com/questions/53513/how-do-i-check-if-a-list-is-empty", "answer_count": 27, "answers": {"id": 53752, "body": "The Pythonic way to do it is from the  PEP 8 style guide .     For sequences, (strings, lists, tuples), use the fact that empty sequences are false:   # Correct: if not seq: if seq:  # Wrong: if len(seq): if not len(seq):", "score": 1544}}
{"question": "How do I check if a list is empty?", "tags": ["python", "list"], "link": "https://stackoverflow.com/questions/53513/how-do-i-check-if-a-list-is-empty", "answer_count": 27, "answers": {"id": 53522, "body": "if not a:     print(\"List is empty\")    Using the  implicit booleanness  of the empty  list  is quite Pythonic.", "score": 7280}}
{"question": "How do I concatenate two lists in Python?", "tags": ["python", "list", "concatenation"], "link": "https://stackoverflow.com/questions/1720421/how-do-i-concatenate-two-lists-in-python", "answer_count": 31, "answers": {"id": 1724975, "body": "It's also possible to create a generator that simply iterates over the items in both lists using  itertools.chain() . This allows you to chain lists (or any iterable) together for processing without copying the items to a new list:   import itertools for item in itertools.chain(listone, listtwo):     # Do something with each list item", "score": 406}}
{"question": "How do I concatenate two lists in Python?", "tags": ["python", "list", "concatenation"], "link": "https://stackoverflow.com/questions/1720421/how-do-i-concatenate-two-lists-in-python", "answer_count": 31, "answers": {"id": 35631185, "body": "Python >= 3.5 alternative:  [*l1, *l2]   Another alternative has been introduced via the acceptance of  PEP 448  which deserves mentioning.   The PEP, titled  Additional Unpacking Generalizations , generally reduced some syntactic restrictions when using the starred  *  expression in Python; with it, joining two lists (applies to any iterable) can now also be done with:   >>> l1 = [1, 2, 3] >>> l2 = [4, 5, 6] >>> joined_list = [*l1, *l2]  # unpack both iterables in a list literal >>> print(joined_list) [1, 2, 3, 4, 5, 6]    This functionality  was defined  for Python 3.5, but it hasn't been backported to previous versions in the 3.x family. In unsupported versions a  SyntaxError  is going to be raised.   As with the other approaches, this too  creates as shallow copy  of the elements in the corresponding lists.     The  upside  to this approach is that you really don't need lists in order to perform it; anything that is iterable will do. As stated in the PEP:     This is also useful as a more readable way of summing iterables into a list, such as  my_list + list(my_tuple) + list(my_range)  which is now equivalent to just  [*my_list, *my_tuple, *my_range] .     So while addition with  +  would raise a  TypeError  due to type mismatch:   l = [1, 2, 3] r = range(4, 7) res = l + r    The following won't:   res = [*l, *r]    because it will first unpack the contents of the iterables and then simply create a  list  from the contents.", "score": 681}}
{"question": "How do I concatenate two lists in Python?", "tags": ["python", "list", "concatenation"], "link": "https://stackoverflow.com/questions/1720421/how-do-i-concatenate-two-lists-in-python", "answer_count": 31, "answers": {"id": 1720432, "body": "Use the  +  operator to combine the lists:   listone = [1, 2, 3] listtwo = [4, 5, 6]  joinedlist = listone + listtwo    Output:   >>> joinedlist [1, 2, 3, 4, 5, 6]    NOTE: This will create a new list with a shallow copy of the items in the first list, followed by a shallow copy of the items in the second list. Use  copy.deepcopy()  to get deep copies of lists.", "score": 5607}}
{"question": "How do I change the size of figures drawn with Matplotlib?", "tags": ["python", "pandas", "matplotlib", "seaborn", "figsize"], "link": "https://stackoverflow.com/questions/332289/how-do-i-change-the-size-of-figures-drawn-with-matplotlib", "answer_count": 16, "answers": {"id": 41717533, "body": "Using plt.rcParams   There is also this workaround in case you want to change the size without using the figure environment. So in case you are using  plt.plot()  for example, you can set a tuple with width and height.   import matplotlib.pyplot as plt plt.rcParams[\"figure.figsize\"] = (20,3)    This is very useful when you plot inline (e.g., with  IPython Notebook ). As  asmaier noticed , it is preferable to not put this statement in the same cell of the imports statements.   To reset the global figure size back to default for subsequent plots:   plt.rcParams[\"figure.figsize\"] = plt.rcParamsDefault[\"figure.figsize\"]    Conversion to cm   The  figsize  tuple accepts inches, so if you want to set it in centimetres you have to divide them by 2.54. Have a look at  this question .", "score": 802}}
{"question": "How do I change the size of figures drawn with Matplotlib?", "tags": ["python", "pandas", "matplotlib", "seaborn", "figsize"], "link": "https://stackoverflow.com/questions/332289/how-do-i-change-the-size-of-figures-drawn-with-matplotlib", "answer_count": 16, "answers": {"id": 4306340, "body": "If you've already got the figure created, you can use  figure.set_size_inches  to adjust the figure size:   fig = matplotlib.pyplot.gcf() fig.set_size_inches(18.5, 10.5) fig.savefig('test2png.png', dpi=100)    To propagate the size change to an existing GUI window, add  forward=True :   fig.set_size_inches(18.5, 10.5, forward=True)    Additionally as  Erik Shilts  mentioned in the comments you can also use  figure.set_dpi  to \"[s]et the resolution of the figure in dots-per-inch\"   fig.set_dpi(100)", "score": 1189}}
{"question": "How do I change the size of figures drawn with Matplotlib?", "tags": ["python", "pandas", "matplotlib", "seaborn", "figsize"], "link": "https://stackoverflow.com/questions/332289/how-do-i-change-the-size-of-figures-drawn-with-matplotlib", "answer_count": 16, "answers": {"id": 638443, "body": "figure  tells you the call signature:   from matplotlib.pyplot import figure  figure(figsize=(8, 6), dpi=80)    figure(figsize=(1,1))  would create an inch-by-inch image, which would be 80-by-80 pixels unless you also give a different dpi argument.", "score": 1923}}
{"question": "How do I make a time delay?", "tags": ["python", "delay", "sleep", "timedelay"], "link": "https://stackoverflow.com/questions/510348/how-do-i-make-a-time-delay", "answer_count": 13, "answers": {"id": 44666336, "body": "How can I make a time delay in Python?     In a single thread I suggest the  sleep function :   >>> from time import sleep  >>> sleep(4)    This function actually suspends the processing of the thread in which it is called by the operating system, allowing other threads and processes to execute while it sleeps.   Use it for that purpose, or simply to delay a function from executing. For example:   >>> def party_time(): ...     print('hooray!') ... >>> sleep(3); party_time() hooray!    \"hooray!\" is printed 3 seconds after I hit  Enter .   Example using  sleep  with multiple threads and processes   Again,  sleep  suspends your thread - it uses next to zero processing power.   To demonstrate, create a script like this (I first attempted this in an interactive Python 3.5 shell, but sub-processes can't find the  party_later  function for some reason):   from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed from time import sleep, time  def party_later(kind='', n=''):     sleep(3)     return kind + n + ' party time!: ' + __name__  def main():     with ProcessPoolExecutor() as proc_executor:         with ThreadPoolExecutor() as thread_executor:             start_time = time()             proc_future1 = proc_executor.submit(party_later, kind='proc', n='1')             proc_future2 = proc_executor.submit(party_later, kind='proc', n='2')             thread_future1 = thread_executor.submit(party_later, kind='thread', n='1')             thread_future2 = thread_executor.submit(party_later, kind='thread', n='2')             for f in as_completed([               proc_future1, proc_future2, thread_future1, thread_future2,]):                 print(f.result())             end_time = time()     print('total time to execute four 3-sec functions:', end_time - start_time)  if __name__ == '__main__':     main()    Example output from this script:   thread1 party time!: __main__ thread2 party time!: __main__ proc1 party time!: __mp_main__ proc2 party time!: __mp_main__ total time to execute four 3-sec functions: 3.4519670009613037    Multithreading   You can trigger a function to be called at a later time in a separate thread with the  Timer   threading  object:   >>> from threading import Timer >>> t = Timer(3, party_time, args=None, kwargs=None) >>> t.start() >>> >>> hooray!  >>>    The blank line illustrates that the function printed to my standard output, and I had to hit  Enter  to ensure I was on a prompt.   The upside of this method is that while the  Timer  thread was waiting, I was able to do other things, in this case, hitting  Enter  one time - before the function executed (see the first empty prompt).   There isn't a respective object in the  multiprocessing library . You can create one, but it probably doesn't exist for a reason. A sub-thread makes a lot more sense for a simple timer than a whole new subprocess.", "score": 115}}
{"question": "How do I make a time delay?", "tags": ["python", "delay", "sleep", "timedelay"], "link": "https://stackoverflow.com/questions/510348/how-do-i-make-a-time-delay", "answer_count": 13, "answers": {"id": 64486, "body": "Use  sleep()  from the  time  module. It can take a float argument for sub-second resolution.   from time import sleep sleep(0.1)  # Time in seconds", "score": 876}}
{"question": "How do I make a time delay?", "tags": ["python", "delay", "sleep", "timedelay"], "link": "https://stackoverflow.com/questions/510348/how-do-i-make-a-time-delay", "answer_count": 13, "answers": {"id": 510351, "body": "This delays for 2.5 seconds:   import time  time.sleep(2.5)      Here is another example where something is run approximately once a minute:   import time  while True:     print(\"This prints once a minute.\")     time.sleep(60) # Delay for 1 minute (60 seconds).", "score": 3532}}
{"question": "Understanding Python super() with __init__() methods", "tags": ["python", "class", "oop", "inheritance", "super"], "link": "https://stackoverflow.com/questions/576169/understanding-python-super-with-init-methods", "answer_count": 7, "answers": {"id": 19257335, "body": "It's been noted that in Python 3.0+ you can use   super().__init__()    to make your call, which is concise and does not require you to reference the parent OR class names explicitly, which can be handy. I just want to add that for Python 2.7 or under, some people implement a name-insensitive behaviour by writing  self.__class__  instead of the class name, i.e.   super(self.__class__, self).__init__()  # DON'T DO THIS!    HOWEVER, this breaks calls to  super  for any classes that inherit from your class, where  self.__class__  could return a child class. For example:   class Polygon(object):     def __init__(self, id):         self.id = id  class Rectangle(Polygon):     def __init__(self, id, width, height):         super(self.__class__, self).__init__(id)         self.shape = (width, height)  class Square(Rectangle):     pass    Here I have a class  Square , which is a sub-class of  Rectangle . Say I don't want to write a separate constructor for  Square  because the constructor for  Rectangle  is good enough, but for whatever reason I want to implement a Square so I can reimplement some other method.   When I create a  Square  using  mSquare = Square('a', 10,10) , Python calls the constructor for  Rectangle  because I haven't given  Square  its own constructor. However, in the constructor for  Rectangle , the call  super(self.__class__,self)  is going to return the superclass of  mSquare , so it calls the constructor for  Rectangle  again. This is how the infinite loop happens, as was mentioned by @S_C. In this case, when I run  super(...).__init__()  I am calling the constructor for  Rectangle  but since I give it no arguments, I will get an error.", "score": 283}}
{"question": "Understanding Python super() with __init__() methods", "tags": ["python", "class", "oop", "inheritance", "super"], "link": "https://stackoverflow.com/questions/576169/understanding-python-super-with-init-methods", "answer_count": 7, "answers": {"id": 27134600, "body": "I'm trying to understand  super()     The reason we use  super  is so that child classes that may be using cooperative multiple inheritance will call the correct next parent class function in the Method Resolution Order (MRO).   In Python 3, we can call it like this:   class ChildB(Base):     def __init__(self):         super().__init__()    In Python 2, we were required to call  super  like this with the defining class's name and  self , but we'll avoid this from now on because it's redundant, slower (due to the name lookups), and more verbose (so update your Python if you haven't already!):           super(ChildB, self).__init__()    Without super, you are limited in your ability to use multiple inheritance because you hard-wire the next parent's call:           Base.__init__(self) # Avoid this.    I further explain below.     \"What difference is there actually in this code?:\"     class ChildA(Base):     def __init__(self):         Base.__init__(self)  class ChildB(Base):     def __init__(self):         super().__init__()    The primary difference in this code is that in  ChildB  you get a layer of indirection in the  __init__  with  super , which uses the class in which it is defined to determine the next class's  __init__  to look up in the MRO.   I illustrate this difference in an answer at the  canonical question, How to use 'super' in Python? , which demonstrates  dependency injection  and  cooperative multiple inheritance .   If Python didn't have  super   Here's code that's actually closely equivalent to  super  (how it's implemented in C, minus some checking and fallback behavior, and translated to Python):   class ChildB(Base):     def __init__(self):         mro = type(self).mro()         check_next = mro.index(ChildB) + 1 # next after *this* class.         while check_next < len(mro):             next_class = mro[check_next]             if '__init__' in next_class.__dict__:                 next_class.__init__(self)                 break             check_next += 1    Written a little more like native Python:   class ChildB(Base):     def __init__(self):         mro = type(self).mro()         for next_class in mro[mro.index(ChildB) + 1:]: # slice to end             if hasattr(next_class, '__init__'):                 next_class.__init__(self)                 break    If we didn't have the  super  object, we'd have to write this manual code everywhere (or recreate it!) to ensure that we call the proper next method in the Method Resolution Order!   How does super do this in Python 3 without being told explicitly which class and instance from the method it was called from?   It gets the calling stack frame, and finds the class (implicitly stored as a local free variable,  __class__ , making the calling function a closure over the class) and the first argument to that function, which should be the instance or class that informs it which Method Resolution Order (MRO) to use.   Since it requires that first argument for the MRO,  using  super  with static methods is impossible as they do not have access to the MRO of the class from which they are called .   Criticisms of other answers:     super()  lets you avoid referring to the base class explicitly, which can be nice. . But the main advantage comes with multiple inheritance, where all sorts of fun stuff can happen. See the standard docs on super if you haven't already.     It's rather hand-wavey and doesn't tell us much, but the point of  super  is not to avoid writing the parent class. The point is to ensure that the next method in line in the method resolution order (MRO) is called. This becomes important in multiple inheritance.   I'll explain here.   class Base(object):     def __init__(self):         print(\"Base init'ed\")  class ChildA(Base):     def __init__(self):         print(\"ChildA init'ed\")         Base.__init__(self)  class ChildB(Base):     def __init__(self):         print(\"ChildB init'ed\")         super().__init__()    And let's create a dependency that we want to be called after the Child:   class UserDependency(Base):     def __init__(self):         print(\"UserDependency init'ed\")         super().__init__()    Now remember,  ChildB  uses super,  ChildA  does not:   class UserA(ChildA, UserDependency):     def __init__(self):         print(\"UserA init'ed\")         super().__init__()  class UserB(ChildB, UserDependency):     def __init__(self):         print(\"UserB init'ed\")         super().__init__()    And  UserA  does not call the UserDependency method:   >>> UserA() UserA init'ed ChildA init'ed Base init'ed <__main__.UserA object at 0x0000000003403BA8>    But  UserB  does in-fact call UserDependency because  ChildB  invokes  super :   >>> UserB() UserB init'ed ChildB init'ed UserDependency init'ed Base init'ed <__main__.UserB object at 0x0000000003403438>    Criticism for another answer   In no circumstance should you do the following, which another answer suggests, as you'll definitely get errors when you subclass ChildB:   super(self.__class__, self).__init__()  # DON'T DO THIS! EVER.    (That answer is not clever or particularly interesting, but in spite of direct criticism in the comments and over 17 downvotes, the answerer persisted in suggesting it until a kind editor fixed his problem.)   Explanation: Using  self.__class__  as a substitute for explicitly passing the class by name in  super()  will lead to recursion.  super  lets us look up the next parent in the MRO (see the first section of this answer) for child classes. If we tell  super  we're in the child's method, it will then lookup the next method in line (probably this same one we are calling it from) resulting in recursion, causing either a logical failure (as in the answerer's example) or a  RuntimeError  when the maximum recursion depth is exceeded.   class Polygon(object):     def __init__(self, id):         self.id = id  class Rectangle(Polygon):     def __init__(self, id, width, height):         super(self.__class__, self).__init__(id)         self.shape = (width, height)  class Square(Rectangle):     pass  >>> Square('a', 10, 10) Traceback (most recent call last):   File \" \", line 1, in     File \" \", line 3, in __init__ TypeError: __init__() missing 2 required positional arguments: 'width' and 'height'    Python 3's new  super()  calling method with no arguments fortunately allows us to sidestep this issue.", "score": 1292}}
{"question": "Understanding Python super() with __init__() methods", "tags": ["python", "class", "oop", "inheritance", "super"], "link": "https://stackoverflow.com/questions/576169/understanding-python-super-with-init-methods", "answer_count": 7, "answers": {"id": 576183, "body": "super()  lets you avoid referring to the base class explicitly, which can be nice. But the main advantage comes with multiple inheritance, where all sorts of  fun stuff  can happen. See the  standard docs on super  if you haven't already.   Note that  the syntax changed in Python 3.0 : you can just say  super().__init__()  instead of  super(ChildB, self).__init__()  which IMO is quite a bit nicer. The standard docs also refer to a  guide to using  super()  which is quite explanatory.", "score": 2362}}
{"question": "How do I print colored text to the terminal?", "tags": ["python", "terminal", "output", "ansi-colors"], "link": "https://stackoverflow.com/questions/287871/how-do-i-print-colored-text-to-the-terminal", "answer_count": 67, "answers": {"id": 3332860, "body": "The answer is  Colorama  for all cross-platform coloring in Python.   It supports Python 3.5+ as well as Python 2.7.   And as of January 2023, it is maintained.   Example Code:   from colorama import init as colorama_init from colorama import Fore from colorama import Style  colorama_init()  print(f\"This is {Fore.GREEN}color{Style.RESET_ALL}!\")    Example Screenshot:", "score": 1107}}
{"question": "How do I print colored text to the terminal?", "tags": ["python", "terminal", "output", "ansi-colors"], "link": "https://stackoverflow.com/questions/287871/how-do-i-print-colored-text-to-the-terminal", "answer_count": 67, "answers": {"id": 293633, "body": "There is also the  Python termcolor module . Usage is pretty simple:   from termcolor import colored  print(colored('hello', 'red'), colored('world', 'green'))    It may not be sophisticated enough, however, for game programming and the \"colored blocks\" that you want to do...   To get the ANSI codes working on windows, first run   os.system('color')", "score": 1214}}
{"question": "How do I print colored text to the terminal?", "tags": ["python", "terminal", "output", "ansi-colors"], "link": "https://stackoverflow.com/questions/287871/how-do-i-print-colored-text-to-the-terminal", "answer_count": 67, "answers": {"id": 287944, "body": "This somewhat depends on what platform you are on. The most common way to do this is by printing ANSI escape sequences. For a simple example, here's some Python code from the  Blender build scripts :   class bcolors:     HEADER = '\\033[95m'     OKBLUE = '\\033[94m'     OKCYAN = '\\033[96m'     OKGREEN = '\\033[92m'     WARNING = '\\033[93m'     FAIL = '\\033[91m'     ENDC = '\\033[0m'     BOLD = '\\033[1m'     UNDERLINE = '\\033[4m'    To use code like this, you can do something like:   print(bcolors.WARNING + \"Warning: No active frommets remain. Continue?\" + bcolors.ENDC)    Or, with Python 3.6+:   print(f\"{bcolors.WARNING}Warning: No active frommets remain. Continue?{bcolors.ENDC}\")    This will work on unixes including OS X, Linux and Windows (provided you use  ANSICON , or in Windows 10 provided you enable  VT100 emulation ). There are ANSI codes for setting the color, moving the cursor, and more.   If you are going to get complicated with this (and it sounds like you are if you are writing a game), you should look into the \" curses \" module, which handles a lot of the complicated parts of this for you. The  Python Curses HowTO  is a good introduction.   If you are not using extended ASCII (i.e., not on a PC), you are stuck with the ASCII characters below 127, and '#' or '@' is probably your best bet for a block. If you can ensure your terminal is using a IBM  extended ASCII character set , you have many more options. Characters 176, 177, 178 and 219 are the \"block characters\".   Some modern text-based programs, such as \"Dwarf Fortress\", emulate text mode in a graphical mode, and use images of the classic PC font. You can find some of these bitmaps that you can use on the  Dwarf Fortress Wiki  see ( user-made tilesets ).   The  Text Mode Demo Contest  has more resources for doing graphics in text mode.", "score": 2998}}
{"question": "Manually raising (throwing) an exception in Python", "tags": ["python", "exception"], "link": "https://stackoverflow.com/questions/2052390/manually-raising-throwing-an-exception-in-python", "answer_count": 11, "answers": {"id": 40493467, "body": "In Python 3 there are four different syntaxes for raising exceptions:     raise exception   raise exception (args)   raise   raise exception (args) from original_exception     1. Raise exception vs. 2. raise exception (args)   If you use  raise exception (args)  to raise an exception then the  args  will be printed when you print the exception object - as shown in the example below.   # Raise exception (args) try:     raise ValueError(\"I have raised an Exception\") except ValueError as exp:     print(\"Error\", exp)     # Output -> Error I have raised an Exception   # Raise exception try:     raise ValueError except ValueError as exp:     print(\"Error\", exp)     # Output -> Error    3. Statement  raise   The  raise  statement without any arguments re-raises the last exception.   This is useful if you need to perform some actions after catching the exception and then want to re-raise it. But if there wasn't any exception before, the  raise  statement raises  a  TypeError  Exception.   def somefunction():     print(\"some cleaning\")  a = 10 b = 0 result = None  try:     result = a / b     print(result)  except Exception:            # Output ->     somefunction()           # Some cleaning     raise                    # Traceback (most recent call last):                              # File \"python\", line 9, in                                # ZeroDivisionError: division by zero    4. Raise exception (args) from original_exception   This statement is used to create exception chaining in which an exception that is raised in response to another exception can contain the details of the original exception - as shown in the example below.   class MyCustomException(Exception):     pass  a = 10 b = 0 reuslt = None try:     try:         result = a / b      except ZeroDivisionError as exp:         print(\"ZeroDivisionError -- \",exp)         raise MyCustomException(\"Zero Division \") from exp  except MyCustomException as exp:     print(\"MyException\",exp)     print(exp.__cause__)    Output:   ZeroDivisionError --  division by zero MyException Zero Division division by zero", "score": 109}}
{"question": "Manually raising (throwing) an exception in Python", "tags": ["python", "exception"], "link": "https://stackoverflow.com/questions/2052390/manually-raising-throwing-an-exception-in-python", "answer_count": 11, "answers": {"id": 2052396, "body": "Don't do this . Raising a bare  Exception  is absolutely  not  the right thing to do; see  Aaron Hall's excellent answer  instead.     It can't get much more Pythonic than this:   raise Exception(\"I know Python!\")    Replace  Exception  with the specific type of exception you want to throw.   See  the raise statement documentation  for Python if you'd like more information.", "score": 583}}
{"question": "Manually raising (throwing) an exception in Python", "tags": ["python", "exception"], "link": "https://stackoverflow.com/questions/2052390/manually-raising-throwing-an-exception-in-python", "answer_count": 11, "answers": {"id": 24065533, "body": "How do I manually throw/raise an exception in Python?     Use the most specific Exception constructor that semantically fits your issue .   Be specific in your message, e.g.:   raise ValueError('A very specific bad thing happened.')    Don't raise generic exceptions   Avoid raising a generic  Exception . To catch it, you'll have to catch all other more specific exceptions that subclass it.   Problem 1: Hiding bugs   raise Exception('I know Python!') # Don't! If you catch, likely to hide bugs.    For example:   def demo_bad_catch():     try:         raise ValueError('Represents a hidden bug, do not catch this')         raise Exception('This is the exception you expect to handle')     except Exception as error:         print('Caught this error: ' + repr(error))  >>> demo_bad_catch() Caught this error: ValueError('Represents a hidden bug, do not catch this',)    Problem 2: Won't catch   And more specific catches won't catch the general exception:   def demo_no_catch():     try:         raise Exception('general exceptions not caught by specific handling')     except ValueError as e:         print('we will not catch exception: Exception')    >>> demo_no_catch() Traceback (most recent call last):   File \" \", line 1, in     File \" \", line 3, in demo_no_catch Exception: general exceptions not caught by specific handling    Best Practices:  raise  statement   Instead, use the most specific Exception constructor that semantically fits your issue .   raise ValueError('A very specific bad thing happened')    which also handily allows an arbitrary number of arguments to be passed to the constructor:   raise ValueError('A very specific bad thing happened', 'foo', 'bar', 'baz')     These arguments are accessed by the  args  attribute on the  Exception  object. For example:   try:     some_code_that_may_raise_our_value_error() except ValueError as err:     print(err.args)    prints   ('message', 'foo', 'bar', 'baz')        In Python 2.5, an actual  message  attribute was added to  BaseException  in favor of encouraging users to subclass Exceptions and stop using  args , but  the introduction of  message  and the original deprecation of args has been retracted .   Best Practices:  except  clause   When inside an except clause, you might want to, for example, log that a specific type of error happened, and then re-raise. The best way to do this while preserving the stack trace is to use a bare raise statement. For example:   logger = logging.getLogger(__name__)  try:     do_something_in_app_that_breaks_easily() except AppError as error:     logger.error(error)     raise                 # just this!     # raise AppError      # Don't do this, you'll lose the stack trace!    Don't modify your errors... but if you insist.   You can preserve the stacktrace (and error value) with  sys.exc_info() , but  this is way more error prone  and  has compatibility problems between Python 2 and 3 , prefer to use a bare  raise  to re-raise.   To explain - the  sys.exc_info()  returns the type, value, and traceback.   type, value, traceback = sys.exc_info()    This is the syntax in Python 2 - note this is not compatible with Python 3:   raise AppError, error, sys.exc_info()[2] # avoid this. # Equivalently, as error *is* the second object: raise sys.exc_info()[0], sys.exc_info()[1], sys.exc_info()[2]    If you want to, you can modify what happens with your new raise - e.g. setting new  args  for the instance:   def error():     raise ValueError('oops!')  def catch_error_modify_message():     try:         error()     except ValueError:         error_type, error_instance, traceback = sys.exc_info()         error_instance.args = (error_instance.args[0] + '  ',)         raise error_type, error_instance, traceback    And we have preserved the whole traceback while modifying the args. Note that this is  not a best practice  and it is  invalid syntax  in Python 3 (making keeping compatibility much harder to work around).   >>> catch_error_modify_message() Traceback (most recent call last):   File \" \", line 1, in     File \" \", line 3, in catch_error_modify_message   File \" \", line 2, in error ValueError: oops!      In  Python 3 :   raise error.with_traceback(sys.exc_info()[2])    Again: avoid manually manipulating tracebacks. It's  less efficient  and more error prone. And if you're using threading and  sys.exc_info  you may even get the wrong traceback (especially if you're using exception handling for control flow - which I'd personally tend to avoid.)   Python 3, Exception chaining   In Python 3, you can chain Exceptions, which preserve tracebacks:   raise RuntimeError('specific message') from error    Be aware:     this  does  allow changing the error type raised, and   this is  not  compatible with Python 2.     Deprecated Methods:   These can easily hide and even get into production code. You want to raise an exception, and doing them will raise an exception,  but not the one intended!   Valid in Python 2, but not in Python 3  is the following:   raise ValueError, 'message' # Don't do this, it's deprecated!    Only  valid in much older versions of Python  (2.4 and lower), you may still see people raising strings:   raise 'message' # really really wrong. don't do this.    In all modern versions, this will actually raise a  TypeError , because you're not raising a  BaseException  type. If you're not checking for the right exception and don't have a reviewer that's aware of the issue, it could get into production.   Example Usage   I raise Exceptions to warn consumers of my API if they're using it incorrectly:   def api_func(foo):     '''foo should be either 'baz' or 'bar'. returns something very useful.'''     if foo not in _ALLOWED_ARGS:         raise ValueError('{foo} wrong, use \"baz\" or \"bar\"'.format(foo=repr(foo)))    Create your own error types when apropos     \"I want to make an error on purpose, so that it would go into the except\"     You can create your own error types, if you want to indicate something specific is wrong with your application, just subclass the appropriate point in the exception hierarchy:   class MyAppLookupError(LookupError):     '''raise this when there's a lookup error for my app'''    and usage:   if important_key not in resource_dict and not ok_to_be_missing:     raise MyAppLookupError('resource is missing, and that is not ok.')", "score": 4305}}
{"question": "How do I pass a variable by reference?", "tags": ["python", "reference", "parameter-passing", "pass-by-reference"], "link": "https://stackoverflow.com/questions/986006/how-do-i-pass-a-variable-by-reference", "answer_count": 43, "answers": {"id": 25670170, "body": "I found the other answers rather long and complicated, so I created this simple diagram to explain the way Python treats variables and parameters.", "score": 468}}
{"question": "How do I pass a variable by reference?", "tags": ["python", "reference", "parameter-passing", "pass-by-reference"], "link": "https://stackoverflow.com/questions/986006/how-do-i-pass-a-variable-by-reference", "answer_count": 43, "answers": {"id": 8140747, "body": "The problem comes from a misunderstanding of what variables are in Python. If you're used to most traditional languages, you have a mental model of what happens in the following sequence:   a = 1 a = 2    You believe that  a  is a memory location that stores the value  1 , then is updated to store the value  2 . That's not how things work in Python. Rather,  a  starts as a reference to an object with the value  1 , then gets reassigned as a reference to an object with the value  2 . Those two objects may continue to coexist even though  a  doesn't refer to the first one anymore; in fact they may be shared by any number of other references within the program.   When you call a function with a parameter, a new reference is created that refers to the object passed in. This is separate from the reference that was used in the function call, so there's no way to update that reference and make it refer to a new object. In your example:   def __init__(self):     self.variable = 'Original'     self.Change(self.variable)  def Change(self, var):     var = 'Changed'    self.variable  is a reference to the string object  'Original' . When you call  Change  you create a second reference  var  to the object. Inside the function you reassign the reference  var  to a different string object  'Changed' , but the reference  self.variable  is separate and does not change.   The only way around this is to pass a mutable object. Because both references refer to the same object, any changes to the object are reflected in both places.   def __init__(self):              self.variable = ['Original']     self.Change(self.variable)  def Change(self, var):     var[0] = 'Changed'", "score": 908}}
{"question": "How do I pass a variable by reference?", "tags": ["python", "reference", "parameter-passing", "pass-by-reference"], "link": "https://stackoverflow.com/questions/986006/how-do-i-pass-a-variable-by-reference", "answer_count": 43, "answers": {"id": 986145, "body": "Arguments are  passed by assignment . The rationale behind this is twofold:     the parameter passed in is actually a  reference  to an object (but the reference is passed by value)   some data types are mutable, but others aren't     So:     If you pass a  mutable  object into a method, the method gets a reference to that same object and you can mutate it to your heart's delight, but if you rebind the reference in the method, the outer scope will know nothing about it, and after you're done, the outer reference will still point at the original object.    If you pass an  immutable  object to a method, you still can't rebind the outer reference, and you can't even mutate the object.     To make it even more clear, let's have some examples.    List - a mutable type   Let's try to modify the list that was passed to a method:   def try_to_change_list_contents(the_list):     print('got', the_list)     the_list.append('four')     print('changed to', the_list)  outer_list = ['one', 'two', 'three']  print('before, outer_list =', outer_list) try_to_change_list_contents(outer_list) print('after, outer_list =', outer_list)    Output:   before, outer_list = ['one', 'two', 'three'] got ['one', 'two', 'three'] changed to ['one', 'two', 'three', 'four'] after, outer_list = ['one', 'two', 'three', 'four']    Since the parameter passed in is a reference to  outer_list , not a copy of it, we can use the mutating list methods to change it and have the changes reflected in the outer scope.   Now let's see what happens when we try to change the reference that was passed in as a parameter:   def try_to_change_list_reference(the_list):     print('got', the_list)     the_list = ['and', 'we', 'can', 'not', 'lie']     print('set to', the_list)  outer_list = ['we', 'like', 'proper', 'English']  print('before, outer_list =', outer_list) try_to_change_list_reference(outer_list) print('after, outer_list =', outer_list)    Output:   before, outer_list = ['we', 'like', 'proper', 'English'] got ['we', 'like', 'proper', 'English'] set to ['and', 'we', 'can', 'not', 'lie'] after, outer_list = ['we', 'like', 'proper', 'English']    Since the  the_list  parameter was passed by value, assigning a new list to it had no effect that the code outside the method could see. The  the_list  was a copy of the  outer_list  reference, and we had  the_list  point to a new list, but there was no way to change where  outer_list  pointed.   String - an immutable type   It's immutable, so there's nothing we can do to change the contents of the string   Now, let's try to change the reference   def try_to_change_string_reference(the_string):     print('got', the_string)     the_string = 'In a kingdom by the sea'     print('set to', the_string)  outer_string = 'It was many and many a year ago'  print('before, outer_string =', outer_string) try_to_change_string_reference(outer_string) print('after, outer_string =', outer_string)    Output:   before, outer_string = It was many and many a year ago got It was many and many a year ago set to In a kingdom by the sea after, outer_string = It was many and many a year ago    Again, since the  the_string  parameter was passed by value, assigning a new string to it had no effect that the code outside the method could see. The  the_string  was a copy of the  outer_string  reference, and we had  the_string  point to a new string, but there was no way to change where  outer_string  pointed.   I hope this clears things up a little.   EDIT:  It's been noted that this doesn't answer the question that @David originally asked, \"Is there something I can do to pass the variable by actual reference?\". Let's work on that.   How do we get around this?   As @Andrea's answer shows, you could return the new value. This doesn't change the way things are passed in, but does let you get the information you want back out:   def return_a_whole_new_string(the_string):     new_string = something_to_do_with_the_old_string(the_string)     return new_string  # then you could call it like my_string = return_a_whole_new_string(my_string)    If you really wanted to avoid using a return value, you could create a class to hold your value and pass it into the function or use an existing class, like a list:   def use_a_wrapper_to_simulate_pass_by_reference(stuff_to_change):     new_string = something_to_do_with_the_old_string(stuff_to_change[0])     stuff_to_change[0] = new_string  # then you could call it like wrapper = [my_string] use_a_wrapper_to_simulate_pass_by_reference(wrapper)  do_something_with(wrapper[0])    Although this seems a little cumbersome.", "score": 3583}}
{"question": "How do I clone a list so that it doesn&#39;t change unexpectedly after assignment?", "tags": ["python", "list", "clone", "mutable"], "link": "https://stackoverflow.com/questions/2612802/how-do-i-clone-a-list-so-that-it-doesnt-change-unexpectedly-after-assignment", "answer_count": 25, "answers": {"id": 17810305, "body": "I've  been told  that Python 3.3+  adds the  list.copy()  method, which should be as fast as slicing:   newlist = old_list.copy()", "score": 186}}
{"question": "How do I clone a list so that it doesn&#39;t change unexpectedly after assignment?", "tags": ["python", "list", "clone", "mutable"], "link": "https://stackoverflow.com/questions/2612802/how-do-i-clone-a-list-so-that-it-doesnt-change-unexpectedly-after-assignment", "answer_count": 25, "answers": {"id": 2612990, "body": "Felix already provided an excellent answer, but I thought I'd do a speed comparison of the various methods:     10.59 sec (105.9 \u00b5s/itn) -   copy.deepcopy(old_list)   10.16 sec (101.6 \u00b5s/itn) - pure Python  Copy()  method copying classes with deepcopy   1.488 sec (14.88 \u00b5s/itn) - pure Python  Copy()  method not copying classes (only dicts/lists/tuples)   0.325 sec (3.25 \u00b5s/itn) -  for item in old_list: new_list.append(item)   0.217 sec (2.17 \u00b5s/itn) -  [i for i in old_list]  (a  list comprehension )   0.186 sec (1.86 \u00b5s/itn) -  copy.copy(old_list)   0.075 sec (0.75 \u00b5s/itn) -  list(old_list)   0.053 sec (0.53 \u00b5s/itn) -  new_list = []; new_list.extend(old_list)   0.039 sec (0.39 \u00b5s/itn) -  old_list[:]  ( list slicing )     So the fastest is list slicing. But be aware that  copy.copy() ,  list[:]  and  list(list) , unlike  copy.deepcopy()  and the python version don't copy any lists, dictionaries and class instances in the list, so if the originals change, they will change in the copied list too and vice versa.   (Here's the script if anyone's interested or wants to raise any issues:)   from copy import deepcopy  class old_class:     def __init__(self):         self.blah = 'blah'  class new_class(object):     def __init__(self):         self.blah = 'blah'  dignore = {str: None, unicode: None, int: None, type(None): None}  def Copy(obj, use_deepcopy=True):     t = type(obj)      if t in (list, tuple):         if t == tuple:             # Convert to a list if a tuple to             # allow assigning to when copying             is_tuple = True             obj = list(obj)         else:             # Otherwise just do a quick slice copy             obj = obj[:]             is_tuple = False          # Copy each item recursively         for x in xrange(len(obj)):             if type(obj[x]) in dignore:                 continue             obj[x] = Copy(obj[x], use_deepcopy)          if is_tuple:             # Convert back into a tuple again             obj = tuple(obj)      elif t == dict:         # Use the fast shallow dict copy() method and copy any         # values which aren't immutable (like lists, dicts etc)         obj = obj.copy()         for k in obj:             if type(obj[k]) in dignore:                 continue             obj[k] = Copy(obj[k], use_deepcopy)      elif t in dignore:         # Numeric or string/unicode?         # It's immutable, so ignore it!         pass      elif use_deepcopy:         obj = deepcopy(obj)     return obj  if __name__ == '__main__':     import copy     from time import time      num_times = 100000     L = [None, 'blah', 1, 543.4532,          ['foo'], ('bar',), {'blah': 'blah'},          old_class(), new_class()]      t = time()     for i in xrange(num_times):         Copy(L)     print 'Custom Copy:', time()-t      t = time()     for i in xrange(num_times):         Copy(L, use_deepcopy=False)     print 'Custom Copy Only Copying Lists/Tuples/Dicts (no classes):', time()-t      t = time()     for i in xrange(num_times):         copy.copy(L)     print 'copy.copy:', time()-t      t = time()     for i in xrange(num_times):         copy.deepcopy(L)     print 'copy.deepcopy:', time()-t      t = time()     for i in xrange(num_times):         L[:]     print 'list slicing [:]:', time()-t      t = time()     for i in xrange(num_times):         list(L)     print 'list(L):', time()-t      t = time()     for i in xrange(num_times):         [i for i in L]     print 'list expression(L):', time()-t      t = time()     for i in xrange(num_times):         a = []         a.extend(L)     print 'list extend:', time()-t      t = time()     for i in xrange(num_times):         a = []         for y in L:             a.append(y)     print 'list append:', time()-t      t = time()     for i in xrange(num_times):         a = []         a.extend(i for i in L)     print 'generator expression extend:', time()-t", "score": 767}}
{"question": "How do I clone a list so that it doesn&#39;t change unexpectedly after assignment?", "tags": ["python", "list", "clone", "mutable"], "link": "https://stackoverflow.com/questions/2612802/how-do-i-clone-a-list-so-that-it-doesnt-change-unexpectedly-after-assignment", "answer_count": 25, "answers": {"id": 2612815, "body": "new_list = my_list  doesn't actually create a second list. The assignment just copies the reference to the list, not the actual list, so both  new_list  and  my_list  refer to the same list after the assignment.   To actually copy the list, you have several options:     You can use the built-in  list.copy()  method (available since Python 3.3):   new_list = old_list.copy()      You can slice it:   new_list = old_list[:]    Alex Martelli 's opinion (at least  back in 2007 ) about this is, that  it is a weird syntax and it does not make sense to use it ever . ;) (In his opinion, the next one is more readable).     You can use the built-in  list()  constructor:   new_list = list(old_list)      You can use generic  copy.copy() :   import copy new_list = copy.copy(old_list)    This is a little slower than  list()  because it has to find out the datatype of  old_list  first.     If you need to copy the elements of the list as well, use generic  copy.deepcopy() :   import copy new_list = copy.deepcopy(old_list)    Obviously the slowest and most memory-needing method, but sometimes unavoidable. This operates recursively; it will handle any number of levels of nested lists (or other containers).       Example:   import copy  class Foo(object):     def __init__(self, val):          self.val = val      def __repr__(self):         return f'Foo({self.val!r})'  foo = Foo(1)  a = ['foo', foo] b = a.copy() c = a[:] d = list(a) e = copy.copy(a) f = copy.deepcopy(a)  # edit orignal list and instance  a.append('baz') foo.val = 5  print(f'original: {a}\\nlist.copy(): {b}\\nslice: {c}\\nlist(): {d}\\ncopy: {e}\\ndeepcopy: {f}')    Result:   original: ['foo', Foo(5), 'baz'] list.copy(): ['foo', Foo(5)] slice: ['foo', Foo(5)] list(): ['foo', Foo(5)] copy: ['foo', Foo(5)] deepcopy: ['foo', Foo(1)]", "score": 4141}}
{"question": "How do I sort a dictionary by value?", "tags": ["python", "sorting", "dictionary"], "link": "https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value", "answer_count": 34, "answers": {"id": 2258273, "body": "You could use:   sorted(d.items(), key=lambda x: x[1])    This will sort the dictionary by the values of each entry within the dictionary from smallest to largest.   To sort it in descending order just add  reverse=True :   sorted(d.items(), key=lambda x: x[1], reverse=True)    Input:   d = {'one':1,'three':3,'five':5,'two':2,'four':4} a = sorted(d.items(), key=lambda x: x[1])     print(a)    Output:   [('one', 1), ('two', 2), ('three', 3), ('four', 4), ('five', 5)]", "score": 1164}}
{"question": "How do I sort a dictionary by value?", "tags": ["python", "sorting", "dictionary"], "link": "https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value", "answer_count": 34, "answers": {"id": 3177911, "body": "As simple as:  sorted(dict1, key=dict1.get)   Well, it is actually possible to do a \"sort by dictionary values\". Recently I had to do that in a Code Golf (Stack Overflow question  Code golf: Word frequency chart ). Abridged, the problem was of the kind: given a text, count how often each word is encountered and display a list of the top words, sorted by decreasing frequency.    If you construct a dictionary with the words as keys and the number of occurrences of each word as value, simplified here as:   from collections import defaultdict d = defaultdict(int) for w in text.split():     d[w] += 1    then you can get a list of the words, ordered by frequency of use with  sorted(d, key=d.get)  - the sort iterates over the dictionary keys, using the number of word occurrences as a sort key .    for w in sorted(d, key=d.get, reverse=True):     print(w, d[w])    I am writing this detailed explanation to illustrate what people often mean by \"I can easily sort a dictionary by key, but how do I sort by value\" - and I think the original post was trying to address such an issue. And the solution is to do sort of list of the keys, based on the values, as shown above.", "score": 1639}}
{"question": "How do I sort a dictionary by value?", "tags": ["python", "sorting", "dictionary"], "link": "https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value", "answer_count": 34, "answers": {"id": 613218, "body": "Python 3.7+ or CPython 3.6   Dicts preserve insertion order in Python 3.7+. Same in CPython 3.6, but  it's an implementation detail .   >>> x = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0} >>> {k: v for k, v in sorted(x.items(), key=lambda item: item[1])} {0: 0, 2: 1, 1: 2, 4: 3, 3: 4}    or   >>> dict(sorted(x.items(), key=lambda item: item[1])) {0: 0, 2: 1, 1: 2, 4: 3, 3: 4}    Older Python   It is not possible to sort a dictionary, only to get a representation of a dictionary that is sorted. Dictionaries are inherently orderless, but other types, such as lists and tuples, are not. So you need an ordered data type to represent sorted values, which will be a list\u2014probably a list of tuples.   For instance,   import operator x = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0} sorted_x = sorted(x.items(), key=operator.itemgetter(1))    sorted_x  will be a list of tuples sorted by the second element in each tuple.  dict(sorted_x) == x .   And for those wishing to sort on keys instead of values:   import operator x = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0} sorted_x = sorted(x.items(), key=operator.itemgetter(0))    In Python3 since  unpacking is not allowed  we can use   x = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0} sorted_x = sorted(x.items(), key=lambda kv: kv[1])    If you want the output as a dict, you can use  collections.OrderedDict :   import collections  sorted_dict = collections.OrderedDict(sorted_x)", "score": 7056}}
{"question": "How can I access environment variables in Python?", "tags": ["python", "environment-variables"], "link": "https://stackoverflow.com/questions/4906977/how-can-i-access-environment-variables-in-python", "answer_count": 17, "answers": {"id": 49211023, "body": "Actually it can be done this way:   import os  for key, value in os.environ.items():     print(f'{key}: {value}')    Or simply:   for key, value in os.environ.items():     print('{}: {}'.format(key, value))    or:   for i, j in os.environ.items():     print(i, j)    For viewing the value in the parameter:   print(os.environ['HOME'])    Or:   print(os.environ.get('HOME'))    To set the value:   os.environ['HOME'] = '/new/value'", "score": 110}}
{"question": "How can I access environment variables in Python?", "tags": ["python", "environment-variables"], "link": "https://stackoverflow.com/questions/4906977/how-can-i-access-environment-variables-in-python", "answer_count": 17, "answers": {"id": 11447648, "body": "To check if the key exists (returns  True  or  False )   'HOME' in os.environ    You can also use  get()  when printing the key; useful if you want to use a default.   print(os.environ.get('HOME', '/home/username/'))    where  /home/username/  is the default", "score": 380}}
{"question": "How can I access environment variables in Python?", "tags": ["python", "environment-variables"], "link": "https://stackoverflow.com/questions/4906977/how-can-i-access-environment-variables-in-python", "answer_count": 17, "answers": {"id": 4907053, "body": "Environment variables are accessed through  os.environ :   import os print(os.environ['HOME'])    To see a list of all environment variables:   print(os.environ)      If a key is not present, attempting to access it will raise a  KeyError . To avoid this:   # Returns `None` if the key doesn't exist print(os.environ.get('KEY_THAT_MIGHT_EXIST'))  # Returns `default_value` if the key doesn't exist print(os.environ.get('KEY_THAT_MIGHT_EXIST', default_value))  # Returns `default_value` if the key doesn't exist print(os.getenv('KEY_THAT_MIGHT_EXIST', default_value))", "score": 4798}}
{"question": "How do I list all files of a directory?", "tags": ["python", "directory"], "link": "https://stackoverflow.com/questions/3207219/how-do-i-list-all-files-of-a-directory", "answer_count": 21, "answers": {"id": 41447012, "body": "list in the current directory   With  listdir  in  os  module you get the files and the folders in the current dir   import os  arr = os.listdir()    Looking in a directory   arr = os.listdir('c:\\\\files')    with  glob  you can specify a type of file to list like this   import glob  txtfiles = [] for file in glob.glob(\"*.txt\"):     txtfiles.append(file)    or   mylist = [f for f in glob.glob(\"*.txt\")]    get the full path of only files in the current directory   import os from os import listdir from os.path import isfile, join  cwd = os.getcwd() onlyfiles = [os.path.join(cwd, f) for f in os.listdir(cwd) if  os.path.isfile(os.path.join(cwd, f))] print(onlyfiles)   ['G:\\\\getfilesname\\\\getfilesname.py', 'G:\\\\getfilesname\\\\example.txt']    Getting the full path name with  os.path.abspath   You get the full path in return    import os  files_path = [os.path.abspath(x) for x in os.listdir()]  print(files_path)    ['F:\\\\documenti\\applications.txt', 'F:\\\\documenti\\collections.txt']    Walk: going through sub directories   os.walk returns the root, the directories list and the files list, that is why I unpacked them in r, d, f in the for loop; it, then, looks for other files and directories in the subfolders of the root and so on until there are no subfolders.   import os  # Getting the current work directory (cwd) thisdir = os.getcwd()  # r=root, d=directories, f = files for r, d, f in os.walk(thisdir):     for file in f:         if file.endswith(\".docx\"):             print(os.path.join(r, file))    To go up in the directory tree   # Method 1 x = os.listdir('..')  # Method 2 x= os.listdir('/')    Get files of a particular subdirectory with  os.listdir()   import os  x = os.listdir(\"./content\")    os.walk('.') - current directory    import os  arr = next(os.walk('.'))[2]  print(arr)    >>> ['5bs_Turismo1.pdf', '5bs_Turismo1.pptx', 'esperienza.txt']    next(os.walk('.')) and os.path.join('dir', 'file')    import os  arr = []  for d,r,f in next(os.walk(\"F:\\\\_python\")):      for file in f:          arr.append(os.path.join(r,file))   for f in arr:      print(files)  >>> F:\\\\_python\\\\dict_class.py >>> F:\\\\_python\\\\programmi.txt    next... walk    [os.path.join(r,file) for r,d,f in next(os.walk(\"F:\\\\_python\")) for file in f]    >>> ['F:\\\\_python\\\\dict_class.py', 'F:\\\\_python\\\\programmi.txt']    os.walk   x = [os.path.join(r,file) for r,d,f in os.walk(\"F:\\\\_python\") for file in f] print(x)  >>> ['F:\\\\_python\\\\dict.py', 'F:\\\\_python\\\\progr.txt', 'F:\\\\_python\\\\readl.py']    os.listdir() - get only txt files    arr_txt = [x for x in os.listdir() if x.endswith(\".txt\")]      Using  glob  to get the full path of the files   from path import path from glob import glob  x = [path(f).abspath() for f in glob(\"F:\\\\*.txt\")]    Using  os.path.isfile  to avoid directories in the list   import os.path listOfFiles = [f for f in os.listdir() if os.path.isfile(f)]    Using  pathlib  from Python 3.4   import pathlib  flist = [] for p in pathlib.Path('.').iterdir():     if p.is_file():         print(p)         flist.append(p)    With  list comprehension :   flist = [p for p in pathlib.Path('.').iterdir() if p.is_file()]    Use glob method in pathlib.Path()   import pathlib  py = pathlib.Path().glob(\"*.py\")    Get all and only files with os.walk: checks only in the third element returned, i.e. the list of the files   import os x = [i[2] for i in os.walk('.')] y=[] for t in x:     for f in t:         y.append(f)    Get only files with next in a directory: returns only the file in the root folder    import os  x = next(os.walk('F://python'))[2]    Get only directories with next and walk in a directory, because in the [1] element there are the folders only    import os  next(os.walk('F://python'))[1] # for the current dir use ('.')    >>> ['python3','others']    Get all the  subdir  names with  walk   for r,d,f in os.walk(\"F:\\\\_python\"):     for dirs in d:         print(dirs)    os.scandir()  from Python 3.5 and greater   import os x = [f.name for f in os.scandir() if f.is_file()]  # Another example with `scandir` (a little variation from docs.python.org) # This one is more efficient than `os.listdir`. # In this case, it shows the files only in the current directory # where the script is executed.  import os with os.scandir() as i:     for entry in i:         if entry.is_file():             print(entry.name)", "score": 1593}}
{"question": "How do I list all files of a directory?", "tags": ["python", "directory"], "link": "https://stackoverflow.com/questions/3207219/how-do-i-list-all-files-of-a-directory", "answer_count": 21, "answers": {"id": 3215392, "body": "I prefer using the  glob  module, as it does pattern matching and expansion.   import glob print(glob.glob(\"/home/adam/*\"))    It does pattern matching intuitively   import glob # All files and directories ending with .txt and that don't begin with a dot: print(glob.glob(\"/home/adam/*.txt\"))  # All files and directories ending with .txt with depth of 2 folders, ignoring names beginning with a dot: print(glob.glob(\"/home/adam/*/*.txt\"))     It will return a list with the queried files and directories:   ['/home/adam/file1.txt', '/home/adam/file2.txt', .... ]    Note that  glob  ignores files and directories that begin with a dot  . , as those are considered hidden files and directories, unless the pattern is something like  .* .   Use  glob.escape  to escape strings that are not meant to be patterns:   print(glob.glob(glob.escape(directory_name) + \"/*.txt\"))", "score": 2634}}
{"question": "How do I list all files of a directory?", "tags": ["python", "directory"], "link": "https://stackoverflow.com/questions/3207219/how-do-i-list-all-files-of-a-directory", "answer_count": 21, "answers": {"id": 3207973, "body": "os.listdir()  returns everything inside a directory -- including both  files  and  directories .   os.path 's  isfile()  can be used to only list files:   from os import listdir from os.path import isfile, join onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]    Alternatively,  os.walk()   yields two lists  for each directory it visits -- one for  files  and one for  dirs . If you only want the top directory you can break the first time it yields:   from os import walk  f = [] for (dirpath, dirnames, filenames) in walk(mypath):     f.extend(filenames)     break    or, shorter:   from os import walk  filenames = next(walk(mypath), (None, None, []))[2]  # [] if no file", "score": 6552}}
{"question": "What does ** (double star/asterisk) and * (star/asterisk) do for parameters?", "tags": ["python", "syntax", "parameter-passing", "variadic-functions", "argument-unpacking"], "link": "https://stackoverflow.com/questions/36901/what-does-double-star-asterisk-and-star-asterisk-do-for-parameters", "answer_count": 28, "answers": {"id": 36911, "body": "The single * means that there can be any number of extra positional arguments.  foo()  can be invoked like  foo(1,2,3,4,5) . In the body of foo() param2 is a sequence containing 2-5.   The double ** means there can be any number of extra named parameters.  bar()  can be invoked like  bar(1, a=2, b=3) . In the body of bar() param2 is a dictionary containing {'a':2, 'b':3 }   With the following code:   def foo(param1, *param2):     print(param1)     print(param2)  def bar(param1, **param2):     print(param1)     print(param2)  foo(1,2,3,4,5) bar(1,a=2,b=3)    the output is   1 (2, 3, 4, 5) 1 {'a': 2, 'b': 3}", "score": 225}}
{"question": "What does ** (double star/asterisk) and * (star/asterisk) do for parameters?", "tags": ["python", "syntax", "parameter-passing", "variadic-functions", "argument-unpacking"], "link": "https://stackoverflow.com/questions/36901/what-does-double-star-asterisk-and-star-asterisk-do-for-parameters", "answer_count": 28, "answers": {"id": 36926, "body": "It's also worth noting that you can use  *  and  **  when calling functions as well. This is a shortcut that allows you to pass multiple arguments to a function directly using either a list/tuple or a dictionary. For example, if you have the following function:   def foo(x,y,z):     print(\"x=\" + str(x))     print(\"y=\" + str(y))     print(\"z=\" + str(z))    You can do things like:   >>> mylist = [1,2,3] >>> foo(*mylist) x=1 y=2 z=3  >>> mydict = {'x':1,'y':2,'z':3} >>> foo(**mydict) x=1 y=2 z=3  >>> mytuple = (1, 2, 3) >>> foo(*mytuple) x=1 y=2 z=3    Note: The keys in  mydict  have to be named exactly like the parameters of function  foo . Otherwise it will throw a  TypeError :   >>> mydict = {'x':1,'y':2,'z':3,'badnews':9} >>> foo(**mydict) Traceback (most recent call last):   File \" \", line 1, in   TypeError: foo() got an unexpected keyword argument 'badnews'", "score": 846}}
{"question": "What does ** (double star/asterisk) and * (star/asterisk) do for parameters?", "tags": ["python", "syntax", "parameter-passing", "variadic-functions", "argument-unpacking"], "link": "https://stackoverflow.com/questions/36901/what-does-double-star-asterisk-and-star-asterisk-do-for-parameters", "answer_count": 28, "answers": {"id": 36908, "body": "The  *args  and  **kwargs  are common idioms to allow an arbitrary number of arguments to functions, as described in the section  more on defining functions  in the Python tutorial.   The  *args  will give you all positional arguments  as a tuple :   def foo(*args):     for a in args:         print(a)          foo(1) # 1  foo(1, 2, 3) # 1 # 2 # 3    The  **kwargs  will give you all keyword arguments as a dictionary:   def bar(**kwargs):     for a in kwargs:         print(a, kwargs[a])    bar(name='one', age=27) # name one # age 27    Both idioms can be mixed with normal arguments to allow a set of fixed and some variable arguments:   def foo(kind, *args, bar=None, **kwargs):     print(kind, args, bar, kwargs)  foo(123, 'a', 'b', apple='red') # 123 ('a', 'b') None {'apple': 'red'}    It is also possible to use this the other way around:   def foo(a, b, c):     print(a, b, c)  obj = {'b':10, 'c':'lee'}  foo(100, **obj) # 100 10 lee    Another usage of the  *l  idiom is to  unpack argument lists  when calling a function.   def foo(bar, lee):     print(bar, lee)  baz = [1, 2]  foo(*baz) # 1 2    In Python 3 it is possible to use  *l  on the left side of an assignment ( Extended Iterable Unpacking ), though it gives a list instead of a tuple in this context:   first, *rest = [1, 2, 3, 4] # first = 1 # rest = [2, 3, 4]    Also Python 3 adds a new semantic (refer  PEP 3102 ):   def func(arg1, arg2, arg3, *, kwarg1, kwarg2):     pass    Such function accepts only 3 positional arguments, and everything after  *  can only be passed as keyword arguments.   Note:   A Python  dict , semantically used for keyword argument passing, is arbitrarily ordered. However, in Python 3.6+, keyword arguments are guaranteed to remember insertion order. \"The order of elements in  **kwargs  now corresponds to the order in which keyword arguments were passed to the function.\" -  What\u2019s New In Python 3.6 . In fact, all dicts in CPython 3.6 will remember insertion order as an implementation detail, and this becomes standard in Python 3.7.", "score": 3319}}
{"question": "&quot;Least Astonishment&quot; and the Mutable Default Argument", "tags": ["python", "language-design", "default-parameters", "least-astonishment"], "link": "https://stackoverflow.com/questions/1132941/least-astonishment-and-the-mutable-default-argument", "answer_count": 34, "answers": {"id": 11416002, "body": "The relevant part of the  documentation :     Default parameter values are evaluated from left to right when the function definition is executed.  This means that the expression is evaluated once, when the function is defined, and that the same \u201cpre-computed\u201d value is used for each call. This is especially important to understand when a default parameter is a mutable object, such as a list or a dictionary: if the function modifies the object (e.g. by appending an item to a list), the default value is in effect modified. This is generally not what was intended. A way around this is to use  None  as the default, and explicitly test for it in the body of the function, e.g.:   def whats_on_the_telly(penguin=None):     if penguin is None:         penguin = []     penguin.append(\"property of the zoo\")     return penguin", "score": 325}}
{"question": "&quot;Least Astonishment&quot; and the Mutable Default Argument", "tags": ["python", "language-design", "default-parameters", "least-astonishment"], "link": "https://stackoverflow.com/questions/1132941/least-astonishment-and-the-mutable-default-argument", "answer_count": 34, "answers": {"id": 1133013, "body": "Suppose you have the following code   fruits = (\"apples\", \"bananas\", \"loganberries\")  def eat(food=fruits):     ...    When I see the declaration of eat, the least astonishing thing is to think that if the first parameter is not given, that it will be equal to the tuple  (\"apples\", \"bananas\", \"loganberries\")   However, suppose later on in the code, I do something like   def some_random_function():     global fruits     fruits = (\"blueberries\", \"mangos\")    then if default parameters were bound at function execution rather than function declaration, I would be astonished (in a very bad way) to discover that fruits had been changed. This would be more astonishing IMO than discovering that your  foo  function above was mutating the list.   The real problem lies with mutable variables, and all languages have this problem to some extent. Here's a question: suppose in Java I have the following code:   StringBuffer s = new StringBuffer(\"Hello World!\"); Map  counts = new HashMap (); counts.put(s, 5); s.append(\"!!!!\"); System.out.println( counts.get(s) );  // does this work?    Now, does my map use the value of the  StringBuffer  key when it was placed into the map, or does it store the key by reference? Either way, someone is astonished; either the person who tried to get the object out of the  Map  using a value identical to the one they put it in with, or the person who can't seem to retrieve their object even though the key they're using is literally the same object that was used to put it into the map (this is actually why Python doesn't allow its mutable built-in data types to be used as dictionary keys).   Your example is a good one of a case where Python newcomers will be surprised and bitten. But I'd argue that if we \"fixed\" this, then that would only create a different situation where they'd be bitten instead, and that one would be even less intuitive. Moreover, this is always the case when dealing with mutable variables; you always run into cases where someone could intuitively expect one or the opposite behavior depending on what code they're writing.   I personally like Python's current approach: default function arguments are evaluated when the function is defined and that object is always the default. I suppose they could special-case using an empty list, but that kind of special casing would cause even more astonishment, not to mention be backwards incompatible.", "score": 338}}
{"question": "&quot;Least Astonishment&quot; and the Mutable Default Argument", "tags": ["python", "language-design", "default-parameters", "least-astonishment"], "link": "https://stackoverflow.com/questions/1132941/least-astonishment-and-the-mutable-default-argument", "answer_count": 34, "answers": {"id": 1145781, "body": "Actually, this is not a design flaw, and it is not because of internals or performance. It comes simply from the fact that functions in Python are  first-class objects , and not only a piece of code.   As soon as you think of it this way, then it completely makes sense: a function is an  object  being evaluated on its definition; default parameters are kind of  \"member data\"  and therefore their state may change from one call to the other - exactly as in any other object.   In any case, the Effbot ( Fredrik Lundh ) has a very nice explanation of the reasons for this behavior in  Default Parameter Values in Python . I found it very clear, and I really suggest reading it for a better knowledge of how function objects work.", "score": 1953}}
{"question": "How can I delete a file or folder in Python?", "tags": ["python", "file-io", "directory", "delete-file"], "link": "https://stackoverflow.com/questions/6996603/how-can-i-delete-a-file-or-folder-in-python", "answer_count": 18, "answers": {"id": 6996662, "body": "Use    shutil.rmtree(path[, ignore_errors[, onerror]])    (See complete documentation on  shutil ) and/or   os.remove    and   os.rmdir    (Complete documentation on  os .)", "score": 129}}
{"question": "How can I delete a file or folder in Python?", "tags": ["python", "file-io", "directory", "delete-file"], "link": "https://stackoverflow.com/questions/6996603/how-can-i-delete-a-file-or-folder-in-python", "answer_count": 18, "answers": {"id": 42641792, "body": "Python syntax to delete a file   import os os.remove(\"/tmp/ .txt\")    or   import os os.unlink(\"/tmp/ .txt\")    or   pathlib  Library for Python version >= 3.4   file_to_rem = pathlib.Path(\"/tmp/ .txt\") file_to_rem.unlink()    Path.unlink(missing_ok=False)   Unlink method used to remove the file or the symbolik link.       If missing_ok is false (the default), FileNotFoundError is raised if    the path does not exist.   If missing_ok is true, FileNotFoundError exceptions will be ignored    (same behavior as the POSIX rm -f command).   Changed in version 3.8: The missing_ok parameter was added.       Best practice   First, check if the file or folder exists and then delete it. You can achieve this in two ways:     os.path.isfile(\"/path/to/file\")   Use  exception handling.     EXAMPLE  for  os.path.isfile   #!/usr/bin/python import os  myfile = \"/tmp/foo.txt\" # If file exists, delete it. if os.path.isfile(myfile):     os.remove(myfile) else:     # If it fails, inform the user.     print(\"Error: %s file not found\" % myfile)    Exception Handling   #!/usr/bin/python import os  # Get input. myfile = raw_input(\"Enter file name to delete: \")  # Try to delete the file. try:     os.remove(myfile) except OSError as e:     # If it fails, inform the user.     print(\"Error: %s - %s.\" % (e.filename, e.strerror))    Respective output    Enter file name to delete : demo.txt Error: demo.txt - No such file or directory.  Enter file name to delete : rrr.txt Error: rrr.txt - Operation not permitted.  Enter file name to delete : foo.txt    Python syntax to delete a folder   shutil.rmtree()    Example for  shutil.rmtree()   #!/usr/bin/python import os import sys import shutil  # Get directory name mydir = raw_input(\"Enter directory name: \")  # Try to remove the tree; if it fails, throw an error using try...except. try:     shutil.rmtree(mydir) except OSError as e:     print(\"Error: %s - %s.\" % (e.filename, e.strerror))", "score": 780}}
{"question": "How can I delete a file or folder in Python?", "tags": ["python", "file-io", "directory", "delete-file"], "link": "https://stackoverflow.com/questions/6996603/how-can-i-delete-a-file-or-folder-in-python", "answer_count": 18, "answers": {"id": 6996628, "body": "Use one of these methods:     pathlib.Path.unlink()  removes a file or symbolic link.     pathlib.Path.rmdir()  removes an empty directory.     shutil.rmtree()  deletes a directory and all its contents.         On Python 3.3 and below, you can use these methods instead of the  pathlib  ones:     os.remove()  removes a file.     os.unlink()  removes a symbolic link.     os.rmdir()  removes an empty directory.", "score": 5008}}
{"question": "Does Python have a string &#39;contains&#39; substring method?", "tags": ["python", "string", "substring", "contains"], "link": "https://stackoverflow.com/questions/3437059/does-python-have-a-string-contains-substring-method", "answer_count": 10, "answers": {"id": 27138045, "body": "Does Python have a string contains substring method?     99%  of use cases will be covered using the keyword,  in , which returns  True  or  False :   'substring' in any_string    For the use case of getting the index, use  str.find  (which returns -1 on failure, and has optional positional arguments):   start = 0 stop = len(any_string) any_string.find('substring', start, stop)    or  str.index  (like  find  but raises ValueError on failure):   start = 100  end = 1000 any_string.index('substring', start, end)    Explanation   Use the  in  comparison operator because     the language intends its usage, and   other Python programmers will expect you to use it.     >>> 'foo' in '**foo**' True    The opposite (complement), which the original question asked for, is  not in :   >>> 'foo' not in '**foo**' # returns False False    This is semantically the same as  not 'foo' in '**foo**'  but it's much more readable and explicitly provided for in the language as a readability improvement.   Avoid using  __contains__   The \"contains\" method implements the behavior for  in . This example,   str.__contains__('**foo**', 'foo')    returns  True . You could also call this function from the instance of the superstring:   '**foo**'.__contains__('foo')    But don't. Methods that start with underscores are considered semantically non-public. The only reason to use this is when implementing or extending the  in  and  not in  functionality (e.g. if subclassing  str ):   class NoisyString(str):     def __contains__(self, other):         print(f'testing if \"{other}\" in \"{self}\"')         return super(NoisyString, self).__contains__(other)  ns = NoisyString('a string with a substring inside')    and now:   >>> 'substring' in ns testing if \"substring\" in \"a string with a substring inside\" True    Don't use  find  and  index  to test for \"contains\"   Don't use the following string methods to test for \"contains\":   >>> '**foo**'.index('foo') 2 >>> '**foo**'.find('foo') 2  >>> '**oo**'.find('foo') -1 >>> '**oo**'.index('foo')  Traceback (most recent call last):   File \" \", line 1, in       '**oo**'.index('foo') ValueError: substring not found    Other languages may have no methods to directly test for substrings, and so you would have to use these types of methods, but with Python, it is much more efficient to use the  in  comparison operator.   Also, these are not drop-in replacements for  in . You may have to handle the exception or  -1  cases, and if they return  0  (because they found the substring at the beginning) the boolean interpretation is  False  instead of  True .   If you really mean  not any_string.startswith(substring)  then say it.   Performance comparisons   We can compare various ways of accomplishing the same goal.   import timeit  def in_(s, other):     return other in s  def contains(s, other):     return s.__contains__(other)  def find(s, other):     return s.find(other) != -1  def index(s, other):     try:         s.index(other)     except ValueError:         return False     else:         return True    perf_dict = { 'in:True': min(timeit.repeat(lambda: in_('superstring', 'str'))), 'in:False': min(timeit.repeat(lambda: in_('superstring', 'not'))), '__contains__:True': min(timeit.repeat(lambda: contains('superstring', 'str'))), '__contains__:False': min(timeit.repeat(lambda: contains('superstring', 'not'))), 'find:True': min(timeit.repeat(lambda: find('superstring', 'str'))), 'find:False': min(timeit.repeat(lambda: find('superstring', 'not'))), 'index:True': min(timeit.repeat(lambda: index('superstring', 'str'))), 'index:False': min(timeit.repeat(lambda: index('superstring', 'not'))), }    And now we see that using  in  is much faster than the others. Less time to do an equivalent operation is better:   >>> perf_dict {'in:True': 0.16450627865128808,  'in:False': 0.1609668098178645,  '__contains__:True': 0.24355481654697542,  '__contains__:False': 0.24382793854783813,  'find:True': 0.3067379407923454,  'find:False': 0.29860888058124146,  'index:True': 0.29647137792585454,  'index:False': 0.5502287584545229}    How can  in  be faster than  __contains__  if  in  uses  __contains__ ?   This is a fine follow-on question.   Let's disassemble functions with the methods of interest:   >>> from dis import dis >>> dis(lambda: 'a' in 'b')   1           0 LOAD_CONST               1 ('a')               2 LOAD_CONST               2 ('b')               4 COMPARE_OP               6 (in)               6 RETURN_VALUE >>> dis(lambda: 'b'.__contains__('a'))   1           0 LOAD_CONST               1 ('b')               2 LOAD_METHOD              0 (__contains__)               4 LOAD_CONST               2 ('a')               6 CALL_METHOD              1               8 RETURN_VALUE    so we see that the  .__contains__  method has to be separately looked up and then called from the Python virtual machine - this should adequately explain the difference.", "score": 560}}
{"question": "Does Python have a string &#39;contains&#39; substring method?", "tags": ["python", "string", "substring", "contains"], "link": "https://stackoverflow.com/questions/3437059/does-python-have-a-string-contains-substring-method", "answer_count": 10, "answers": {"id": 3437068, "body": "You can use  str.find :   s = \"This be a string\" if s.find(\"is\") == -1:     print(\"Not found\") else:     print(\"Found\")      The  find()  method should be used only if you need to know the position of sub. To check if sub is a substring or not, use the  in  operator. (c) Python reference", "score": 940}}
{"question": "Does Python have a string &#39;contains&#39; substring method?", "tags": ["python", "string", "substring", "contains"], "link": "https://stackoverflow.com/questions/3437059/does-python-have-a-string-contains-substring-method", "answer_count": 10, "answers": {"id": 3437070, "body": "Use the  in  operator :   if \"blah\" not in somestring:      continue    Note: This is case-sensitive.", "score": 8606}}
{"question": "How can I add new keys to a dictionary?", "tags": ["python", "dictionary", "key", "lookup"], "link": "https://stackoverflow.com/questions/1024847/how-can-i-add-new-keys-to-a-dictionary", "answer_count": 20, "answers": {"id": 1165836, "body": "To add multiple keys simultaneously, use  dict.update() :   >>> x = {1:2} >>> print(x) {1: 2}  >>> d = {3:4, 5:6, 7:8} >>> x.update(d) >>> print(x) {1: 2, 3: 4, 5: 6, 7: 8}    For adding a single key, the accepted answer has less computational overhead.", "score": 1237}}
{"question": "How can I add new keys to a dictionary?", "tags": ["python", "dictionary", "key", "lookup"], "link": "https://stackoverflow.com/questions/1024847/how-can-i-add-new-keys-to-a-dictionary", "answer_count": 20, "answers": {"id": 8381589, "body": "I feel like consolidating info about Python dictionaries:   Creating an empty dictionary   data = {} # OR data = dict()    Creating a dictionary with initial values   data = {'a': 1, 'b': 2, 'c': 3} # OR data = dict(a=1, b=2, c=3) # OR data = {k: v for k, v in (('a', 1), ('b',2), ('c',3))}    Inserting/Updating a single value   data['a'] = 1  # Updates if 'a' exists, else adds 'a' # OR data.update({'a': 1}) # OR data.update(dict(a=1)) # OR data.update(a=1)    Inserting/Updating multiple values   data.update({'c':3,'d':4})  # Updates 'c' and adds 'd'    Python\u00a03.9+:   The  update operator   |=  now works for dictionaries:   data |= {'c':3,'d':4}    Creating a merged dictionary without modifying originals   data3 = {} data3.update(data)  # Modifies data3, not data data3.update(data2)  # Modifies data3, not data2    Python\u00a03.5+:   This uses a new feature called  dictionary unpacking .   data = {**data1, **data2, **data3}    Python\u00a03.9+:   The  merge operator   |  now works for dictionaries:   data = data1 | {'c':3,'d':4}    Deleting items in dictionary   del data[key]  # Removes specific element in a dictionary data.pop(key)  # Removes the key & returns the value data.clear()  # Clears entire dictionary    Check if a key is already in dictionary   key in data    Iterate through pairs in a dictionary   for key in data: # Iterates just through the keys, ignoring the values for key, value in d.items(): # Iterates through the pairs for key in d.keys(): # Iterates just through key, ignoring the values for value in d.values(): # Iterates just through value, ignoring the keys    Create a dictionary from two lists   data = dict(zip(list_with_keys, list_with_values))", "score": 1354}}
{"question": "How can I add new keys to a dictionary?", "tags": ["python", "dictionary", "key", "lookup"], "link": "https://stackoverflow.com/questions/1024847/how-can-i-add-new-keys-to-a-dictionary", "answer_count": 20, "answers": {"id": 1024851, "body": "You create a new key/value pair on a dictionary by assigning a value to that key   d = {'key': 'value'} print(d)  # {'key': 'value'}  d['mynewkey'] = 'mynewvalue'  print(d)  # {'key': 'value', 'mynewkey': 'mynewvalue'}    If the key doesn't exist, it's added and points to that value. If it exists, the current value it points to is overwritten.", "score": 4447}}
{"question": "How do I select rows from a DataFrame based on column values?", "tags": ["python", "pandas", "dataframe", "indexing", "filter"], "link": "https://stackoverflow.com/questions/17071871/how-do-i-select-rows-from-a-dataframe-based-on-column-values", "answer_count": 20, "answers": {"id": 31296878, "body": "tl;dr   The Pandas equivalent to   select * from table where column_name = some_value    is   table[table.column_name == some_value]    Multiple conditions:   table[(table.column_name == some_value) | (table.column_name2 == some_value2)]    or   table.query('column_name == some_value | column_name2 == some_value2')    Code example   import pandas as pd  # Create data set d = {'foo':[100, 111, 222],      'bar':[333, 444, 555]} df = pd.DataFrame(d)  # Full dataframe: df  # Shows: #    bar   foo # 0  333   100 # 1  444   111 # 2  555   222  # Output only the row(s) in df where foo is 222: df[df.foo == 222]  # Shows: #    bar  foo # 2  555  222    In the above code it is the line  df[df.foo == 222]  that gives the rows based on the column value,  222  in this case.   Multiple conditions are also possible:   df[(df.foo == 222) | (df.bar == 444)] #    bar  foo # 1  444  111 # 2  555  222    But at that point I would recommend using the  query  function, since it's less verbose and yields the same result:   df.query('foo == 222 | bar == 444')", "score": 354}}
{"question": "How do I select rows from a DataFrame based on column values?", "tags": ["python", "pandas", "dataframe", "indexing", "filter"], "link": "https://stackoverflow.com/questions/17071871/how-do-i-select-rows-from-a-dataframe-based-on-column-values", "answer_count": 20, "answers": {"id": 46165056, "body": "There are several ways to select rows from a Pandas dataframe:     Boolean indexing ( df[df['col'] == value ] )   Positional indexing ( df.iloc[...] )   Label indexing ( df.xs(...) )   df.query(...)  API     Below I show you examples of each, with advice when to use certain techniques. Assume our criterion is column  'A'  ==  'foo'   (Note on performance: For each base type, we can keep things simple by using the Pandas API or we can venture outside the API, usually into NumPy, and speed things up.)     Setup   The first thing we'll need is to identify a condition that will act as our criterion for selecting rows. We'll start with the OP's case  column_name == some_value , and include some other common use cases.   Borrowing from @unutbu:   import pandas as pd, numpy as np  df = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),                    'B': 'one one two three two two one three'.split(),                    'C': np.arange(8), 'D': np.arange(8) * 2})      1. Boolean indexing   ... Boolean indexing requires finding the true value of each row's  'A'  column being equal to  'foo' , then using those truth values to identify which rows to keep.  Typically, we'd name this series, an array of truth values,  mask .  We'll do so here as well.   mask = df['A'] == 'foo'    We can then use this mask to slice or index the data frame   df[mask]       A      B  C   D 0  foo    one  0   0 2  foo    two  2   4 4  foo    two  4   8 6  foo    one  6  12 7  foo  three  7  14    This is one of the simplest ways to accomplish this task and if performance or intuitiveness isn't an issue, this should be your chosen method.  However, if performance is a concern, then you might want to consider an alternative way of creating the  mask .     2. Positional indexing   Positional indexing ( df.iloc[...] ) has its use cases, but this isn't one of them.  In order to identify where to slice, we first need to perform the same boolean analysis we did above.  This leaves us performing one extra step to accomplish the same task.   mask = df['A'] == 'foo' pos = np.flatnonzero(mask) df.iloc[pos]       A      B  C   D 0  foo    one  0   0 2  foo    two  2   4 4  foo    two  4   8 6  foo    one  6  12 7  foo  three  7  14    3. Label indexing   Label  indexing can be very handy, but in this case, we are again doing more work for no benefit   df.set_index('A', append=True, drop=False).xs('foo', level=1)       A      B  C   D 0  foo    one  0   0 2  foo    two  2   4 4  foo    two  4   8 6  foo    one  6  12 7  foo  three  7  14    4.  df.query()  API   pd.DataFrame.query  is a very elegant/intuitive way to perform this task, but is often slower.  However , if you pay attention to the timings below, for large data, the query is very efficient. More so than the standard approach and of similar magnitude as my best suggestion.   df.query('A == \"foo\"')       A      B  C   D 0  foo    one  0   0 2  foo    two  2   4 4  foo    two  4   8 6  foo    one  6  12 7  foo  three  7  14      My preference is to use the  Boolean   mask   Actual improvements can be made by modifying how we create our  Boolean   mask .   mask  alternative 1   Use the underlying NumPy array and forgo the overhead of creating another  pd.Series   mask = df['A'].values == 'foo'    I'll show more complete time tests at the end, but just take a look at the performance gains we get using the sample data frame.  First, we look at the difference in creating the  mask   %timeit mask = df['A'].values == 'foo' %timeit mask = df['A'] == 'foo'  5.84 \u00b5s \u00b1 195 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each) 166 \u00b5s \u00b1 4.45 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)    Evaluating the  mask  with the NumPy array is ~ 30 times faster.  This is partly due to NumPy evaluation often being faster. It is also partly due to the lack of overhead necessary to build an index and a corresponding  pd.Series  object.   Next, we'll look at the timing for slicing with one  mask  versus the other.   mask = df['A'].values == 'foo' %timeit df[mask] mask = df['A'] == 'foo' %timeit df[mask]  219 \u00b5s \u00b1 12.3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) 239 \u00b5s \u00b1 7.03 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)    The performance gains aren't as pronounced.  We'll see if this holds up over more robust testing.     mask  alternative 2  We could have reconstructed the data frame as well.  There is a big caveat when reconstructing a dataframe\u2014you must take care of the  dtypes  when doing so!   Instead of  df[mask]  we will do this   pd.DataFrame(df.values[mask], df.index[mask], df.columns).astype(df.dtypes)    If the data frame is of mixed type, which our example is, then when we get  df.values  the resulting array is of  dtype   object  and consequently, all columns of the new data frame will be of  dtype   object .  Thus requiring the  astype(df.dtypes)  and killing any potential performance gains.   %timeit df[m] %timeit pd.DataFrame(df.values[mask], df.index[mask], df.columns).astype(df.dtypes)  216 \u00b5s \u00b1 10.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) 1.43 ms \u00b1 39.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)    However, if the data frame is not of mixed type, this is a very useful way to do it.   Given   np.random.seed([3,1415]) d1 = pd.DataFrame(np.random.randint(10, size=(10, 5)), columns=list('ABCDE'))  d1     A  B  C  D  E 0  0  2  7  3  8 1  7  0  6  8  6 2  0  2  0  4  9 3  7  3  2  4  3 4  3  6  7  7  4 5  5  3  7  5  9 6  8  7  6  4  7 7  6  2  6  6  5 8  2  8  7  5  8 9  4  7  6  1  5      %%timeit mask = d1['A'].values == 7 d1[mask]  179 \u00b5s \u00b1 8.73 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)    Versus   %%timeit mask = d1['A'].values == 7 pd.DataFrame(d1.values[mask], d1.index[mask], d1.columns)  87 \u00b5s \u00b1 5.12 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)    We cut the time in half.     mask  alternative 3   @unutbu also shows us how to use  pd.Series.isin  to account for each element of  df['A']  being in a set of values.  This evaluates to the same thing if our set of values is a set of one value, namely  'foo' .  But it also generalizes to include larger sets of values if needed.  Turns out, this is still pretty fast even though it is a more general solution.  The only real loss is in intuitiveness for those not familiar with the concept.   mask = df['A'].isin(['foo']) df[mask]       A      B  C   D 0  foo    one  0   0 2  foo    two  2   4 4  foo    two  4   8 6  foo    one  6  12 7  foo  three  7  14    However, as before, we can utilize NumPy to improve performance while sacrificing virtually nothing. We'll use  np.in1d   mask = np.in1d(df['A'].values, ['foo']) df[mask]       A      B  C   D 0  foo    one  0   0 2  foo    two  2   4 4  foo    two  4   8 6  foo    one  6  12 7  foo  three  7  14      Timing   I'll include other concepts mentioned in other posts as well for reference.   Code Below   Each  column  in this table represents a different length data frame over which we test each function. Each column shows relative time taken, with the fastest function given a base index of  1.0 .   res.div(res.min())                           10        30        100       300       1000      3000      10000     30000 mask_standard         2.156872  1.850663  2.034149  2.166312  2.164541  3.090372  2.981326  3.131151 mask_standard_loc     1.879035  1.782366  1.988823  2.338112  2.361391  3.036131  2.998112  2.990103 mask_with_values      1.010166  1.000000  1.005113  1.026363  1.028698  1.293741  1.007824  1.016919 mask_with_values_loc  1.196843  1.300228  1.000000  1.000000  1.038989  1.219233  1.037020  1.000000 query                 4.997304  4.765554  5.934096  4.500559  2.997924  2.397013  1.680447  1.398190 xs_label              4.124597  4.272363  5.596152  4.295331  4.676591  5.710680  6.032809  8.950255 mask_with_isin        1.674055  1.679935  1.847972  1.724183  1.345111  1.405231  1.253554  1.264760 mask_with_in1d        1.000000  1.083807  1.220493  1.101929  1.000000  1.000000  1.000000  1.144175    You'll notice that the fastest times seem to be shared between  mask_with_values  and  mask_with_in1d .   res.T.plot(loglog=True)      Functions   def mask_standard(df):     mask = df['A'] == 'foo'     return df[mask]  def mask_standard_loc(df):     mask = df['A'] == 'foo'     return df.loc[mask]  def mask_with_values(df):     mask = df['A'].values == 'foo'     return df[mask]  def mask_with_values_loc(df):     mask = df['A'].values == 'foo'     return df.loc[mask]  def query(df):     return df.query('A == \"foo\"')  def xs_label(df):     return df.set_index('A', append=True, drop=False).xs('foo', level=-1)  def mask_with_isin(df):     mask = df['A'].isin(['foo'])     return df[mask]  def mask_with_in1d(df):     mask = np.in1d(df['A'].values, ['foo'])     return df[mask]      Testing   res = pd.DataFrame(     index=[         'mask_standard', 'mask_standard_loc', 'mask_with_values', 'mask_with_values_loc',         'query', 'xs_label', 'mask_with_isin', 'mask_with_in1d'     ],     columns=[10, 30, 100, 300, 1000, 3000, 10000, 30000],     dtype=float )  for j in res.columns:     d = pd.concat([df] * j, ignore_index=True)     for i in res.index:a         stmt = '{}(d)'.format(i)         setp = 'from __main__ import d, {}'.format(i)         res.at[i, j] = timeit(stmt, setp, number=50)      Special Timing   Looking at the special case when we have a single non-object  dtype  for the entire data frame.   Code Below   spec.div(spec.min())                       10        30        100       300       1000      3000      10000     30000 mask_with_values  1.009030  1.000000  1.194276  1.000000  1.236892  1.095343  1.000000  1.000000 mask_with_in1d    1.104638  1.094524  1.156930  1.072094  1.000000  1.000000  1.040043  1.027100 reconstruct       1.000000  1.142838  1.000000  1.355440  1.650270  2.222181  2.294913  3.406735    Turns out, reconstruction isn't worth it past a few hundred rows.   spec.T.plot(loglog=True)      Functions   np.random.seed([3,1415]) d1 = pd.DataFrame(np.random.randint(10, size=(10, 5)), columns=list('ABCDE'))  def mask_with_values(df):     mask = df['A'].values == 'foo'     return df[mask]  def mask_with_in1d(df):     mask = np.in1d(df['A'].values, ['foo'])     return df[mask]  def reconstruct(df):     v = df.values     mask = np.in1d(df['A'].values, ['foo'])     return pd.DataFrame(v[mask], df.index[mask], df.columns)  spec = pd.DataFrame(     index=['mask_with_values', 'mask_with_in1d', 'reconstruct'],     columns=[10, 30, 100, 300, 1000, 3000, 10000, 30000],     dtype=float )    Testing   for j in spec.columns:     d = pd.concat([df] * j, ignore_index=True)     for i in spec.index:         stmt = '{}(d)'.format(i)         setp = 'from __main__ import d, {}'.format(i)         spec.at[i, j] = timeit(stmt, setp, number=50)", "score": 845}}
{"question": "How do I select rows from a DataFrame based on column values?", "tags": ["python", "pandas", "dataframe", "indexing", "filter"], "link": "https://stackoverflow.com/questions/17071871/how-do-i-select-rows-from-a-dataframe-based-on-column-values", "answer_count": 20, "answers": {"id": 17071908, "body": "To select rows whose column value equals a scalar,  some_value , use  == :   df.loc[df['column_name'] == some_value]    To select rows whose column value is in an iterable,  some_values , use  isin :   df.loc[df['column_name'].isin(some_values)]    Combine multiple conditions with  & :   df.loc[(df['column_name'] >= A) & (df['column_name'] <= B)]    Note the parentheses. Due to Python's  operator precedence rules ,  &  binds more tightly than  <=  and  >= . Thus, the parentheses in the last example are necessary. Without the parentheses   df['column_name'] >= A & df['column_name'] <= B    is parsed as   df['column_name'] >= (A & df['column_name']) <= B    which results in a  Truth value of a Series is ambiguous error .     To select rows whose column value  does not equal   some_value , use  != :   df.loc[df['column_name'] != some_value]    The  isin  returns a boolean Series, so to select rows whose value is  not  in  some_values , negate the boolean Series using  ~ :   df = df.loc[~df['column_name'].isin(some_values)] # .loc is not in-place replacement      For example,   import pandas as pd import numpy as np df = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),                    'B': 'one one two three two two one three'.split(),                    'C': np.arange(8), 'D': np.arange(8) * 2}) print(df) #      A      B  C   D # 0  foo    one  0   0 # 1  bar    one  1   2 # 2  foo    two  2   4 # 3  bar  three  3   6 # 4  foo    two  4   8 # 5  bar    two  5  10 # 6  foo    one  6  12 # 7  foo  three  7  14  print(df.loc[df['A'] == 'foo'])    yields        A      B  C   D 0  foo    one  0   0 2  foo    two  2   4 4  foo    two  4   8 6  foo    one  6  12 7  foo  three  7  14      If you have multiple values you want to include, put them in a list (or more generally, any iterable) and use  isin :   print(df.loc[df['B'].isin(['one','three'])])    yields        A      B  C   D 0  foo    one  0   0 1  bar    one  1   2 3  bar  three  3   6 6  foo    one  6  12 7  foo  three  7  14      Note, however, that if you wish to do this many times, it is more efficient to make an index first, and then use  df.loc :   df = df.set_index(['B']) print(df.loc['one'])    yields          A  C   D B               one  foo  0   0 one  bar  1   2 one  foo  6  12    or, to include multiple values from the index use  df.index.isin :   df.loc[df.index.isin(['one','two'])]    yields          A  C   D B               one  foo  0   0 one  bar  1   2 two  foo  2   4 two  foo  4   8 two  bar  5  10 one  foo  6  12", "score": 6606}}
{"question": "What is the difference between __str__ and __repr__?", "tags": ["python", "magic-methods", "repr"], "link": "https://stackoverflow.com/questions/1436703/what-is-the-difference-between-str-and-repr", "answer_count": 29, "answers": {"id": 1436756, "body": "Unless you specifically act to ensure otherwise, most classes don't have helpful results for either:   >>> class Sic(object): pass ...  >>> print(str(Sic())) <__main__.Sic object at 0x8b7d0> >>> print(repr(Sic())) <__main__.Sic object at 0x8b7d0>    As you see -- no difference, and no info beyond the class and object's  id .  If you only override one of the two:   >>> class Sic(object):  ...   def __repr__(self): return 'foo' ...  >>> print(str(Sic())) foo >>> print(repr(Sic())) foo >>> class Sic(object): ...   def __str__(self): return 'foo' ...  >>> print(str(Sic())) foo >>> print(repr(Sic())) <__main__.Sic object at 0x2617f0>    As you see, if you override  __repr__ , that's ALSO used for  __str__ , but not vice versa.   Other crucial tidbits to know:  __str__  on a built-on container uses the  __repr__ , NOT the  __str__ , for the items it contains. And, despite the words on the subject found in typical docs, hardly anybody bothers making the  __repr__  of objects be a string that  eval  may use to build an equal object (it's just too hard, AND not knowing how the relevant module was actually imported makes it actually flat out impossible).   So, my advice: focus on making  __str__  reasonably human-readable, and  __repr__  as unambiguous as you possibly can, even if that interferes with the fuzzy unattainable goal of making  __repr__ 's returned value acceptable as input to  eval !", "score": 556}}
{"question": "What is the difference between __str__ and __repr__?", "tags": ["python", "magic-methods", "repr"], "link": "https://stackoverflow.com/questions/1436703/what-is-the-difference-between-str-and-repr", "answer_count": 29, "answers": {"id": 1438297, "body": "My rule of thumb:   __repr__  is for developers,  __str__  is for customers.", "score": 840}}
{"question": "What is the difference between __str__ and __repr__?", "tags": ["python", "magic-methods", "repr"], "link": "https://stackoverflow.com/questions/1436703/what-is-the-difference-between-str-and-repr", "answer_count": 29, "answers": {"id": 2626364, "body": "Alex Martelli summarized well  but, surprisingly, was too succinct.   First, let me reiterate the main points in  Alex \u2019s post:     The default implementation is useless (it\u2019s hard to think of one which wouldn\u2019t be, but yeah)   __repr__  goal is to be unambiguous   __str__  goal is to be readable   Container\u2019s  __str__  uses contained objects\u2019  __repr__     Default implementation is useless   This is mostly a surprise because Python\u2019s defaults tend to be fairly useful. However, in this case, having a default for  __repr__  which would act like:   return \"%s(%r)\" % (self.__class__, self.__dict__)    Or in new f-string formatting:   return f\"{self.__class__!s}({self.__dict__!r})\"    would have been too dangerous (for example, too easy to get into infinite recursion if objects reference each other). So Python cops out. Note that there is one default which is true: if  __repr__  is defined, and  __str__  is not, the object will behave as though  __str__=__repr__ .   This means, in simple terms: almost every object you implement should have a functional  __repr__  that\u2019s usable for understanding the object. Implementing  __str__  is optional: do that if you need a \u201cpretty print\u201d functionality (for example, used by a report generator).   The goal of  __repr__  is to be unambiguous   Let me come right out and say it \u2014 I do not believe in debuggers. I don\u2019t really know how to use any debugger, and have never used one seriously. Furthermore, I believe that the big fault in debuggers is their basic nature \u2014 most failures I debug happened a long long time ago, in a galaxy far far away. This means that I do believe, with religious fervor, in logging. Logging is the lifeblood of any decent fire-and-forget server system. Python makes it easy to log: with maybe some project specific wrappers, all you need is a   log(INFO, \"I am in the weird function and a is\", a, \"and b is\", b, \"but I got a null C \u2014 using default\", default_c)    But you have to do the last step \u2014 make sure every object you implement has a useful repr, so code like that can just work. This is why the \u201ceval\u201d thing comes up: if you have enough information so  eval(repr(c))==c , that means you know everything there is to know about  c . If that\u2019s easy enough, at least in a fuzzy way, do it. If not, make sure you have enough information about  c  anyway. I usually use an eval-like format:  \"MyClass(this=%r,that=%r)\" % (self.this,self.that) . It does not mean that you can actually construct MyClass, or that those are the right constructor arguments \u2014 but it is a useful form to express \u201cthis is everything you need to know about this instance\u201d.   Note: I used  %r  above, not  %s . You always want to use  repr()  [or  %r  formatting character, equivalently] inside  __repr__  implementation, or you\u2019re defeating the goal of repr. You want to be able to differentiate  MyClass(3)  and  MyClass(\"3\") .   The goal of  __str__  is to be readable   Specifically, it is not intended to be unambiguous \u2014 notice that  str(3)==str(\"3\") . Likewise, if you implement an IP abstraction, having the str of it look like 192.168.1.1 is just fine. When implementing a date/time abstraction, the str can be \"2010/4/12 15:35:22\", etc. The goal is to represent it in a way that a user, not a programmer, would want to read it. Chop off useless digits, pretend to be some other class \u2014 as long is it supports readability, it is an improvement.   Container\u2019s  __str__  uses contained objects\u2019  __repr__   This seems surprising, doesn\u2019t it? It is a little, but how readable would it be if it used their  __str__ ?   [moshe is, 3, hello world, this is a list, oh I don't know, containing just 4 elements]    Not very. Specifically, the strings in a container would find it way too easy to disturb its string representation. In the face of ambiguity, remember, Python resists the temptation to guess. If you want the above behavior when you\u2019re printing a list, just   print(\"[\" + \", \".join(lst) + \"]\")    (you can probably also figure out what to do about dictionaries).   Summary   Implement  __repr__  for any class you implement. This should be second nature. Implement  __str__  if you think it would be useful to have a string version which errs on the side of readability.", "score": 3590}}
{"question": "Convert bytes to a string in Python 3", "tags": ["python", "string", "python-3.x"], "link": "https://stackoverflow.com/questions/606191/convert-bytes-to-a-string-in-python-3", "answer_count": 23, "answers": {"id": 12073686, "body": "This joins together a list of bytes into a string:   >>> bytes_data = [112, 52, 52] >>> \"\".join(map(chr, bytes_data)) 'p44'", "score": 264}}
{"question": "Convert bytes to a string in Python 3", "tags": ["python", "string", "python-3.x"], "link": "https://stackoverflow.com/questions/606191/convert-bytes-to-a-string-in-python-3", "answer_count": 23, "answers": {"id": 606205, "body": "Decode the byte string and turn it in to a character (Unicode) string.     Python 3:   encoding = 'utf-8' b'hello'.decode(encoding)    or   str(b'hello', encoding)      Python 2:   encoding = 'utf-8' 'hello'.decode(encoding)    or   unicode('hello', encoding)", "score": 428}}
{"question": "Convert bytes to a string in Python 3", "tags": ["python", "string", "python-3.x"], "link": "https://stackoverflow.com/questions/606191/convert-bytes-to-a-string-in-python-3", "answer_count": 23, "answers": {"id": 606199, "body": "Decode the  bytes  object  to produce a string:   >>> b\"abcde\".decode(\"utf-8\") 'abcde'    The above example  assumes  that the  bytes  object is in UTF-8, because it is a common encoding. However, you should use the encoding your data is actually in!", "score": 5844}}
{"question": "What is __init__.py for?", "tags": ["python", "module", "package", "python-packaging"], "link": "https://stackoverflow.com/questions/448271/what-is-init-py-for", "answer_count": 14, "answers": {"id": 18979314, "body": "In addition to labeling a directory as a Python package and defining  __all__ ,  __init__.py  allows you to define any variable at the package level.  Doing so is often convenient if a package defines something that will be imported frequently, in an API-like fashion. This pattern promotes adherence to the Pythonic \"flat is better than nested\" philosophy.   An example   Here is an example from one of my projects, in which I frequently import a  sessionmaker  called  Session  to interact with my database. I wrote a \"database\" package with a few modules:   database/     __init__.py     schema.py     insertions.py     queries.py    My  __init__.py  contains the following code:   import os  from sqlalchemy.orm import sessionmaker from sqlalchemy import create_engine  engine = create_engine(os.environ['DATABASE_URL']) Session = sessionmaker(bind=engine)    Since I define  Session  here, I can start a new session using the syntax below. This code would be the same executed from inside or outside of the \"database\" package directory.   from database import Session session = Session()    Of course, this is a small convenience -- the alternative would be to define  Session  in a new file like \"create_session.py\" in my database package, and start new sessions using:   from database.create_session import Session session = Session()    Further reading   There is a pretty interesting reddit thread covering appropriate uses of  __init__.py  here:   http://www.reddit.com/r/Python/comments/1bbbwk/whats_your_opinion_on_what_to_include_in_init_py/   The majority opinion seems to be that  __init__.py  files should be very thin to avoid violating the \"explicit is better than implicit\" philosophy.", "score": 687}}
{"question": "What is __init__.py for?", "tags": ["python", "module", "package", "python-packaging"], "link": "https://stackoverflow.com/questions/448271/what-is-init-py-for", "answer_count": 14, "answers": {"id": 4116384, "body": "Files named  __init__.py  are used to mark directories on disk as Python package directories. If you have the files   mydir/spam/__init__.py mydir/spam/module.py    and  mydir  is on your path, you can import the code in  module.py  as   import spam.module    or   from spam import module    If you remove the  __init__.py  file, Python will no longer look for submodules inside that directory, so attempts to import the module will fail.   The  __init__.py  file is usually empty, but can be used to export selected portions of the package under more convenient name, hold convenience functions, etc. Given the example above, the contents of the init module can be accessed as   import spam    This answer is based on  this webpage .", "score": 1333}}
{"question": "What is __init__.py for?", "tags": ["python", "module", "package", "python-packaging"], "link": "https://stackoverflow.com/questions/448271/what-is-init-py-for", "answer_count": 14, "answers": {"id": 448279, "body": "It used to be a required part of a package ( old, pre-3.3 \"regular package\" , not  newer 3.3+ \"namespace package\" ).   Here's the documentation.     Python defines two types of packages, regular packages and namespace packages. Regular packages are traditional packages as they existed in Python 3.2 and earlier. A regular package is typically implemented as a directory containing an  __init__.py  file. When a regular package is imported, this  __init__.py  file is implicitly executed, and the objects it defines are bound to names in the package\u2019s namespace. The  __init__.py  file can contain the same Python code that any other module can contain, and Python will add some additional attributes to the module when it is imported.     But just click the link, it contains an example, more information, and an explanation of namespace packages, the kind of packages without  __init__.py .", "score": 2086}}
{"question": "How do I copy a file?", "tags": ["python", "file", "copy", "filesystems", "file-copying"], "link": "https://stackoverflow.com/questions/123198/how-do-i-copy-a-file", "answer_count": 21, "answers": {"id": 123238, "body": "copy2(src,dst)  is often more useful than  copyfile(src,dst)  because:     it allows  dst  to be a  directory  (instead of the complete target filename), in which case the  basename  of  src  is used for creating the new file;   it preserves the original modification and access info (mtime and atime) in the file metadata (however, this comes with a slight overhead).     Here is a short example:   import shutil shutil.copy2('/src/dir/file.ext', '/dst/dir/newname.ext') # complete target filename given shutil.copy2('/src/file.ext', '/dst/dir') # target filename is /dst/dir/file.ext", "score": 1000}}
{"question": "How do I copy a file?", "tags": ["python", "file", "copy", "filesystems", "file-copying"], "link": "https://stackoverflow.com/questions/123198/how-do-i-copy-a-file", "answer_count": 21, "answers": {"id": 30359308, "body": "Function   Copies metadata   Copies permissions   Uses file object   Destination may be directory           shutil.copy   No   Yes   No   Yes       shutil.copyfile   No   No   No   No       shutil.copy2   Yes   Yes   No   Yes       shutil.copyfileobj   No   No   Yes   No", "score": 2196}}
{"question": "How do I copy a file?", "tags": ["python", "file", "copy", "filesystems", "file-copying"], "link": "https://stackoverflow.com/questions/123198/how-do-i-copy-a-file", "answer_count": 21, "answers": {"id": 123212, "body": "shutil  has many methods you can use. One of which is:   import shutil  shutil.copyfile(src, dst)  # 2nd option shutil.copy(src, dst)  # dst can be a folder; use shutil.copy2() to preserve timestamp      Copy the contents of the file named  src  to a file named  dst . Both  src  and  dst  need to be the entire filename of the files, including path.   The destination location must be writable; otherwise, an  IOError  exception will be raised.   If  dst  already exists, it will be replaced.   Special files such as character or block devices and pipes cannot be copied with this function.   With  copy ,  src  and  dst  are path names given as  str s.     Another  shutil  method to look at is  shutil.copy2() . It's similar but preserves more metadata (e.g. time stamps).   If you use  os.path  operations, use  copy  rather than  copyfile .  copyfile  will only accept strings.", "score": 4833}}
{"question": "How can I catch multiple exceptions in one line? (in the &quot;except&quot; block)", "tags": ["python", "exception"], "link": "https://stackoverflow.com/questions/6470428/how-can-i-catch-multiple-exceptions-in-one-line-in-the-except-block", "answer_count": 6, "answers": {"id": 26650022, "body": "From  Python documentation -> 8.3 Handling Exceptions :     A  try  statement may have more than one except clause, to specify   handlers for different exceptions. At most one handler will be   executed. Handlers only handle exceptions that occur in the   corresponding try clause, not in other handlers of the same try   statement. An except clause may name multiple exceptions as a   parenthesized tuple, for example:   except (RuntimeError, TypeError, NameError):     pass    Note that the parentheses around this tuple are required, because   except  ValueError, e:  was the syntax used for what is normally   written as  except ValueError as e:  in modern Python (described   below). The old syntax is still supported for backwards compatibility.   This means  except RuntimeError, TypeError  is not equivalent to    except (RuntimeError, TypeError):  but to  except RuntimeError as   TypeError:  which is not what you want.", "score": 76}}
{"question": "How can I catch multiple exceptions in one line? (in the &quot;except&quot; block)", "tags": ["python", "exception"], "link": "https://stackoverflow.com/questions/6470428/how-can-i-catch-multiple-exceptions-in-one-line-in-the-except-block", "answer_count": 6, "answers": {"id": 24338247, "body": "How do I catch multiple exceptions in one line (except block)     Do this:   try:     may_raise_specific_errors(): except (SpecificErrorOne, SpecificErrorTwo) as error:     handle(error) # might log or have some other default behavior...    The parentheses are required due to older syntax that used the commas to assign the error object to a name. The  as  keyword is used for the assignment. You can use any name for the error object, I prefer  error  personally.   Best Practice   To do this in a manner currently and forward compatible with Python, you need to separate the Exceptions with commas and wrap them with parentheses to differentiate from earlier syntax that assigned the exception instance to a variable name by following the Exception type to be caught with a comma.    Here's an example of simple usage:   import sys  try:     mainstuff() except (KeyboardInterrupt, EOFError): # the parens are necessary     sys.exit(0)    I'm specifying only these exceptions to avoid hiding bugs, which if I encounter I expect the full stack trace from.   This is documented here:  https://docs.python.org/tutorial/errors.html   You can assign the exception to a variable, ( e  is common, but you might prefer a more verbose variable if you have long exception handling or your IDE only highlights selections larger than that, as mine does.) The instance has an args attribute. Here is an example:   import sys  try:     mainstuff() except (KeyboardInterrupt, EOFError) as err:      print(err)     print(err.args)     sys.exit(0)    Note that in Python 3, the  err  object falls out of scope when the  except  block is concluded.   Deprecated   You may see code that assigns the error with a comma. This usage, the only form available in Python 2.5 and earlier, is deprecated, and if you wish your code to be forward compatible in Python 3, you should update the syntax to use the new form:   import sys  try:     mainstuff() except (KeyboardInterrupt, EOFError), err: # don't do this in Python 2.6+     print err     print err.args     sys.exit(0)    If you see the comma name assignment in your codebase, and you're using Python 2.5 or higher, switch to the new way of doing it so your code remains compatible when you upgrade.   The  suppress  context manager   The accepted answer is really 4 lines of code, minimum:   try:     do_something() except (IDontLikeYouException, YouAreBeingMeanException) as e:     pass    The  try ,  except ,  pass  lines can be handled in a single line with the  suppress context manager, available in Python 3.4 :   from contextlib import suppress  with suppress(IDontLikeYouException, YouAreBeingMeanException):      do_something()    So when you want to  pass  on certain exceptions, use  suppress .", "score": 548}}
{"question": "How can I catch multiple exceptions in one line? (in the &quot;except&quot; block)", "tags": ["python", "exception"], "link": "https://stackoverflow.com/questions/6470428/how-can-i-catch-multiple-exceptions-in-one-line-in-the-except-block", "answer_count": 6, "answers": {"id": 6470452, "body": "From  Python Documentation :     An except clause may name multiple exceptions as a parenthesized tuple, for example     except (IDontLikeYouException, YouAreBeingMeanException) as e:     pass    Or, for Python 2 only:   except (IDontLikeYouException, YouAreBeingMeanException), e:     pass    Separating the exception from the variable with a comma will still work in Python 2.6 and 2.7, but is now deprecated and does not work in Python 3; now you should be using  as .", "score": 5242}}
{"question": "How do I get the current time in Python?", "tags": ["python", "datetime", "time"], "link": "https://stackoverflow.com/questions/415511/how-do-i-get-the-current-time-in-python", "answer_count": 54, "answers": {"id": 14229023, "body": "from datetime import datetime datetime.now().strftime('%Y-%m-%d %H:%M:%S')    Example output:  '2013-09-18 11:16:32'   See list of  strftime  directives .", "score": 886}}
{"question": "How do I get the current time in Python?", "tags": ["python", "datetime", "time"], "link": "https://stackoverflow.com/questions/415511/how-do-i-get-the-current-time-in-python", "answer_count": 54, "answers": {"id": 415525, "body": "Use  time.strftime() :   >>> from time import gmtime, strftime >>> strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()) '2009-01-05 22:14:39'", "score": 1233}}
{"question": "How do I get the current time in Python?", "tags": ["python", "datetime", "time"], "link": "https://stackoverflow.com/questions/415511/how-do-i-get-the-current-time-in-python", "answer_count": 54, "answers": {"id": 415519, "body": "Use  datetime :   >>> import datetime >>> now = datetime.datetime.now() >>> now datetime.datetime(2009, 1, 6, 15, 8, 24, 78915) >>> print(now) 2009-01-06 15:08:24.789150    For just the clock time without the date:   >>> now.time() datetime.time(15, 8, 24, 78915) >>> print(now.time()) 15:08:24.789150      To save typing, you can import the  datetime  object from the  datetime  module:   >>> from datetime import datetime    Then remove the prefix  datetime.  from all of the above.", "score": 4006}}
{"question": "How can I use a global variable in a function?", "tags": ["python", "global-variables", "scope"], "link": "https://stackoverflow.com/questions/423379/how-can-i-use-a-global-variable-in-a-function", "answer_count": 27, "answers": {"id": 423401, "body": "You may want to explore the notion of  namespaces . In Python, the  module  is the natural place for  global  data:     Each module has its own private symbol table, which is used as the global symbol table by all functions defined in the module. Thus, the author of a module can use global variables in the module without worrying about accidental clashes with a user\u2019s global variables. On the other hand, if you know what you are doing you can touch a module\u2019s global variables with the same notation used to refer to its functions,  modname.itemname .     A specific use of global-in-a-module is described here -  How do I share global variables across modules? , and for completeness the contents are shared here:     The canonical way to share information across modules within a single program is to create a special configuration module (often called  config  or  cfg ). Just import the configuration module in all modules of your application; the module then becomes available as a global name. Because there is only one instance of each module, any changes made to the module object get reflected everywhere. For example:       File: config.py       x = 0   # Default value of the 'x' configuration setting        File: mod.py     import config config.x = 1      File: main.py     import config import mod print config.x", "score": 283}}
{"question": "How can I use a global variable in a function?", "tags": ["python", "global-variables", "scope"], "link": "https://stackoverflow.com/questions/423379/how-can-i-use-a-global-variable-in-a-function", "answer_count": 27, "answers": {"id": 423668, "body": "If I'm understanding your situation correctly, what you're seeing is the result of how Python handles local (function) and global (module) namespaces.   Say you've got a module like this:   # sample.py _my_global = 5  def func1():     _my_global = 42  def func2():     print _my_global  func1() func2()    You might be expecting this to print 42, but instead, it prints 5.  As has already been mentioned, if you add a ' global ' declaration to  func1() , then  func2()  will print 42.   def func1():     global _my_global      _my_global = 42    What's going on here is that Python assumes that any name that is  assigned to , anywhere within a function, is local to that function unless explicitly told otherwise.  If it is only  reading  from a name, and the name doesn't exist locally, it will try to look up the name in any containing scopes (e.g. the module's global scope).   When you assign 42 to the name  _my_global , therefore, Python creates a local variable that shadows the global variable of the same name.  That local goes out of scope and is  garbage-collected  when  func1()  returns; meanwhile,  func2()  can never see anything other than the (unmodified) global name.  Note that this namespace decision happens at compile time, not at runtime -- if you were to read the value of  _my_global  inside  func1()  before you assign to it, you'd get an  UnboundLocalError , because Python has already decided that it must be a local variable but it has not had any value associated with it yet.  But by using the ' global ' statement, you tell Python that it should look elsewhere for the name instead of assigning to it locally.   (I believe that this behavior originated largely through optimization of local namespaces -- without this behavior,  Python's VM would need to perform at least three name lookups each time a new name is assigned to inside a function (to ensure that the name didn't already exist at module/builtin level), which would significantly slow down a very common operation.)", "score": 928}}
{"question": "How can I use a global variable in a function?", "tags": ["python", "global-variables", "scope"], "link": "https://stackoverflow.com/questions/423379/how-can-i-use-a-global-variable-in-a-function", "answer_count": 27, "answers": {"id": 423596, "body": "You can use a global variable within other functions by declaring it as  global   within each function that assigns a value to it :   globvar = 0  def set_globvar_to_one():     global globvar    # Needed to modify global copy of globvar     globvar = 1  def print_globvar():     print(globvar)     # No need for global declaration to read value of globvar  set_globvar_to_one() print_globvar()       # Prints 1    Since it's unclear whether  globvar = 1  is creating a local variable or changing a global variable, Python defaults to creating a local variable, and makes you explicitly choose the other behavior with the  global  keyword.   See other answers if you want to share a global variable across modules.", "score": 5268}}
{"question": "How can I iterate over rows in a Pandas DataFrame?", "tags": ["python", "pandas", "dataframe", "loops"], "link": "https://stackoverflow.com/questions/16476924/how-can-i-iterate-over-rows-in-a-pandas-dataframe", "answer_count": 35, "answers": {"id": 41022840, "body": "First consider if you really need to  iterate  over rows in a DataFrame. See  cs95's answer  for alternatives.   If you still need to iterate over rows, you can use methods below. Note some   important caveats  which are not mentioned in any of the other answers.     DataFrame.iterrows()   for index, row in df.iterrows():     print(row[\"c1\"], row[\"c2\"])      DataFrame.itertuples()   for row in df.itertuples(index=True, name='Pandas'):     print(row.c1, row.c2)        itertuples()  is supposed to be faster than  iterrows()   But be aware, according to the docs (pandas 0.24.2 at the moment):     iterrows:  dtype  might not match from row to row    Because iterrows returns a Series for each row, it  does not preserve  dtypes across the rows (dtypes are preserved across columns for DataFrames). To preserve dtypes while iterating over the rows, it is better to use itertuples() which returns namedtuples of the values and which is generally much faster than iterrows()           iterrows: Do not modify rows     You should  never modify  something you are iterating over. This is not guaranteed to work in all cases. Depending on the data types, the iterator returns a copy and not a view, and writing to it will have no effect.     Use  DataFrame.apply()  instead:   new_df = df.apply(lambda x: x * 2, axis=1)      itertuples:     The column names will be renamed to positional names if they are invalid Python identifiers, repeated, or start with an underscore. With a large number of columns (>255), regular tuples are returned.         See  pandas docs on iteration  for more details.", "score": 579}}
{"question": "How can I iterate over rows in a Pandas DataFrame?", "tags": ["python", "pandas", "dataframe", "loops"], "link": "https://stackoverflow.com/questions/16476924/how-can-i-iterate-over-rows-in-a-pandas-dataframe", "answer_count": 35, "answers": {"id": 55557758, "body": "How to iterate over rows in a DataFrame in Pandas     Answer: DON'T * !   Iteration in Pandas is an anti-pattern and is something you should only do when you have exhausted every other option. You should not use any function with \" iter \" in its name for more than a few thousand rows or you will have to get used to a  lot  of waiting.   Do you want to print a DataFrame? Use  DataFrame.to_string() .   Do you want to compute something? In that case, search for methods in this order (list modified from  here ):     Vectorization   Cython  routines   List Comprehensions (vanilla  for  loop)   DataFrame.apply() : i) \u00a0Reductions that can be performed in Cython, ii) Iteration in Python space   items()   iteritems()   (deprecated since v1.5.0)   DataFrame.itertuples()   DataFrame.iterrows()     iterrows  and  itertuples  (both receiving many votes in answers to this question) should be used in very rare circumstances, such as generating row objects/nametuples for sequential processing, which is really the only thing these functions are useful for.   Appeal to Authority   The documentation page  on iteration has a huge red warning box that says:     Iterating through pandas objects is generally slow. In many cases, iterating manually over the rows is not needed [...].     * It's actually a little more complicated than \"don't\".  df.iterrows()  is the correct answer to this question, but \"vectorize your ops\" is the better one. I will concede that there are circumstances where iteration cannot be avoided (for example, some operations where the result depends on the value computed for the previous row). However, it takes some familiarity with the library to know when. If you're not sure whether you need an iterative solution, you probably don't. PS: To know more about my rationale for writing this answer, skip to the very bottom.     Faster than Looping:  Vectorization ,  Cython   A good number of basic operations and computations are \"vectorised\" by pandas (either through NumPy, or through Cythonized functions). This includes arithmetic, comparisons, (most) reductions, reshaping (such as pivoting), joins, and groupby operations. Look through the documentation on  Essential Basic Functionality  to find a suitable vectorised method for your problem.   If none exists, feel free to write your own using custom  Cython extensions .     Next Best Thing:  List Comprehensions *   List comprehensions should be your next port of call if 1) there is no vectorized solution available, 2) performance is important, but not important enough to go through the hassle of cythonizing your code, and 3) you're trying to perform elementwise transformation on your code. There is a  good amount of evidence  to suggest that list comprehensions are sufficiently fast (and even sometimes faster) for many common Pandas tasks.   The formula is simple,   # Iterating over one column - `f` is some function that processes your data result = [f(x) for x in df['col']]  # Iterating over two columns, use `zip` result = [f(x, y) for x, y in zip(df['col1'], df['col2'])]  # Iterating over multiple columns - same data type result = [f(row[0], ..., row[n]) for row in df[['col1', ...,'coln']].to_numpy()]  # Iterating over multiple columns - differing data type result = [f(row[0], ..., row[n]) for row in zip(df['col1'], ..., df['coln'])]    If you can encapsulate your business logic into a function, you can use a list comprehension that calls it. You can make arbitrarily complex things work through the simplicity and speed of raw Python code.   Caveats   List comprehensions assume that your data is easy to work with - what that means is your data types are consistent and you don't have NaNs, but this cannot always be guaranteed.     The first one is more obvious, but when dealing with NaNs, prefer in-built pandas methods if they exist (because they have much better corner-case handling logic), or ensure your business logic includes appropriate NaN handling logic.   When dealing with mixed data types you should iterate over  zip(df['A'], df['B'], ...)  instead of  df[['A', 'B']].to_numpy()  as the latter implicitly upcasts data to the most common type. As an example if A is numeric and B is string,  to_numpy()  will cast the entire array to string, which may not be what you want. Fortunately  zip ping your columns together is the most straightforward workaround to this.     *Your mileage may vary for the reasons outlined in the  Caveats  section above.     An Obvious Example   Let's demonstrate the difference with a simple example of adding two pandas columns  A + B . This is a vectorizable operation, so it will be easy to contrast the performance of the methods discussed above.     Benchmarking code, for your reference . The line at the bottom measures a function written in numpandas, a style of Pandas that mixes heavily with NumPy to squeeze out maximum performance. Writing numpandas code should be avoided unless you know what you're doing. Stick to the API where you can (i.e., prefer  vec  over  vec_numpy ).   I should mention, however, that it isn't always this cut and dry. Sometimes the answer to \"what is the best method for an operation\" is \"it depends on your data\". My advice is to test out different approaches on your data before settling on one.     My Personal Opinion  *   Most of the analyses performed on the various alternatives to the iter family has been through the lens of performance. However, in most situations you will typically be working on a reasonably sized dataset (nothing beyond a few thousand or 100K rows) and performance will come second to simplicity/readability of the solution.   Here is my personal preference when selecting a method to use for a problem.   For the novice:     Vectorization  (when possible) ;  apply() ; List Comprehensions;  itertuples() / iteritems() ;  iterrows() ; Cython     For the more experienced:     Vectorization  (when possible) ;  apply() ; List Comprehensions; Cython;  itertuples() / iteritems() ;  iterrows()     Vectorization prevails as the most idiomatic method for any problem that can be vectorized. Always seek to vectorize! When in doubt, consult the docs, or look on Stack Overflow for an existing question on your particular task.   I do tend to go on about how bad  apply  is in a lot of my posts, but I do concede it is easier for a beginner to wrap their head around what it's doing. Additionally, there are quite a few use cases for  apply  has explained in  this post of mine .   Cython ranks lower down on the list because it takes more time and effort to pull off correctly. You will usually never need to write code with pandas that demands this level of performance that even a list comprehension cannot satisfy.   * As with any personal opinion, please take with heaps of salt!     Further Reading     10 Minutes to pandas , and  Essential Basic Functionality  - Useful links that introduce you to Pandas and its library of vectorized*/cythonized functions.     Enhancing Performance  - A primer from the documentation on enhancing standard Pandas operations     Are for-loops in pandas really bad? When should I care?  - a detailed write-up by me on list comprehensions and their suitability for various operations (mainly ones involving non-numeric data)     When should I (not) want to use pandas apply() in my code?  -  apply  is slow (but not as slow as the  iter*  family. There are, however, situations where one can (or should) consider  apply  as a serious alternative, especially in some  GroupBy  operations).       * Pandas string methods are \"vectorized\" in the sense that they are specified on the series but operate on each element. The underlying mechanisms are still iterative, because string operations are inherently hard to vectorize.     Why I Wrote this Answer   A common trend I notice from new users is to ask questions of the form \"How can I iterate over my df to do X?\". Showing code that calls  iterrows()  while doing something inside a  for  loop. Here is why. A new user to the library who has not been introduced to the concept of vectorization will likely envision the code that solves their problem as iterating over their data to do something. Not knowing how to iterate over a DataFrame, the first thing they do is Google it and end up here, at this question. They then see the accepted answer telling them how to, and they close their eyes and run this code without ever first questioning if iteration is the right thing to do.   The aim of this answer is to help new users understand that iteration is not necessarily the solution to every problem, and that better, faster and more idiomatic solutions could exist, and that it is worth investing time in exploring them. I'm not trying to start a war of iteration vs. vectorization, but I want new users to be informed when developing solutions to their problems with this library.   And finally ... a TLDR to summarize this post", "score": 2515}}
{"question": "How can I iterate over rows in a Pandas DataFrame?", "tags": ["python", "pandas", "dataframe", "loops"], "link": "https://stackoverflow.com/questions/16476924/how-can-i-iterate-over-rows-in-a-pandas-dataframe", "answer_count": 35, "answers": {"id": 16476974, "body": "DataFrame.iterrows  is a generator which yields both the index and row (as a Series):   import pandas as pd  df = pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 110, 120]}) df = df.reset_index()  # make sure indexes pair with number of rows  for index, row in df.iterrows():     print(row['c1'], row['c2'])    10 100 11 110 12 120      Obligatory disclaimer from the  documentation     Iterating through pandas objects is generally  slow . In many cases, iterating manually over the rows is not needed and can be avoided with one of the following approaches:     Look for a  vectorized  solution: many operations can be performed using built-in methods or NumPy functions, (boolean) indexing, \u2026   When you have a function that cannot work on the full DataFrame/Series at once, it is better to use  apply()  instead of iterating over the values. See the docs on  function application .   If you need to do iterative manipulations on the values but performance is important, consider writing the inner loop with cython or numba. See the  enhancing performance  section for some examples of this approach.       Other answers in this thread delve into greater depth on alternatives to iter* functions if you are interested to learn more.", "score": 5479}}
{"question": "Iterating over dictionaries using &#39;for&#39; loops", "tags": ["python", "loops", "dictionary", "key"], "link": "https://stackoverflow.com/questions/3294889/iterating-over-dictionaries-using-for-loops", "answer_count": 17, "answers": {"id": 3295662, "body": "Iterating over a  dict  iterates through its keys in no particular order, as you can see here:   (This is practically  no longer the case since Python 3.6 , but note that it's only guaranteed behaviour since Python 3.7.)   >>> d = {'x': 1, 'y': 2, 'z': 3} >>> list(d) ['y', 'x', 'z'] >>> d.keys() ['y', 'x', 'z']    For your example, it is a better idea to use  dict.items() :   >>> d.items() [('y', 2), ('x', 1), ('z', 3)]    This gives you a list of tuples. When you loop over them like this, each tuple is unpacked into  k  and  v  automatically:   for k,v in d.items():     print(k, 'corresponds to', v)    Using  k  and  v  as variable names when looping over a  dict  is quite common if the body of the loop is only a few lines. For more complicated loops it may be a good idea to use more descriptive names:   for letter, number in d.items():     print(letter, 'corresponds to', number)    It's a good idea to get into the habit of using format strings:   for letter, number in d.items():     print('{0} corresponds to {1}'.format(letter, number))", "score": 277}}
{"question": "Iterating over dictionaries using &#39;for&#39; loops", "tags": ["python", "loops", "dictionary", "key"], "link": "https://stackoverflow.com/questions/3294889/iterating-over-dictionaries-using-for-loops", "answer_count": 17, "answers": {"id": 3295295, "body": "It's not that key is a special word, but that dictionaries implement the iterator protocol.  You could do this in your class, e.g. see  this question  for how to build class iterators.   In the case of dictionaries, it's implemented at the C level.  The details are available in  PEP 234 .  In particular, the section titled \"Dictionary Iterators\":       Dictionaries implement a tp_iter slot that returns an efficient   iterator that iterates over the keys of the dictionary. [...] This    means that we can write   for k in dict: ...    which is equivalent to, but much faster than   for k in dict.keys(): ...    as long as the restriction on modifications to the dictionary   (either by the loop or by another thread) are not violated.   Add methods to dictionaries that return different kinds of   iterators explicitly:   for key in dict.iterkeys(): ...  for value in dict.itervalues(): ...  for key, value in dict.iteritems(): ...    This means that  for x in dict  is shorthand for  for x in    dict.iterkeys() .       In Python 3,  dict.iterkeys() ,  dict.itervalues()  and  dict.iteritems()  are no longer supported. Use  dict.keys() ,  dict.values()  and  dict.items()  instead.", "score": 565}}
{"question": "Iterating over dictionaries using &#39;for&#39; loops", "tags": ["python", "loops", "dictionary", "key"], "link": "https://stackoverflow.com/questions/3294889/iterating-over-dictionaries-using-for-loops", "answer_count": 17, "answers": {"id": 3294899, "body": "key  is just a variable name.     for key in d:    will simply loop over the keys in the dictionary, rather than the keys and values.  To loop over both key and value you can use the following:   For Python 3.x:   for key, value in d.items():    For Python 2.x:   for key, value in d.iteritems():    To test for yourself, change the word  key  to  poop .   In Python 3.x,  iteritems()  was replaced with simply  items() , which returns a set-like view backed by the dict, like  iteritems()  but even better.  This is also available in 2.7 as  viewitems() .    The operation  items()  will work for both 2 and 3, but in 2 it will return a list of the dictionary's  (key, value)  pairs, which will not reflect changes to the dict that happen after the  items()  call. If you want the 2.x behavior in 3.x, you can call  list(d.items()) .", "score": 6990}}
{"question": "How can I find the index for a given item in a list?", "tags": ["python", "list", "indexing"], "link": "https://stackoverflow.com/questions/176918/how-can-i-find-the-index-for-a-given-item-in-a-list", "answer_count": 47, "answers": {"id": 17300987, "body": "To get all indexes:   indexes = [i for i, x in enumerate(xs) if x == 'foo']", "score": 246}}
{"question": "How can I find the index for a given item in a list?", "tags": ["python", "list", "indexing"], "link": "https://stackoverflow.com/questions/176918/how-can-i-find-the-index-for-a-given-item-in-a-list", "answer_count": 47, "answers": {"id": 17202481, "body": "The majority of answers explain how to find  a single index , but their methods do not return multiple indexes if the item is in the list multiple times. Use  enumerate() :   for i, j in enumerate(['foo', 'bar', 'baz']):     if j == 'bar':         print(i)    The  index()  function only returns the first occurrence, while  enumerate()  returns all occurrences.   As a list comprehension:   [i for i, j in enumerate(['foo', 'bar', 'baz']) if j == 'bar']      Here's also another small solution with  itertools.count()  (which is pretty much the same approach as enumerate):   from itertools import izip as zip, count # izip for maximum efficiency [i for i, j in zip(count(), ['foo', 'bar', 'baz']) if j == 'bar']    This is more efficient for larger lists than using  enumerate() :   $ python -m timeit -s \"from itertools import izip as zip, count\" \"[i for i, j in zip(count(), ['foo', 'bar', 'baz']*500) if j == 'bar']\" 10000 loops, best of 3: 174 usec per loop $ python -m timeit \"[i for i, j in enumerate(['foo', 'bar', 'baz']*500) if j == 'bar']\" 10000 loops, best of 3: 196 usec per loop", "score": 722}}
{"question": "How can I find the index for a given item in a list?", "tags": ["python", "list", "indexing"], "link": "https://stackoverflow.com/questions/176918/how-can-i-find-the-index-for-a-given-item-in-a-list", "answer_count": 47, "answers": {"id": 176921, "body": ">>> [\"foo\", \"bar\", \"baz\"].index(\"bar\") 1    See  the documentation  for the built-in  .index()  method of the list:     list.index(x[, start[, end]])    Return zero-based index in the list of the first item whose value is equal to  x . Raises a  ValueError  if there is no such item.   The optional arguments  start  and  end  are interpreted as in the  slice notation  and are used to limit the search to a particular subsequence of the list. The returned index is computed relative to the beginning of the full sequence rather than the start argument.     Caveats   Linear time-complexity in list length   An  index  call checks every element of the list in order, until it finds a match. If the list is long, and if there is no guarantee that the value will be near the beginning, this can slow down the code.   This problem can only be completely avoided by using a different data structure. However, if the element is known to be within a certain part of the list, the  start  and  end  parameters can be used to narrow the search.   For example:   >>> import timeit >>> timeit.timeit('l.index(999_999)', setup='l = list(range(0, 1_000_000))', number=1000) 9.356267921015387 >>> timeit.timeit('l.index(999_999, 999_990, 1_000_000)', setup='l = list(range(0, 1_000_000))', number=1000) 0.0004404920036904514    The second call is orders of magnitude faster, because it only has to search through 10 elements, rather than all 1 million.   Only the index of the  first match  is returned   A call to  index  searches through the list in order until it finds a match, and  stops there.  If there could be more than one occurrence of the value, and all indices are needed,  index  cannot solve the problem:   >>> [1, 1].index(1) # the `1` index is not found. 0    Instead, use a  list comprehension or generator expression to do the search , with  enumerate  to get indices :   >>> # A list comprehension gives a list of indices directly: >>> [i for i, e in enumerate([1, 2, 1]) if e == 1] [0, 2] >>> # A generator comprehension gives us an iterable object... >>> g = (i for i, e in enumerate([1, 2, 1]) if e == 1) >>> # which can be used in a `for` loop, or manually iterated with `next`: >>> next(g) 0 >>> next(g) 2    The list comprehension and generator expression techniques still work if there is only one match, and are more generalizable.   Raises an exception if there is no match   As noted in the documentation above, using  .index  will raise an exception if the searched-for value is not in the list:   >>> [1, 1].index(2) Traceback (most recent call last):   File \" \", line 1, in   ValueError: 2 is not in list    If this is a concern, either  explicitly check first  using  item in my_list , or handle the exception with  try / except  as appropriate.   The explicit check is simple and readable, but it must iterate the list a second time. See  What is the EAFP principle in Python?  for more guidance on this choice.", "score": 6084}}
{"question": "How slicing in Python works", "tags": ["python", "slice", "sequence"], "link": "https://stackoverflow.com/questions/509211/how-slicing-in-python-works", "answer_count": 38, "answers": {"id": 509377, "body": "Enumerating the possibilities allowed by the grammar for the sequence  x :   >>> x[:]                # [x[0],   x[1],          ..., x[-1]    ] >>> x[low:]             # [x[low], x[low+1],      ..., x[-1]    ] >>> x[:high]            # [x[0],   x[1],          ..., x[high-1]] >>> x[low:high]         # [x[low], x[low+1],      ..., x[high-1]] >>> x[::stride]         # [x[0],   x[stride],     ..., x[-1]    ] >>> x[low::stride]      # [x[low], x[low+stride], ..., x[-1]    ] >>> x[:high:stride]     # [x[0],   x[stride],     ..., x[high-1]] >>> x[low:high:stride]  # [x[low], x[low+stride], ..., x[high-1]]    Of course, if  (high-low)%stride != 0 , then the end point will be a little lower than  high-1 .   If  stride  is negative, the ordering is changed a bit since we're counting down:   >>> x[::-stride]        # [x[-1],   x[-1-stride],   ..., x[0]    ] >>> x[high::-stride]    # [x[high], x[high-stride], ..., x[0]    ] >>> x[:low:-stride]     # [x[-1],   x[-1-stride],   ..., x[low+1]] >>> x[high:low:-stride] # [x[high], x[high-stride], ..., x[low+1]]    Extended slicing (with commas and ellipses) are mostly used only by special data structures (like NumPy); the basic sequences don't support them.   >>> class slicee: ...     def __getitem__(self, item): ...         return repr(item) ... >>> slicee()[0, 1:2, ::5, ...] '(0, slice(1, 2, None), slice(None, None, 5), Ellipsis)'", "score": 560}}
{"question": "How slicing in Python works", "tags": ["python", "slice", "sequence"], "link": "https://stackoverflow.com/questions/509211/how-slicing-in-python-works", "answer_count": 38, "answers": {"id": 509297, "body": "The  Python tutorial  talks about it (scroll down a bit until you get to the part about slicing).   The ASCII art diagram is helpful too for remembering how slices work:    +---+---+---+---+---+---+  | P | y | t | h | o | n |  +---+---+---+---+---+---+    0   1   2   3   4   5   -6  -5  -4  -3  -2  -1      One way to remember how slices work is to think of the indices as pointing  between  characters, with the left edge of the first character numbered 0. Then the right edge of the last character of a string of  n  characters has index  n .", "score": 724}}
{"question": "How slicing in Python works", "tags": ["python", "slice", "sequence"], "link": "https://stackoverflow.com/questions/509211/how-slicing-in-python-works", "answer_count": 38, "answers": {"id": 509295, "body": "The syntax is:   a[start:stop]  # items start through stop-1 a[start:]      # items start through the rest of the array a[:stop]       # items from the beginning through stop-1 a[:]           # a copy of the whole array    There is also the  step  value, which can be used with any of the above:   a[start:stop:step] # start through not past stop, by step    The key point to remember is that the  :stop  value represents the first value that is  not  in the selected slice. So, the difference between  stop  and  start  is the number of elements selected (if  step  is 1, the default).   The other feature is that  start  or  stop  may be a  negative  number, which means it counts from the end of the array instead of the beginning. So:   a[-1]    # last item in the array a[-2:]   # last two items in the array a[:-2]   # everything except the last two items    Similarly,  step  may be a negative number:   a[::-1]    # all items in the array, reversed a[1::-1]   # the first two items, reversed a[:-3:-1]  # the last two items, reversed a[-3::-1]  # everything except the last two items, reversed    Python is kind to the programmer if there are fewer items than you ask for. For example, if you ask for  a[:-2]  and  a  only contains one element, you get an empty list instead of an error. Sometimes you would prefer the error, so you have to be aware that this may happen.   Relationship with the  slice  object   A  slice  object  can represent a slicing operation, i.e.:   a[start:stop:step]    is equivalent to:   a[slice(start, stop, step)]    Slice objects also behave slightly differently depending on the number of arguments, similar to  range() , i.e. both  slice(stop)  and  slice(start, stop[, step])  are supported. To skip specifying a given argument, one might use  None , so that e.g.  a[start:]  is equivalent to  a[slice(start, None)]  or  a[::-1]  is equivalent to  a[slice(None, None, -1)] .   While the  : -based notation is very helpful for simple slicing, the explicit use of  slice()  objects simplifies the programmatic generation of slicing.", "score": 6621}}
{"question": "What is the difference between @staticmethod and @classmethod in Python?", "tags": ["python", "oop", "static-methods", "python-decorators", "class-method"], "link": "https://stackoverflow.com/questions/136097/what-is-the-difference-between-staticmethod-and-classmethod-in-python", "answer_count": 36, "answers": {"id": 136149, "body": "Basically  @classmethod  makes a method whose first argument is the class it's called from (rather than the class instance),  @staticmethod  does not have any implicit arguments.", "score": 206}}
{"question": "What is the difference between @staticmethod and @classmethod in Python?", "tags": ["python", "oop", "static-methods", "python-decorators", "class-method"], "link": "https://stackoverflow.com/questions/136097/what-is-the-difference-between-staticmethod-and-classmethod-in-python", "answer_count": 36, "answers": {"id": 136138, "body": "A  staticmethod  is a method that knows nothing about the class or instance it was called on. It just gets the arguments that were passed, no implicit first argument.   A  classmethod , on the other hand, is a method that gets passed the class it was called on, or the class of the instance it was called on, as first argument. This is useful when you want the method to be a factory for the class: since it gets the actual class it was called on as first argument, you can always instantiate the right class, even when subclasses are involved. Observe for instance how  dict.fromkeys() , a classmethod, returns an instance of the subclass when called on a subclass:   >>> class DictSubclass(dict): ...     def __repr__(self): ...         return \"DictSubclass\" ...  >>> dict.fromkeys(\"abc\") {'a': None, 'c': None, 'b': None} >>> DictSubclass.fromkeys(\"abc\") DictSubclass >>>", "score": 979}}
{"question": "What is the difference between @staticmethod and @classmethod in Python?", "tags": ["python", "oop", "static-methods", "python-decorators", "class-method"], "link": "https://stackoverflow.com/questions/136097/what-is-the-difference-between-staticmethod-and-classmethod-in-python", "answer_count": 36, "answers": {"id": 1669524, "body": "Maybe a bit of example code will help: Notice the difference in the call signatures of  foo ,  class_foo  and  static_foo :   class A(object):     def foo(self, x):         print(f\"executing foo({self}, {x})\")      @classmethod     def class_foo(cls, x):         print(f\"executing class_foo({cls}, {x})\")      @staticmethod     def static_foo(x):         print(f\"executing static_foo({x})\")  a = A()    Below is the usual way an object instance calls a method. The object instance,  a , is implicitly passed as the first argument.   a.foo(1) # executing foo(<__main__.A object at 0xb7dbef0c>, 1)      With classmethods , the class of the object instance is implicitly passed as the first argument instead of  self .   a.class_foo(1) # executing class_foo( , 1)    You can also call  class_foo  using the class. In fact, if you define something to be a classmethod, it is probably because you intend to call it from the class rather than from a class instance.  A.foo(1)  would have raised a TypeError, but  A.class_foo(1)  works just fine:   A.class_foo(1) # executing class_foo( , 1)    One use people have found for class methods is to create  inheritable alternative constructors .     With staticmethods , neither  self  (the object instance) nor   cls  (the class) is implicitly passed as the first argument. They behave like plain functions except that you can call them from an instance or the class:   a.static_foo(1) # executing static_foo(1)  A.static_foo('hi') # executing static_foo(hi)    Staticmethods are used to group functions which have some logical connection with a class to the class.     foo  is just a function, but when you call  a.foo  you don't just get the function, you get a \"partially applied\" version of the function with the object instance  a  bound as the first argument to the function.  foo  expects 2 arguments, while  a.foo  only expects 1 argument.   a  is bound to  foo . That is what is meant by the term \"bound\" below:   print(a.foo) #  >    With  a.class_foo ,  a  is not bound to  class_foo , rather the class  A  is bound to  class_foo .   print(a.class_foo) #  >    Here, with a staticmethod, even though it is a method,  a.static_foo  just returns a good 'ole function with no arguments bound.  static_foo  expects 1 argument, and  a.static_foo  expects 1 argument too.   print(a.static_foo) #      And of course the same thing happens when you call  static_foo  with the class  A  instead.   print(A.static_foo) #", "score": 3990}}
{"question": "How do I make a flat list out of a list of lists?", "tags": ["python", "list", "multidimensional-array", "flatten"], "link": "https://stackoverflow.com/questions/952914/how-do-i-make-a-flat-list-out-of-a-list-of-lists", "answer_count": 34, "answers": {"id": 952946, "body": "Note from the author : This is very inefficient. But fun, because  monoids  are awesome.   >>> xss = [[1, 2, 3], [4, 5, 6], [7], [8, 9]] >>> sum(xss, []) [1, 2, 3, 4, 5, 6, 7, 8, 9]    sum  sums the elements of the iterable  xss , and uses the second argument as the initial value  []  for the sum. (The default initial value is  0 , which is not a list.)   Because you are summing nested lists, you actually get  [1,3]+[2,4]  as a result of  sum([[1,3],[2,4]],[]) , which is equal to  [1,3,2,4] .   Note that only works on lists of lists. For lists of lists of lists, you'll need another solution.", "score": 1438}}
{"question": "How do I make a flat list out of a list of lists?", "tags": ["python", "list", "multidimensional-array", "flatten"], "link": "https://stackoverflow.com/questions/952914/how-do-i-make-a-flat-list-out-of-a-list-of-lists", "answer_count": 34, "answers": {"id": 953097, "body": "You can use  itertools.chain() :   >>> import itertools >>> list2d = [[1,2,3], [4,5,6], [7], [8,9]] >>> merged = list(itertools.chain(*list2d))    Or you can use  itertools.chain.from_iterable()  which doesn't require unpacking the list with the  *  operator:   >>> import itertools >>> list2d = [[1,2,3], [4,5,6], [7], [8,9]] >>> merged = list(itertools.chain.from_iterable(list2d))    This approach is arguably more readable than  [item for sublist in l for item in sublist]  and appears to be faster too:   $ python3 -mtimeit -s'l=[[1,2,3],[4,5,6], [7], [8,9]]*99;import itertools' 'list(itertools.chain.from_iterable(l))' 20000 loops, best of 5: 10.8 usec per loop $ python3 -mtimeit -s'l=[[1,2,3],[4,5,6], [7], [8,9]]*99' '[item for sublist in l for item in sublist]' 10000 loops, best of 5: 21.7 usec per loop $ python3 -mtimeit -s'l=[[1,2,3],[4,5,6], [7], [8,9]]*99' 'sum(l, [])' 1000 loops, best of 5: 258 usec per loop $ python3 -mtimeit -s'l=[[1,2,3],[4,5,6], [7], [8,9]]*99;from functools import reduce' 'reduce(lambda x,y: x+y,l)' 1000 loops, best of 5: 292 usec per loop $ python3 --version Python 3.7.5rc1", "score": 2442}}
{"question": "How do I make a flat list out of a list of lists?", "tags": ["python", "list", "multidimensional-array", "flatten"], "link": "https://stackoverflow.com/questions/952914/how-do-i-make-a-flat-list-out-of-a-list-of-lists", "answer_count": 34, "answers": {"id": 952952, "body": "A list of lists named  xss  can be flattened using a nested  list comprehension :   flat_list = [     x     for xs in xss     for x in xs ]    The above is equivalent to:   flat_list = []  for xs in xss:     for x in xs:         flat_list.append(x)    Here is the corresponding function:   def flatten(xss):     return [x for xs in xss for x in xs]    This is the fastest method. As evidence, using the  timeit  module in the standard library, we see:   $ python -mtimeit -s'xss=[[1,2,3],[4,5,6],[7],[8,9]]*99' '[x for xs in xss for x in xs]' 10000 loops, best of 3: 143 usec per loop  $ python -mtimeit -s'xss=[[1,2,3],[4,5,6],[7],[8,9]]*99' 'sum(xss, [])' 1000 loops, best of 3: 969 usec per loop  $ python -mtimeit -s'xss=[[1,2,3],[4,5,6],[7],[8,9]]*99' 'reduce(lambda xs, ys: xs + ys, xss)' 1000 loops, best of 3: 1.1 msec per loop    Explanation: the methods based on  +  (including the implied use in  sum ) are, of necessity,  O(L**2)  when there are L sublists -- as the intermediate result list keeps getting longer, at each step a new intermediate result list object gets allocated, and all the items in the previous intermediate result must be copied over (as well as a few new ones added at the end). So, for simplicity and without actual loss of generality, say you have L sublists of M items each: the first M items are copied back and forth  L-1  times, the second M items  L-2  times, and so on; total number of copies is M times the sum of x for x from 1 to L excluded, i.e.,  M * (L**2)/2 .   The list comprehension just generates one list, once, and copies each item over (from its original place of residence to the result list) also exactly once.", "score": 7460}}
{"question": "How can I access the index value in a &#39;for&#39; loop?", "tags": ["python", "loops", "list"], "link": "https://stackoverflow.com/questions/522563/how-can-i-access-the-index-value-in-a-for-loop", "answer_count": 28, "answers": {"id": 23886515, "body": "It's pretty simple to start it from  1  other than  0 :   for index, item in enumerate(iterable, start=1):    print index, item  # Used to print in python<3.x    print(index, item) # Migrate to print() after 3.x+", "score": 269}}
{"question": "How can I access the index value in a &#39;for&#39; loop?", "tags": ["python", "loops", "list"], "link": "https://stackoverflow.com/questions/522563/how-can-i-access-the-index-value-in-a-for-loop", "answer_count": 28, "answers": {"id": 28072982, "body": "Using a for loop, how do I access the loop index, from 1 to 5 in this case?     Use  enumerate  to get the index with the element as you iterate:   for index, item in enumerate(items):     print(index, item)    And note that Python's indexes start at zero, so you would get 0 to 4 with the above. If you want the count, 1 to 5, do this:   count = 0 # in case items is empty and you need it after the loop for count, item in enumerate(items, start=1):     print(count, item)    Unidiomatic control flow   What you are asking for is the Pythonic equivalent of the following, which is the algorithm most programmers of lower-level languages would use:     index = 0            # Python's indexing starts at zero for item in items:   # Python's for loops are a \"for each\" loop      print(index, item)     index += 1      Or in languages that do not have a for-each loop:     index = 0 while index < len(items):     print(index, items[index])     index += 1      or sometimes more commonly (but unidiomatically) found in Python:     for index in range(len(items)):     print(index, items[index])      Use the Enumerate Function   Python's  enumerate  function  reduces the visual clutter by hiding the accounting for the indexes, and encapsulating the iterable into another iterable (an  enumerate  object) that yields a two-item tuple of the index and the item that the original iterable would provide. That looks like this:   for index, item in enumerate(items, start=0):   # default is zero     print(index, item)    This code sample is fairly well the  canonical  example of the difference between code that is idiomatic of Python and code that is not. Idiomatic code is sophisticated (but not complicated) Python, written in the way that it was intended to be used. Idiomatic code is expected by the designers of the language, which means that usually this code is not just more readable, but also more efficient.   Getting a count   Even if you don't need indexes as you go, but you need a count of the iterations (sometimes desirable) you can start with  1  and the final number will be your count.   count = 0 # in case items is empty for count, item in enumerate(items, start=1):   # default is zero     print(item)  print('there were {0} items printed'.format(count))    The count seems to be more what you intend to ask for (as opposed to index) when you said you wanted from 1 to 5.     Breaking it down - a step by step explanation   To break these examples down, say we have a list of items that we want to iterate over with an index:   items = ['a', 'b', 'c', 'd', 'e']    Now we pass this iterable to enumerate, creating an enumerate object:   enumerate_object = enumerate(items) # the enumerate object    We can pull the first item out of this iterable that we would get in a loop with the  next  function:   iteration = next(enumerate_object) # first iteration from enumerate print(iteration)    And we see we get a tuple of  0 , the first index, and  'a' , the first item:   (0, 'a')    we can use what is referred to as \" sequence unpacking \" to extract the elements from this two-tuple:   index, item = iteration #   0,  'a' = (0, 'a') # essentially this.    and when we inspect  index , we find it refers to the first index, 0, and  item  refers to the first item,  'a' .   >>> print(index) 0 >>> print(item) a    Conclusion     Python indexes start at zero   To get these indexes from an iterable as you iterate over it, use the enumerate function   Using enumerate in the idiomatic way (along with tuple unpacking) creates code that is more readable and maintainable:     So do this:   for index, item in enumerate(items, start=0):   # Python indexes start at zero     print(index, item)", "score": 1355}}
{"question": "How can I access the index value in a &#39;for&#39; loop?", "tags": ["python", "loops", "list"], "link": "https://stackoverflow.com/questions/522563/how-can-i-access-the-index-value-in-a-for-loop", "answer_count": 28, "answers": {"id": 522578, "body": "Use the built-in function  enumerate() :   for idx, x in enumerate(xs):     print(idx, x)    It is  non-Pythonic  to manually index via  for i in range(len(xs)): x = xs[i]  or manually manage an additional state variable.   Check out  PEP 279  for more.", "score": 9186}}
{"question": "How do I create a directory, and any missing parent directories?", "tags": ["python", "exception", "path", "directory", "operating-system"], "link": "https://stackoverflow.com/questions/273192/how-do-i-create-a-directory-and-any-missing-parent-directories", "answer_count": 27, "answers": {"id": 5032238, "body": "Using try except and the right error code from errno module gets rid of the race condition and is cross-platform:   import os import errno  def make_sure_path_exists(path):     try:         os.makedirs(path)     except OSError as exception:         if exception.errno != errno.EEXIST:             raise    In other words, we try to create the directories, but if they already exist we ignore the error. On the other hand, any other error gets reported. For example, if you create dir 'a' beforehand and remove all permissions from it, you will get an  OSError  raised with  errno.EACCES  (Permission denied, error 13).", "score": 663}}
{"question": "How do I create a directory, and any missing parent directories?", "tags": ["python", "exception", "path", "directory", "operating-system"], "link": "https://stackoverflow.com/questions/273192/how-do-i-create-a-directory-and-any-missing-parent-directories", "answer_count": 27, "answers": {"id": 14364249, "body": "Python 3.5+:   import pathlib pathlib.Path('/my/directory').mkdir(parents=True, exist_ok=True)     pathlib.Path.mkdir  as used above recursively creates the directory and does not raise an exception if the directory already exists. If you don't need or want the parents to be created, skip the  parents  argument.   Python 3.2+:   Using  pathlib :   If you can, install the current  pathlib  backport named  pathlib2 . Do not install the older unmaintained backport named  pathlib . Next, refer to the Python 3.5+ section above and use it the same.   If using Python 3.4, even though it comes with  pathlib , it is missing the useful  exist_ok  option. The backport is intended to offer a newer and superior implementation of  mkdir  which includes this missing option.   Using  os :   import os os.makedirs(path, exist_ok=True)    os.makedirs  as used above recursively creates the directory and does not raise an exception if the directory already exists. It has the optional  exist_ok  argument only if using Python 3.2+, with a default value of  False . This argument does not exist in Python 2.x up to 2.7. As such, there is no need for manual exception handling as with Python 2.7.   Python 2.7+:   Using  pathlib :   If you can, install the current  pathlib  backport named  pathlib2 . Do not install the older unmaintained backport named  pathlib . Next, refer to the Python 3.5+ section above and use it the same.   Using  os :   import os try:      os.makedirs(path) except OSError:     if not os.path.isdir(path):         raise    While a naive solution may first use  os.path.isdir  followed by  os.makedirs , the solution above reverses the order of the two operations. In doing so, it prevents a common race condition having to do with a duplicated attempt at creating the directory, and also disambiguates files from directories.   Note that capturing the exception and using  errno  is of limited usefulness because  OSError: [Errno 17] File exists , i.e.  errno.EEXIST , is raised for both files and directories. It is more reliable simply to check if the directory exists.   Alternative:   mkpath  creates the nested directory, and does nothing if the directory already exists. This works in both Python 2 and 3. Note however that  distutils  has been deprecated, and is scheduled for removal in Python 3.12.   import distutils.dir_util distutils.dir_util.mkpath(path)    Per  Bug 10948 , a severe limitation of this alternative is that it works only once per python process for a given path. In other words, if you use it to create a directory, then delete the directory from inside or outside Python, then use  mkpath  again to recreate the same directory,  mkpath  will simply silently use its invalid cached info of having previously created the directory, and will not actually make the directory again. In contrast,  os.makedirs  doesn't rely on any such cache. This limitation may be okay for some applications.     With regard to the directory's  mode , please refer to the documentation if you care about it.", "score": 1576}}
{"question": "How do I create a directory, and any missing parent directories?", "tags": ["python", "exception", "path", "directory", "operating-system"], "link": "https://stackoverflow.com/questions/273192/how-do-i-create-a-directory-and-any-missing-parent-directories", "answer_count": 27, "answers": {"id": 273227, "body": "On Python \u2265 3.5, use  pathlib.Path.mkdir :   from pathlib import Path Path(\"/my/directory\").mkdir(parents=True, exist_ok=True)    For older versions of Python, I see two answers with good qualities, each with a small flaw, so I will give my take on it:   Try  os.path.exists , and consider  os.makedirs  for the creation.   import os if not os.path.exists(directory):     os.makedirs(directory)    As noted in comments and elsewhere, there's a race condition \u2013 if the directory is created between the  os.path.exists  and the  os.makedirs  calls, the  os.makedirs  will fail with an  OSError . Unfortunately, blanket-catching  OSError  and continuing is not foolproof, as it will ignore a failure to create the directory due to other factors, such as insufficient permissions, full disk, etc.   One option would be to trap the  OSError  and examine the embedded error code (see  Is there a cross-platform way of getting information from Python\u2019s OSError ):   import os, errno  try:     os.makedirs(directory) except OSError as e:     if e.errno != errno.EEXIST:         raise    Alternatively, there could be a second  os.path.exists , but suppose another created the directory after the first check, then removed it before the second one \u2013 we could still be fooled.    Depending on the application, the danger of concurrent operations may be more or less than the danger posed by other factors such as file permissions. The developer would have to know more about the particular application being developed and its expected environment before choosing an implementation.   Modern versions of Python improve this code quite a bit, both by exposing  FileExistsError  (in 3.3+)...   try:     os.makedirs(\"path/to/directory\") except FileExistsError:     # directory already exists     pass    ...and by allowing  a keyword argument to  os.makedirs  called  exist_ok  (in 3.2+).   os.makedirs(\"path/to/directory\", exist_ok=True)  # succeeds even if directory exists.", "score": 7257}}
{"question": "How do I execute a program or call a system command?", "tags": ["python", "shell", "terminal", "subprocess", "command"], "link": "https://stackoverflow.com/questions/89228/how-do-i-execute-a-program-or-call-a-system-command", "answer_count": 66, "answers": {"id": 95246, "body": "Typical implementation:   import subprocess  p = subprocess.Popen('ls', shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT) for line in p.stdout.readlines():     print line, retval = p.wait()    You are free to do what you want with the  stdout  data in the pipe.  In fact, you can simply omit those parameters ( stdout=  and  stderr= ) and it'll behave like  os.system() .", "score": 444}}
{"question": "How do I execute a program or call a system command?", "tags": ["python", "shell", "terminal", "subprocess", "command"], "link": "https://stackoverflow.com/questions/89228/how-do-i-execute-a-program-or-call-a-system-command", "answer_count": 66, "answers": {"id": 92395, "body": "Here is a summary of ways to call external programs, including their advantages and disadvantages:     os.system  passes the command and arguments to your system's shell. This is nice because you can actually run multiple commands at once in this manner and set up pipes and input/output redirection. For example:   os.system(\"some_command < input_file | another_command > output_file\")      However, while this is convenient, you have to manually handle the escaping of shell characters such as spaces, et cetera. On the other hand, this also lets you run commands which are simply shell commands and not actually external programs.     os.popen  will do the same thing as  os.system  except that it gives you a file-like object that you can use to access standard input/output for that process. There are 3 other variants of popen that all handle the i/o slightly differently. If you pass everything as a string, then your command is passed to the shell; if you pass them as a list then you don't need to worry about escaping anything. Example:   print(os.popen(\"ls -l\").read())      subprocess.Popen . This is intended as a replacement for  os.popen , but has the downside of being slightly more complicated by virtue of being so comprehensive. For example, you'd say:   print subprocess.Popen(\"echo Hello World\", shell=True, stdout=subprocess.PIPE).stdout.read()    instead of   print os.popen(\"echo Hello World\").read()    but it is nice to have all of the options there in one unified class instead of 4 different popen functions. See  the documentation .     subprocess.call . This is basically just like the  Popen  class and takes all of the same arguments, but it simply waits until the command completes and gives you the return code. For example:   return_code = subprocess.call(\"echo Hello World\", shell=True)      subprocess.run . Python 3.5+ only. Similar to the above but even more flexible and returns a  CompletedProcess  object when the command finishes executing.     os.fork ,  os.exec ,  os.spawn  are similar to their C language counterparts, but I don't recommend using them directly.       The  subprocess  module should probably be what you use.   Finally, please be aware that for all methods where you pass the final command to be executed by the shell as a string and you are responsible for escaping it.  There are serious security implications  if any part of the string that you pass can not be fully trusted. For example, if a user is entering some/any part of the string. If you are unsure, only use these methods with constants. To give you a hint of the implications consider this code:   print subprocess.Popen(\"echo %s \" % user_input, stdout=PIPE).stdout.read()    and imagine that the user enters something \" my mama didnt love me && rm -rf / \" which could erase the whole filesystem.", "score": 3653}}
{"question": "How do I execute a program or call a system command?", "tags": ["python", "shell", "terminal", "subprocess", "command"], "link": "https://stackoverflow.com/questions/89228/how-do-i-execute-a-program-or-call-a-system-command", "answer_count": 66, "answers": {"id": 89243, "body": "Use  subprocess.run :   import subprocess  subprocess.run([\"ls\", \"-l\"])     Another common way is  os.system  but you shouldn't use it because it is unsafe if any parts of the command come from outside your program or can contain spaces or other special characters, also  subprocess.run  is generally more flexible (you can get the  stdout ,  stderr , the  \"real\" status code , better  error handling , etc.). Even the  documentation for  os.system  recommends using  subprocess  instead.   On Python 3.4 and earlier, use  subprocess.call  instead of  .run :   subprocess.call([\"ls\", \"-l\"])", "score": 5936}}
{"question": "How do I merge two dictionaries in a single expression in Python?", "tags": ["python", "dictionary", "merge"], "link": "https://stackoverflow.com/questions/38987/how-do-i-merge-two-dictionaries-in-a-single-expression-in-python", "answer_count": 45, "answers": {"id": 39437, "body": "An alternative:   z = x.copy() z.update(y)", "score": 762}}
{"question": "How do I merge two dictionaries in a single expression in Python?", "tags": ["python", "dictionary", "merge"], "link": "https://stackoverflow.com/questions/38987/how-do-i-merge-two-dictionaries-in-a-single-expression-in-python", "answer_count": 45, "answers": {"id": 38990, "body": "In your case, you can do:   z = dict(list(x.items()) + list(y.items()))    This will, as you want it, put the final dict in  z , and make the value for key  b  be properly overridden by the second ( y ) dict's value:   >>> x = {'a': 1, 'b': 2} >>> y = {'b': 10, 'c': 11} >>> z = dict(list(x.items()) + list(y.items())) >>> z {'a': 1, 'c': 11, 'b': 10}     If you use Python 2, you can even remove the  list()  calls. To create z:   >>> z = dict(x.items() + y.items()) >>> z {'a': 1, 'c': 11, 'b': 10}    If you use Python version 3.9.0a4 or greater, you can directly use:   >>> x = {'a': 1, 'b': 2} >>> y = {'b': 10, 'c': 11} >>> z = x | y >>> z {'a': 1, 'c': 11, 'b': 10}", "score": 1833}}
{"question": "How do I merge two dictionaries in a single expression in Python?", "tags": ["python", "dictionary", "merge"], "link": "https://stackoverflow.com/questions/38987/how-do-i-merge-two-dictionaries-in-a-single-expression-in-python", "answer_count": 45, "answers": {"id": 26853961, "body": "How can I merge two Python dictionaries in a single expression?   For dictionaries  x  and  y , their shallowly-merged dictionary  z  takes values from  y , replacing those from  x .     In Python 3.9.0 or greater (released 17 October 2020,  PEP-584 ,  discussed here ):   z = x | y      In Python 3.5 or greater:   z = {**x, **y}      In Python 2, (or 3.4 or lower) write a function:   def merge_two_dicts(x, y):     z = x.copy()   # start with keys and values of x     z.update(y)    # modifies z with keys and values of y     return z    and now:   z = merge_two_dicts(x, y)        Explanation   Say you have two dictionaries and you want to merge them into a new dictionary without altering the original dictionaries:   x = {'a': 1, 'b': 2} y = {'b': 3, 'c': 4}    The desired result is to get a new dictionary ( z ) with the values merged, and the second dictionary's values overwriting those from the first.   >>> z {'a': 1, 'b': 3, 'c': 4}    A new syntax for this, proposed in  PEP 448  and  available as of Python 3.5 , is   z = {**x, **y}    And it is indeed a single expression.   Note that we can merge in with literal notation as well:   z = {**x, 'foo': 1, 'bar': 2, **y}    and now:   >>> z {'a': 1, 'b': 3, 'foo': 1, 'bar': 2, 'c': 4}    It is now showing as implemented in the  release schedule for 3.5, PEP 478 , and it has now made its way into the  What's New in Python 3.5  document.   However, since many organizations are still on Python 2, you may wish to do this in a backward-compatible way. The classically Pythonic way, available in Python 2 and Python 3.0-3.4, is to do this as a two-step process:   z = x.copy() z.update(y) # which returns None since it mutates z    In both approaches,  y  will come second and its values will replace  x 's values, thus  b  will point to  3  in our final result.   Not yet on Python 3.5, but want a  single expression   If you are not yet on Python 3.5 or need to write backward-compatible code, and you want this in a  single expression , the most performant while the correct approach is to put it in a function:   def merge_two_dicts(x, y):     \"\"\"Given two dictionaries, merge them into a new dict as a shallow copy.\"\"\"     z = x.copy()     z.update(y)     return z    and then you have a single expression:   z = merge_two_dicts(x, y)    You can also make a function to merge an arbitrary number of dictionaries, from zero to a very large number:   def merge_dicts(*dict_args):     \"\"\"     Given any number of dictionaries, shallow copy and merge into a new dict,     precedence goes to key-value pairs in latter dictionaries.     \"\"\"     result = {}     for dictionary in dict_args:         result.update(dictionary)     return result    This function will work in Python 2 and 3 for all dictionaries. e.g. given dictionaries  a  to  g :   z = merge_dicts(a, b, c, d, e, f, g)     and key-value pairs in  g  will take precedence over dictionaries  a  to  f , and so on.   Critiques of Other Answers   Don't use what you see in the formerly accepted answer:   z = dict(x.items() + y.items())    In Python 2, you create two lists in memory for each dict, create a third list in memory with length equal to the length of the first two put together, and then discard all three lists to create the dict.  In Python 3, this will fail  because you're adding two  dict_items  objects together, not two lists -   >>> c = dict(a.items() + b.items()) Traceback (most recent call last):   File \" \", line 1, in   TypeError: unsupported operand type(s) for +: 'dict_items' and 'dict_items'    and you would have to explicitly create them as lists, e.g.  z = dict(list(x.items()) + list(y.items())) . This is a waste of resources and computation power.   Similarly, taking the union of  items()  in Python 3 ( viewitems()  in Python 2.7) will also fail when values are unhashable objects (like lists, for example). Even if your values are hashable,  since sets are semantically unordered, the behavior is undefined in regards to precedence. So don't do this:   >>> c = dict(a.items() | b.items())    This example demonstrates what happens when values are unhashable:   >>> x = {'a': []} >>> y = {'b': []} >>> dict(x.items() | y.items()) Traceback (most recent call last):   File \" \", line 1, in   TypeError: unhashable type: 'list'    Here's an example where  y  should have precedence, but instead the value from  x  is retained due to the arbitrary order of sets:   >>> x = {'a': 2} >>> y = {'a': 1} >>> dict(x.items() | y.items()) {'a': 2}    Another hack you should not use:   z = dict(x, **y)    This uses the  dict  constructor and is very fast and memory-efficient (even slightly more so than our two-step process) but unless you know precisely what is happening here (that is, the second dict is being passed as keyword arguments to the dict constructor), it's difficult to read, it's not the intended usage, and so it is not Pythonic.   Here's an example of the usage being  remediated in django .   Dictionaries are intended to take hashable keys (e.g.  frozenset s or tuples), but  this method fails in Python 3 when keys are not strings.   >>> c = dict(a, **b) Traceback (most recent call last):   File \" \", line 1, in   TypeError: keyword arguments must be strings    From the  mailing list , Guido van Rossum, the creator of the language, wrote:     I am fine with declaring dict({}, **{1:3}) illegal, since after all it is abuse of the ** mechanism.     and     Apparently dict(x, **y) is going around as \"cool hack\" for \"call x.update(y) and return x\". Personally, I find it more despicable than cool.     It is my understanding (as well as the understanding of the  creator of the language ) that the intended usage for  dict(**y)  is for creating dictionaries for readability purposes, e.g.:   dict(a=1, b=10, c=11)    instead of   {'a': 1, 'b': 10, 'c': 11}    Response to comments     Despite what Guido says,  dict(x, **y)  is in line with the dict specification, which btw. works for both Python 2 and 3. The fact that this only works for string keys is a direct consequence of how keyword parameters work and not a short-coming of dict. Nor is using the ** operator in this place an abuse of the mechanism, in fact, ** was designed precisely to pass dictionaries as keywords.     Again, it doesn't work for 3 when keys are not strings. The implicit calling contract is that namespaces take ordinary dictionaries, while users must only pass keyword arguments that are strings. All other callables enforced it.  dict  broke this consistency in Python 2:   >>> foo(**{('a', 'b'): None}) Traceback (most recent call last):   File \" \", line 1, in   TypeError: foo() keywords must be strings >>> dict(**{('a', 'b'): None}) {('a', 'b'): None}    This inconsistency was bad given other implementations of Python (PyPy, Jython, IronPython). Thus it was fixed in Python 3, as this usage could be a breaking change.   I submit to you that it is malicious incompetence to intentionally write code that only works in one version of a language or that only works given certain arbitrary constraints.   More comments:     dict(x.items() + y.items())  is still the most readable solution for Python 2. Readability counts.     My response:  merge_two_dicts(x, y)  actually seems much clearer to me, if we're actually concerned about readability. And it is not forward compatible, as Python 2 is increasingly deprecated.     {**x, **y}  does not seem to handle nested dictionaries. the contents of nested keys are simply overwritten, not merged [...] I ended up being burnt by these answers that do not merge recursively and I was surprised no one mentioned it. In my interpretation of the word \"merging\" these answers describe \"updating one dict with another\", and not merging.     Yes. I must refer you back to the question, which is asking for a  shallow  merge of  two  dictionaries, with the first's values being overwritten by the second's - in a single expression.   Assuming two dictionaries of dictionaries, one might recursively merge them in a single function, but you should be careful not to modify the dictionaries from either source, and the surest way to avoid that is to make a copy when assigning values. As keys must be hashable and are usually therefore immutable, it is pointless to copy them:   from copy import deepcopy  def dict_of_dicts_merge(x, y):     z = {}     overlapping_keys = x.keys() & y.keys()     for key in overlapping_keys:         z[key] = dict_of_dicts_merge(x[key], y[key])     for key in x.keys() - overlapping_keys:         z[key] = deepcopy(x[key])     for key in y.keys() - overlapping_keys:         z[key] = deepcopy(y[key])     return z    Usage:   >>> x = {'a':{1:{}}, 'b': {2:{}}} >>> y = {'b':{10:{}}, 'c': {11:{}}} >>> dict_of_dicts_merge(x, y) {'b': {2: {}, 10: {}}, 'a': {1: {}}, 'c': {11: {}}}    Coming up with contingencies for other value types is far beyond the scope of this question, so I will point you at  my answer to the canonical question on a \"Dictionaries of dictionaries merge\" .   Less Performant But Correct Ad-hocs   These approaches are less performant, but they will provide correct behavior. They will be  much less  performant than  copy  and  update  or the new unpacking because they iterate through each key-value pair at a higher level of abstraction, but they  do  respect the order of precedence (latter dictionaries have precedence)   You can also chain the dictionaries manually inside a  dict comprehension :   {k: v for d in dicts for k, v in d.items()} # iteritems in Python 2.7    or in Python 2.6 (and perhaps as early as 2.4 when generator expressions were introduced):   dict((k, v) for d in dicts for k, v in d.items()) # iteritems in Python 2    itertools.chain  will chain the iterators over the key-value pairs in the correct order:   from itertools import chain z = dict(chain(x.items(), y.items())) # iteritems in Python 2    Performance Analysis   I'm only going to do the performance analysis of the usages known to behave correctly. (Self-contained so you can copy and paste yourself.)   from timeit import repeat from itertools import chain  x = dict.fromkeys('abcdefg') y = dict.fromkeys('efghijk')  def merge_two_dicts(x, y):     z = x.copy()     z.update(y)     return z  min(repeat(lambda: {**x, **y})) min(repeat(lambda: merge_two_dicts(x, y))) min(repeat(lambda: {k: v for d in (x, y) for k, v in d.items()})) min(repeat(lambda: dict(chain(x.items(), y.items())))) min(repeat(lambda: dict(item for d in (x, y) for item in d.items())))    In Python 3.8.1, NixOS:   >>> min(repeat(lambda: {**x, **y})) 1.0804965235292912 >>> min(repeat(lambda: merge_two_dicts(x, y))) 1.636518670246005 >>> min(repeat(lambda: {k: v for d in (x, y) for k, v in d.items()})) 3.1779992282390594 >>> min(repeat(lambda: dict(chain(x.items(), y.items())))) 2.740647904574871 >>> min(repeat(lambda: dict(item for d in (x, y) for item in d.items()))) 4.266070580109954    $ uname -a Linux nixos 4.19.113 #1-NixOS SMP Wed Mar 25 07:06:15 UTC 2020 x86_64 GNU/Linux    Resources on Dictionaries     My explanation of Python's  dictionary implementation , updated for 3.6.   Answer on how to add new keys to a dictionary   Mapping two lists into a dictionary   The official Python docs on dictionaries   The Dictionary Even Mightier  - talk by Brandon Rhodes at Pycon 2017   Modern Python Dictionaries, A Confluence of Great Ideas  - talk by Raymond Hettinger at Pycon 2017", "score": 9324}}
{"question": "How do I check whether a file exists without exceptions?", "tags": ["python", "file", "file-exists"], "link": "https://stackoverflow.com/questions/82831/how-do-i-check-whether-a-file-exists-without-exceptions", "answer_count": 41, "answers": {"id": 84173, "body": "Unlike  isfile() ,  exists()  will return  True  for directories. So depending on if you want only plain files or also directories, you'll use  isfile()  or  exists() . Here is some simple  REPL  output:   >>> os.path.isfile(\"/etc/password.txt\") True >>> os.path.isfile(\"/etc\") False >>> os.path.isfile(\"/does/not/exist\") False >>> os.path.exists(\"/etc/password.txt\") True >>> os.path.exists(\"/etc\") True >>> os.path.exists(\"/does/not/exist\") False", "score": 1241}}
{"question": "How do I check whether a file exists without exceptions?", "tags": ["python", "file", "file-exists"], "link": "https://stackoverflow.com/questions/82831/how-do-i-check-whether-a-file-exists-without-exceptions", "answer_count": 41, "answers": {"id": 82846, "body": "Use  os.path.exists  to check both files and directories:   import os.path os.path.exists(file_path)    Use  os.path.isfile  to check only files (note: follows  symbolic links ):   os.path.isfile(file_path)", "score": 2671}}
{"question": "How do I check whether a file exists without exceptions?", "tags": ["python", "file", "file-exists"], "link": "https://stackoverflow.com/questions/82831/how-do-i-check-whether-a-file-exists-without-exceptions", "answer_count": 41, "answers": {"id": 82852, "body": "If the reason you're checking is so you can do something like  if file_exists: open_it() , it's safer to use a  try  around the attempt to open it. Checking and then opening risks the file being deleted or moved or something between when you check and when you try to open it.   If you're not planning to open the file immediately, you can use  os.path.isfile  if you need to be sure it's a file.     Return  True  if path is an existing regular file. This follows symbolic links, so both  islink()  and  isfile()  can be true for the same path.     import os.path os.path.isfile(fname)    pathlib   Starting with Python 3.4, the  pathlib  module  offers an object-oriented approach (backported to  pathlib2  in Python 2.7):   from pathlib import Path  my_file = Path(\"/path/to/file\") if my_file.is_file():     # file exists    To check a directory, do:   if my_file.is_dir():     # directory exists    To check whether a  Path  object exists independently of whether is it a file or directory, use  exists() :   if my_file.exists():     # path exists    You can also use  resolve(strict=True)  in a  try  block:   try:     my_abs_path = my_file.resolve(strict=True) except FileNotFoundError:     # doesn't exist else:     # exists", "score": 6742}}
{"question": "What are metaclasses in Python?", "tags": ["python", "oop", "metaclass", "python-class", "python-datamodel"], "link": "https://stackoverflow.com/questions/100003/what-are-metaclasses-in-python", "answer_count": 26, "answers": {"id": 100037, "body": "Note, this answer is for Python 2.x as it was written in 2008, metaclasses are slightly different in 3.x.   Metaclasses are the secret sauce that make 'class' work. The default metaclass for a new style object is called 'type'.   class type(object)   |  type(object) -> the object's type   |  type(name, bases, dict) -> a new type    Metaclasses take 3 args. ' name ', ' bases ' and ' dict '   Here is where the secret starts. Look for where name, bases and the dict come from in this example class definition.   class ThisIsTheName(Bases, Are, Here):     All_the_code_here     def doesIs(create, a):         dict    Lets define a metaclass that will demonstrate how ' class: ' calls it.   def test_metaclass(name, bases, dict):     print 'The Class Name is', name     print 'The Class Bases are', bases     print 'The dict has', len(dict), 'elems, the keys are', dict.keys()      return \"yellow\"  class TestName(object, None, int, 1):     __metaclass__ = test_metaclass     foo = 1     def baz(self, arr):         pass  print 'TestName = ', repr(TestName)  # output =>  The Class Name is TestName The Class Bases are ( , None,  , 1) The dict has 4 elems, the keys are ['baz', '__module__', 'foo', '__metaclass__'] TestName =  'yellow'    And now, an example that actually means something, this will automatically make the variables in the list \"attributes\" set on the class, and set to None.   def init_attributes(name, bases, dict):     if 'attributes' in dict:         for attr in dict['attributes']:             dict[attr] = None      return type(name, bases, dict)  class Initialised(object):     __metaclass__ = init_attributes     attributes = ['foo', 'bar', 'baz']  print 'foo =>', Initialised.foo # output=> foo => None    Note that the magic behaviour that  Initialised  gains by having the metaclass  init_attributes  is not passed onto a subclass of  Initialised .   Here is an even more concrete example, showing how you can subclass 'type' to make a metaclass that performs an action when the class is created. This is quite tricky:   class MetaSingleton(type):     instance = None     def __call__(cls, *args, **kw):         if cls.instance is None:             cls.instance = super(MetaSingleton, cls).__call__(*args, **kw)         return cls.instance  class Foo(object):     __metaclass__ = MetaSingleton  a = Foo() b = Foo() assert a is b", "score": 492}}
{"question": "What are metaclasses in Python?", "tags": ["python", "oop", "metaclass", "python-class", "python-datamodel"], "link": "https://stackoverflow.com/questions/100003/what-are-metaclasses-in-python", "answer_count": 26, "answers": {"id": 100146, "body": "A metaclass is the class of a class. A class defines how an instance of the class (i.e. an object) behaves while a metaclass defines how a class behaves. A class is an instance of a metaclass.   While in Python you can use arbitrary callables for metaclasses (like  Jerub  shows), the better approach is to make it an actual class itself.  type  is the usual metaclass in Python.  type  is itself a class, and it is its own type. You won't be able to recreate something like  type  purely in Python, but Python cheats a little. To create your own metaclass in Python you really just want to subclass  type .   A metaclass is most commonly used as a class-factory. When you create an object by calling the class, Python creates a new class (when it executes the 'class' statement) by calling the metaclass. Combined with the normal  __init__  and  __new__  methods, metaclasses therefore allow you to do 'extra things' when creating a class, like registering the new class with some registry or replace the class with something else entirely.   When the  class  statement is executed, Python first executes the body of the  class  statement as a normal block of code. The resulting namespace (a dict) holds the attributes of the class-to-be. The metaclass is determined by looking at the baseclasses of the class-to-be (metaclasses are inherited), at the  __metaclass__  attribute of the class-to-be (if any) or the  __metaclass__  global variable. The metaclass is then called with the name, bases and attributes of the class to instantiate it.   However, metaclasses actually define the  type  of a class, not just a factory for it, so you can do much more with them. You can, for instance, define normal methods on the metaclass. These metaclass-methods are like classmethods in that they can be called on the class without an instance, but they are also not like classmethods in that they cannot be called on an instance of the class.  type.__subclasses__()  is an example of a method on the  type  metaclass. You can also define the normal 'magic' methods, like  __add__ ,  __iter__  and  __getattr__ , to implement or change how the class behaves.   Here's an aggregated example of the bits and pieces:   def make_hook(f):     \"\"\"Decorator to turn 'foo' method into '__foo__'\"\"\"     f.is_hook = 1     return f  class MyType(type):     def __new__(mcls, name, bases, attrs):          if name.startswith('None'):             return None          # Go over attributes and see if they should be renamed.         newattrs = {}         for attrname, attrvalue in attrs.iteritems():             if getattr(attrvalue, 'is_hook', 0):                 newattrs['__%s__' % attrname] = attrvalue             else:                 newattrs[attrname] = attrvalue          return super(MyType, mcls).__new__(mcls, name, bases, newattrs)      def __init__(self, name, bases, attrs):         super(MyType, self).__init__(name, bases, attrs)          # classregistry.register(self, self.interfaces)         print \"Would register class %s now.\" % self      def __add__(self, other):         class AutoClass(self, other):             pass         return AutoClass         # Alternatively, to autogenerate the classname as well as the class:         # return type(self.__name__ + other.__name__, (self, other), {})      def unregister(self):         # classregistry.unregister(self)         print \"Would unregister class %s now.\" % self  class MyObject:     __metaclass__ = MyType   class NoneSample(MyObject):     pass  # Will print \"NoneType None\" print type(NoneSample), repr(NoneSample)  class Example(MyObject):     def __init__(self, value):         self.value = value     @make_hook     def add(self, other):         return self.__class__(self.value + other.value)  # Will unregister the class Example.unregister()  inst = Example(10) # Will fail with an AttributeError #inst.unregister()  print inst + inst class Sibling(MyObject):     pass  ExampleSibling = Example + Sibling # ExampleSibling is now a subclass of both Example and Sibling (with no # content of its own) although it will believe it's called 'AutoClass' print ExampleSibling print ExampleSibling.__mro__", "score": 3438}}
{"question": "What are metaclasses in Python?", "tags": ["python", "oop", "metaclass", "python-class", "python-datamodel"], "link": "https://stackoverflow.com/questions/100003/what-are-metaclasses-in-python", "answer_count": 26, "answers": {"id": 6581949, "body": "Classes as objects   Prior to delving into metaclasses, a solid grasp of Python classes is beneficial. Python holds a particularly distinctive concept of classes, a notion it adopts from the Smalltalk language.   In most languages, classes are just pieces of code that describe how to produce an object. That is somewhat true in Python too:   >>> class ObjectCreator(object): ...     pass  >>> my_object = ObjectCreator() >>> print(my_object)     <__main__.ObjectCreator object at 0x8974f2c>    But classes are more than that in Python.  Classes are objects too.   Yes, objects.   When a Python script runs, every line of code is executed from top to bottom. When the Python interpreter encounters the  class  keyword, Python creates an  object  out of the \"description\" of the class that follows. Thus, the following instruction   >>> class ObjectCreator(object): ...     pass    ...creates an  object  with the name  ObjectCreator !   This object (the class) is itself capable of creating objects (called  instances ).   But still, it's an object. Therefore, like all objects:     you can assign it to a variable 1   JustAnotherVariable = ObjectCreator      you can attach attributes to it  ObjectCreator.class_attribute = 'foo'      you can pass it as a function parameter  print(ObjectCreator)        1  Note that merely assigning it to another variable doesn't change the class's  __name__ , i.e.,   >>> print(JustAnotherVariable)        >>> print(JustAnotherVariable())     <__main__.ObjectCreator object at 0x8997b4c>      Creating classes dynamically   Since classes are objects, you can create them on the fly, like any object.   First, you can create a class in a function using  class :   >>> def choose_class(name): ...     if name == 'foo': ...         class Foo(object): ...             pass ...         return Foo # return the class, not an instance ...     else: ...         class Bar(object): ...             pass ...         return Bar  >>> MyClass = choose_class('foo')  >>> print(MyClass) # the function returns a class, not an instance        >>> print(MyClass()) # you can create an object from this class     <__main__.Foo object at 0x89c6d4c>    But it's not so dynamic, since you still have to write the whole class yourself.   Since classes are objects, they must be generated by something.   When you use the  class  keyword, Python creates this object automatically. But as with most things in Python, it gives you a way to do it manually.   Remember the function  type ? The good old function that lets you know what type an object is:   >>> print(type(1))        >>> print(type(\"1\"))        >>> print(type(ObjectCreator))        >>> print(type(ObjectCreator()))          Well,  type  also has a completely different ability: it can create classes on the fly.  type  can take the description of a class as parameters, and return a class.   (I  know, it's silly that the same function can have two completely different uses according to the parameters you pass to it. It's an issue due to backward compatibility in Python)   type  works this way:   type(name, bases, attrs)    Where:     name : name of the class   bases : tuple of the parent class (for inheritance, can be empty)   attrs : dictionary containing attributes names and values     e.g.:   >>> class MyShinyClass(object): ...     pass    can be created manually this way:   >>> MyShinyClass = type('MyShinyClass', (), {}) # returns a class object >>> print(MyShinyClass)        >>> print(MyShinyClass()) # create an instance with the class     <__main__.MyShinyClass object at 0x8997cec>    You'll notice that we use  MyShinyClass  as the name of the class and as the variable to hold the class reference. They can be different, but there is no reason to complicate things.   type  accepts a dictionary to define the attributes of the class. So:   >>> class Foo(object): ...     bar = True    Can be translated to:   >>> Foo = type('Foo', (), {'bar':True})    And used as a normal class:   >>> print(Foo)        >>> print(Foo.bar)     True  >>> f = Foo() >>> print(f)     <__main__.Foo object at 0x8a9b84c>  >>> print(f.bar)     True    And of course, you can inherit from it, so:   >>> class FooChild(Foo): ...     pass    would be:   >>> FooChild = type('FooChild', (Foo,), {}) >>> print(FooChild)        >>> print(FooChild.bar) # bar is inherited from Foo     True    Eventually, you'll want to add methods to your class. Just define a function with the proper signature and assign it as an attribute.   >>> def echo_bar(self): ...     print(self.bar)  >>> FooChild = type('FooChild', (Foo,), {'echo_bar': echo_bar})  >>> hasattr(Foo, 'echo_bar')     False  >>> hasattr(FooChild, 'echo_bar')     True  >>> my_foo = FooChild() >>> my_foo.echo_bar()     True    And you can add even more methods after you dynamically create the class, just like adding methods to a normally created class object.   >>> def echo_bar_more(self): ...     print('yet another method')  >>> FooChild.echo_bar_more = echo_bar_more >>> hasattr(FooChild, 'echo_bar_more')     True    You see where we are going: in Python, classes are objects, and you can create a class on the fly, dynamically.   This is what Python does when you use the keyword  class , and it does so by using a metaclass.   What are metaclasses (finally)   Metaclasses are the 'stuff' that creates classes.   You define classes in order to create objects, right?   But we learned that Python classes are objects.   Well, metaclasses are what create these objects. They are the classes' classes, you can picture them this way:   MyClass = MetaClass() my_object = MyClass()    You've seen that  type  lets you do something like this:   MyClass = type('MyClass', (), {})    It's because the function  type  is in fact a metaclass.  type  is the metaclass Python uses to create all classes behind the scenes.   Now you wonder \"why the heck is it written in lowercase, and not  Type ?\"   Well, I guess it's a matter of consistency with  str , the class that creates strings objects, and  int  the class that creates integer objects.  type  is just the class that creates class objects.   You see that by checking the  __class__  attribute.   Everything, and I mean everything, is an object in Python. That includes integers, strings, functions and classes. All of them are objects. And all of them have been created from a class:   >>> age = 35 >>> age.__class__        >>> name = 'bob' >>> name.__class__        >>> def foo(): pass >>> foo.__class__        >>> class Bar(object): pass >>> b = Bar() >>> b.__class__          Now, what is the  __class__  of any  __class__  ?   >>> age.__class__.__class__        >>> name.__class__.__class__        >>> foo.__class__.__class__        >>> b.__class__.__class__          So, a metaclass is just the stuff that creates class objects.   You can call it a 'class factory' if you wish.   type  is the built-in metaclass Python uses, but of course, you can create your own metaclass.   The  __metaclass__  attribute   In Python 2, you can add a  __metaclass__  attribute when you write a class (see next section for the Python 3 syntax):   class Foo(object):     __metaclass__ = something...     [...]    If you do so, Python will use the metaclass to create the class  Foo .   Careful, it's tricky.   You write  class Foo(object)  first, but the class object  Foo  is not created in memory yet.   Python will look for  __metaclass__  in the class definition. If it finds it, it will use it to create the class object  Foo . If it doesn't, it will use  type  to create the class.   Read that several times.   When you do:   class Foo(Bar):     pass    Python does the following:   Is there a  __metaclass__  attribute in  Foo ?   If yes, create in-memory a class object (I said a class object, stay with me here), with the name  Foo  by using what is in  __metaclass__ .   If Python can't find  __metaclass__ , it will look for a  __metaclass__  at the MODULE level, and try to do the same (but only for classes that don't inherit anything, basically old-style classes).   Then if it can't find any  __metaclass__  at all, it will use the  Bar 's (the first parent) own metaclass (which might be the default  type ) to create the class object.   Be careful here that the  __metaclass__  attribute will not be inherited, the metaclass of the parent ( Bar.__class__ ) will be. If  Bar  used a  __metaclass__  attribute that created  Bar  with  type()  (and not  type.__new__() ), the subclasses will not inherit that behavior.   Now the big question is, what can you put in  __metaclass__ ?   The answer is something that can create a class.   And what can create a class?  type , or anything that subclasses or uses it.   Metaclasses in Python 3   The syntax to set the metaclass has been changed in Python 3:   class Foo(object, metaclass=something):     ...    i.e. the  __metaclass__  attribute is no longer used, in favor of a keyword argument in the list of base classes.   The behavior of metaclasses however stays  largely the same .   One thing added to metaclasses in Python 3 is that you can also pass attributes as keyword-arguments into a metaclass, like so:   class Foo(object, metaclass=something, kwarg1=value1, kwarg2=value2):     ...    Read the section below for how Python handles this.   Custom metaclasses   The main purpose of a metaclass is to change the class automatically, when it's created.   You usually do this for APIs, where you want to create classes matching the current context.   Imagine a stupid example, where you decide that all classes in your module should have their attributes written in uppercase. There are several ways to do this, but one way is to set  __metaclass__  at the module level.   This way, all classes of this module will be created using this metaclass, and we just have to tell the metaclass to turn all attributes to uppercase.   Luckily,  __metaclass__  can actually be any callable, it doesn't need to be a formal class (I know, something with 'class' in its name doesn't need to be a class, go figure... but it's helpful).   So we will start with a simple example, by using a function.   # the metaclass will automatically get passed the same argument # that you usually pass to `type` def upper_attr(future_class_name, future_class_parents, future_class_attrs):     \"\"\"       Return a class object, with the list of its attribute turned       into uppercase.     \"\"\"     # pick up any attribute that doesn't start with '__' and uppercase it     uppercase_attrs = {         attr if attr.startswith(\"__\") else attr.upper(): v         for attr, v in future_class_attrs.items()     }      # let `type` do the class creation     return type(future_class_name, future_class_parents, uppercase_attrs)  __metaclass__ = upper_attr # this will affect all classes in the module  class Foo(): # global __metaclass__ won't work with \"object\" though     # but we can define __metaclass__ here instead to affect only this class     # and this will work with \"object\" children     bar = 'bip'    Let's check:   >>> hasattr(Foo, 'bar')     False  >>> hasattr(Foo, 'BAR')     True  >>> Foo.BAR     'bip'    Now, let's do exactly the same, but using a real class for a metaclass:   # remember that `type` is actually a class like `str` and `int` # so you can inherit from it class UpperAttrMetaclass(type):     # __new__ is the method called before __init__     # it's the method that creates the object and returns it     # while __init__ just initializes the object passed as parameter     # you rarely use __new__, except when you want to control how the object     # is created.     # here the created object is the class, and we want to customize it     # so we override __new__     # you can do some stuff in __init__ too if you wish     # some advanced use involves overriding __call__ as well, but we won't     # see this     def __new__(         upperattr_metaclass,         future_class_name,         future_class_parents,         future_class_attrs     ):         uppercase_attrs = {             attr if attr.startswith(\"__\") else attr.upper(): v             for attr, v in future_class_attrs.items()         }         return type(future_class_name, future_class_parents, uppercase_attrs)    Let's rewrite the above, but with shorter and more realistic variable names now that we know what they mean:   class UpperAttrMetaclass(type):     def __new__(cls, clsname, bases, attrs):         uppercase_attrs = {             attr if attr.startswith(\"__\") else attr.upper(): v             for attr, v in attrs.items()         }         return type(clsname, bases, uppercase_attrs)    You may have noticed the extra argument  cls . There is nothing special about it:  __new__  always receives the class it's defined in, as the first parameter. Just like you have  self  for ordinary methods which receive the instance as the first parameter, or the defining class for class methods.   But this is not proper OOP. We are calling  type  directly and we aren't overriding or calling the parent's  __new__ . Let's do that instead:   class UpperAttrMetaclass(type):     def __new__(cls, clsname, bases, attrs):         uppercase_attrs = {             attr if attr.startswith(\"__\") else attr.upper(): v             for attr, v in attrs.items()         }         return type.__new__(cls, clsname, bases, uppercase_attrs)    We can make it even cleaner by using  super , which will ease inheritance (because yes, you can have metaclasses, inheriting from metaclasses, inheriting from type):   class UpperAttrMetaclass(type):     def __new__(cls, clsname, bases, attrs):         uppercase_attrs = {             attr if attr.startswith(\"__\") else attr.upper(): v             for attr, v in attrs.items()         }          # Python 2 requires passing arguments to super:         return super(UpperAttrMetaclass, cls).__new__(             cls, clsname, bases, uppercase_attrs)          # Python 3 can use no-arg super() which infers them:         return super().__new__(cls, clsname, bases, uppercase_attrs)    Oh, and in Python 3 if you do this call with keyword arguments, like this:   class Foo(object, metaclass=MyMetaclass, kwarg1=value1):     ...    It translates to this in the metaclass to use it:   class MyMetaclass(type):     def __new__(cls, clsname, bases, dct, kwargs1=default):         ...    That's it. There is really nothing more about metaclasses.   The reason behind the complexity of the code using metaclasses is not because of metaclasses, it's because you usually use metaclasses to do twisted stuff relying on introspection, manipulating inheritance, vars such as  __dict__ , etc.   Indeed, metaclasses are especially useful to do black magic, and therefore complicated stuff. But by themselves, they are simple:     intercept a class creation   modify the class   return the modified class     Why would you use metaclasses classes instead of functions?   Since  __metaclass__  can accept any callable, why would you use a class since it's obviously more complicated?   There are several reasons to do so:     The intention is clear. When you read  UpperAttrMetaclass(type) , you know what's going to follow   You can use OOP. Metaclass can inherit from metaclass, override parent methods. Metaclasses can even use metaclasses.   Subclasses of a class will be instances of its metaclass if you specified a metaclass-class, but not with a metaclass-function.   You can structure your code better. You never use metaclasses for something as trivial as the above example. It's usually for something complicated. Having the ability to make several methods and group them in one class is very useful to make the code easier to read.   You can hook on  __new__ ,  __init__  and  __call__ . Which will allow you to do different stuff, Even if usually you can do it all in  __new__ , some people are just more comfortable using  __init__ .   These are called metaclasses, damn it! It must mean something!     Why would you use metaclasses?   Now the big question. Why would you use some obscure error-prone feature?   Well, usually you don't:     Metaclasses are deeper magic that 99% of users should never worry about it. If you wonder whether you need them, you don't (the people who actually need them know with certainty that they need them, and don't need an explanation about why).     Python Guru Tim Peters   The main use case for a metaclass is creating an API. A typical example of this is the Django ORM. It allows you to define something like this:   class Person(models.Model):     name = models.CharField(max_length=30)     age = models.IntegerField()    But if you do this:   person = Person(name='bob', age='35') print(person.age)    It won't return an  IntegerField  object. It will return an  int , and can even take it directly from the database.   This is possible because  models.Model  defines  __metaclass__  and it uses some magic that will turn the  Person  you just defined with simple statements into a complex hook to a database field.   Django makes something complex look simple by exposing a simple API and using metaclasses, recreating code from this API to do the real job behind the scenes.   The last word   First, you know that classes are objects that can create instances.   Well, in fact, classes are themselves instances. Of metaclasses.   >>> class Foo(object): pass >>> id(Foo)     142630324    Everything is an object in Python, and they are all either instance of classes or instances of metaclasses.   Except for  type .   type  is actually its own metaclass. This is not something you could reproduce in pure Python, and is done by cheating a little bit at the implementation level.   Secondly, metaclasses are complicated. You may not want to use them for very simple class alterations. You can change classes by using two different techniques:     monkey patching   class decorators     99% of the time you need class alteration, you are better off using these.   But 98% of the time, you don't need class alteration at all.", "score": 9265}}
{"question": "Does Python have a ternary conditional operator?", "tags": ["python", "operators", "conditional-operator"], "link": "https://stackoverflow.com/questions/394809/does-python-have-a-ternary-conditional-operator", "answer_count": 33, "answers": {"id": 394887, "body": "For versions prior to 2.5, there's the trick:   [expression] and [on_true] or [on_false]    It can give wrong results when  on_true  has a false Boolean value. 1   Although it does have the benefit of evaluating expressions left to right, which is clearer in my opinion.   1.  Is there an equivalent of C\u2019s \u201d?:\u201d ternary operator?", "score": 445}}
{"question": "Does Python have a ternary conditional operator?", "tags": ["python", "operators", "conditional-operator"], "link": "https://stackoverflow.com/questions/394809/does-python-have-a-ternary-conditional-operator", "answer_count": 33, "answers": {"id": 470376, "body": "You can index into a tuple:   (falseValue, trueValue)[test]    test  needs to return  True  or  False .  It might be safer to always implement it as:   (falseValue, trueValue)[test == True]    or you can use the built-in  bool()  to assure a  Boolean  value:   (falseValue, trueValue)[bool( )]", "score": 1020}}
{"question": "Does Python have a ternary conditional operator?", "tags": ["python", "operators", "conditional-operator"], "link": "https://stackoverflow.com/questions/394809/does-python-have-a-ternary-conditional-operator", "answer_count": 33, "answers": {"id": 394814, "body": "Yes, it was  added  in version 2.5. The expression syntax is:   a if condition else b    First  condition  is evaluated, then exactly one of either  a  or  b  is evaluated and returned based on the  Boolean  value of  condition . If  condition  evaluates to  True , then  a  is evaluated and returned but  b  is ignored, or else when  b  is evaluated and returned but  a  is ignored.   This allows short-circuiting because when  condition  is true only  a  is evaluated and  b  is not evaluated at all, but when  condition  is false only  b  is evaluated and  a  is not evaluated at all.   For example:   >>> 'true' if True else 'false' 'true' >>> 'true' if False else 'false' 'false'    Note that conditionals are an  expression , not a  statement . This means you can't use  statements  such as  pass , or assignments with  =  (or \"augmented\" assignments like  += ), within a conditional  expression :   >>> pass if False else pass   File \" \", line 1     pass if False else pass          ^ SyntaxError: invalid syntax  >>> # Python parses this as `x = (1 if False else y) = 2` >>> # The `(1 if False else x)` part is actually valid, but >>> # it can't be on the left-hand side of `=`. >>> x = 1 if False else y = 2   File \" \", line 1 SyntaxError: cannot assign to conditional expression  >>> # If we parenthesize it instead... >>> (x = 1) if False else (y = 2)   File \" \", line 1     (x = 1) if False else (y = 2)        ^ SyntaxError: invalid syntax    (In 3.8 and above, the  :=  \"walrus\" operator allows simple assignment of values  as an expression , which is then compatible with this syntax. But please don't write code like that; it will quickly become very difficult to understand.)   Similarly, because it is an expression, the  else  part is  mandatory :   # Invalid syntax: we didn't specify what the value should be if the  # condition isn't met. It doesn't matter if we can verify that # ahead of time. a if True    You can, however, use conditional expressions to assign a variable like so:   x = a if True else b    Or for example to return a value:   # Of course we should just use the standard library `max`; # this is just for demonstration purposes. def my_max(a, b):     return a if a > b else b    Think of the conditional expression as switching between two values. We can use it when we are in a 'one value or another' situation, where we will  do the same thing  with the result, regardless of whether the condition is met. We use the expression to compute the value, and then do something with it. If you need to  do something different  depending on the condition, then use a normal  if   statement  instead.     Keep in mind that it's frowned upon by some Pythonistas for several reasons:     The order of the arguments is different from those of the classic  condition ? a : b  ternary operator from many other languages (such as  C ,  C++ ,  Go ,  Perl ,  Ruby ,  Java ,  JavaScript , etc.), which may lead to bugs when people unfamiliar with Python's \"surprising\" behaviour use it (they may reverse the argument order).   Some find it \"unwieldy\", since it goes contrary to the normal flow of thought (thinking of the condition first and then the effects).   Stylistic reasons. (Although the 'inline  if ' can be  really  useful, and make your script more concise, it really does complicate your code)     If you're having trouble remembering the order, then remember that when read aloud, you (almost) say what you mean. For example,  x = 4 if b > 8 else 9  is read aloud as  x will be 4 if b is greater than 8 otherwise 9 .   Official documentation:     Conditional expressions   Is there an equivalent of C\u2019s \u201d?:\u201d ternary operator?", "score": 9305}}
{"question": "What does if __name__ == &quot;__main__&quot;: do?", "tags": ["python", "namespaces", "program-entry-point", "python-module", "idioms"], "link": "https://stackoverflow.com/questions/419163/what-does-if-name-main-do", "answer_count": 40, "answers": {"id": 419986, "body": "Create the following two files:   # a.py  import b    # b.py  print(\"__name__ equals \" + __name__)  if __name__ == '__main__':     print(\"if-statement was executed\")    Now run each file individually.     Running  python a.py :   $ python a.py __name__ equals b    When  a.py  is executed, it imports the module  b . This causes all the code inside  b  to run. Python sets  globals()['__name__']  in the  b  module to the module's name,  b .     Running  python b.py :   $ python b.py __name__ equals __main__ if-statement was executed    When only the file  b.py  is executed, Python sets  globals()['__name__']  in this file to  \"__main__\" . Therefore, the  if  statement evaluates to  True  this time.", "score": 896}}
{"question": "What does if __name__ == &quot;__main__&quot;: do?", "tags": ["python", "namespaces", "program-entry-point", "python-module", "idioms"], "link": "https://stackoverflow.com/questions/419163/what-does-if-name-main-do", "answer_count": 40, "answers": {"id": 419189, "body": "When your script is run by passing it as a command to the Python interpreter,   python myscript.py    all of the code that is at indentation level 0 gets executed.  Functions and classes that are defined are, well, defined, but none of their code gets run.  Unlike other languages, there's no  main()  function that gets run automatically - the  main()  function is implicitly all the code at the top level.   In this case, the top-level code is an  if  block.   __name__  is a built-in variable which evaluates to the name of the current module.  However, if a module is being run directly (as in  myscript.py  above), then  __name__  instead is set to the string  \"__main__\" .  Thus, you can test whether your script is being run directly or being imported by something else by testing   if __name__ == \"__main__\":     ...    If your script is being imported into another module, its various function and class definitions will be imported and its top-level code will be executed, but the code in the then-body of the  if  clause above won't get run as the condition is not met. As a basic example, consider the following two scripts:   # file one.py def func():     print(\"func() in one.py\")  print(\"top-level in one.py\")  if __name__ == \"__main__\":     print(\"one.py is being run directly\") else:     print(\"one.py is being imported into another module\")    # file two.py import one  print(\"top-level in two.py\") one.func()  if __name__ == \"__main__\":     print(\"two.py is being run directly\") else:     print(\"two.py is being imported into another module\")    Now, if you invoke the interpreter as   python one.py    The output will be   top-level in one.py one.py is being run directly    If you run  two.py  instead:   python two.py    You get   top-level in one.py one.py is being imported into another module top-level in two.py func() in one.py two.py is being run directly    Thus, when module  one  gets loaded, its  __name__  equals  \"one\"  instead of  \"__main__\" .", "score": 2204}}
{"question": "What does if __name__ == &quot;__main__&quot;: do?", "tags": ["python", "namespaces", "program-entry-point", "python-module", "idioms"], "link": "https://stackoverflow.com/questions/419163/what-does-if-name-main-do", "answer_count": 40, "answers": {"id": 419185, "body": "Short Answer   It's boilerplate code that protects users from accidentally invoking the script when they didn't intend to. Here are some common problems when the guard is omitted from a script:     If you import the guardless script in another script (e.g.  import my_script_without_a_name_eq_main_guard ), then the latter script will trigger the former to run  at import time  and  using the second script's command line arguments . This is almost always a mistake.     If you have a custom class in the guardless script and save it to a pickle file, then unpickling it in another script will trigger an import of the guardless script, with the same problems outlined in the previous bullet.       Long Answer   To better understand why and how this matters, we need to take a step back to understand how Python initializes scripts and how this interacts with its module import mechanism.   Whenever the Python interpreter reads a source file, it does two things:     it sets a few special variables like  __name__ , and then     it executes all of the code found in the file.       Let's see how this works and how it relates to your question about the  __name__  checks we always see in Python scripts.   Code Sample   Let's use a slightly different code sample to explore how imports and scripts work.  Suppose the following is in a file called  foo.py .   # Suppose this is foo.py.  print(\"before import\") import math  print(\"before function_a\") def function_a():     print(\"Function A\")  print(\"before function_b\") def function_b():     print(\"Function B {}\".format(math.sqrt(100)))  print(\"before __name__ guard\") if __name__ == '__main__':     function_a()     function_b() print(\"after __name__ guard\")    Special Variables   When the Python interpreter reads a source file, it first defines a few special variables. In this case, we care about the  __name__  variable.   When Your Module Is the Main Program   If you are running your module (the source file) as the main program, e.g.   python foo.py    the interpreter will assign the hard-coded string  \"__main__\"  to the  __name__  variable, i.e.   # It's as if the interpreter inserts this at the top # of your module when run as the main program. __name__ = \"__main__\"     When Your Module Is Imported By Another   On the other hand, suppose some other module is the main program and it imports your module. This means there's a statement like this in the main program, or in some other module the main program imports:   # Suppose this is in some other main program. import foo    The interpreter will search for your  foo.py  file (along with searching for a few other variants), and prior to executing that module, it will assign the name  \"foo\"  from the import statement to the  __name__  variable, i.e.   # It's as if the interpreter inserts this at the top # of your module when it's imported from another module. __name__ = \"foo\"    Executing the Module's Code   After the special variables are set up, the interpreter executes all the code in the module, one statement at a time. You may want to open another window on the side with the code sample so you can follow along with this explanation.   Always     It prints the string  \"before import\"  (without quotes).     It loads the  math  module and assigns it to a variable called  math . This is equivalent to replacing  import math  with the following (note that  __import__  is a low-level function in Python that takes a string and triggers the actual import):       # Find and load a module given its string name, \"math\", # then assign it to a local variable called math. math = __import__(\"math\")      It prints the string  \"before function_a\" .     It executes the  def  block, creating a function object, then assigning that function object to a variable called  function_a .     It prints the string  \"before function_b\" .     It executes the second  def  block, creating another function object, then assigning it to a variable called  function_b .     It prints the string  \"before __name__ guard\" .       Only When Your Module Is the Main Program     If your module is the main program, then it will see that  __name__  was indeed set to  \"__main__\"  and it calls the two functions, printing the strings  \"Function A\"  and  \"Function B 10.0\" .     Only When Your Module Is Imported by Another     ( instead ) If your module is not the main program but was imported by another one, then  __name__  will be  \"foo\" , not  \"__main__\" , and it'll skip the body of the  if  statement.     Always     It will print the string  \"after __name__ guard\"  in both situations.     Summary   In summary, here's what'd be printed in the two cases:   # What gets printed if foo is the main program before import before function_a before function_b before __name__ guard Function A Function B 10.0 after __name__ guard    # What gets printed if foo is imported as a regular module before import before function_a before function_b before __name__ guard after __name__ guard    Why Does It Work This Way?   You might naturally wonder why anybody would want this.  Well, sometimes you want to write a  .py  file that can be both used by other programs and/or modules as a module, and can also be run as the main program itself.  Examples:     Your module is a library, but you want to have a script mode where it runs some unit tests or a demo.     Your module is only used as a main program, but it has some unit tests, and the testing framework works by importing  .py  files like your script and running special test functions. You don't want it to try running the script just because it's importing the module.     Your module is mostly used as a main program, but it also provides a programmer-friendly API for advanced users.       Beyond those examples, it's elegant that running a script in Python is just setting up a few magic variables and importing the script. \"Running\" the script is a side effect of importing the script's module.   Food for Thought     Question: Can I have multiple  __name__  checking blocks?  Answer: it's strange to do so, but the language won't stop you.     Suppose the following is in  foo2.py .  What happens if you say  python foo2.py  on the command-line? Why?       # Suppose this is foo2.py. import os, sys; sys.path.insert(0, os.path.dirname(__file__)) # needed for some interpreters  def function_a():     print(\"a1\")     from foo2 import function_b     print(\"a2\")     function_b()     print(\"a3\")  def function_b():     print(\"b\")  print(\"t1\") if __name__ == \"__main__\":     print(\"m1\")     function_a()     print(\"m2\") print(\"t2\")             Now, figure out what will happen in  foo3.py  (having removed the  __name__  check):     # Suppose this is foo3.py. import os, sys; sys.path.insert(0, os.path.dirname(__file__)) # needed for some interpreters  def function_a():     print(\"a1\")     from foo3 import function_b     print(\"a2\")     function_b()     print(\"a3\")  def function_b():     print(\"b\")  print(\"t1\") print(\"m1\") function_a() print(\"m2\") print(\"t2\")      What will this do when used as a script?  When imported as a module?     # Suppose this is in foo4.py __name__ = \"__main__\"  def bar():     print(\"bar\")      print(\"before __name__ guard\") if __name__ == \"__main__\":     bar() print(\"after __name__ guard\")", "score": 8980}}
{"question": "What does the &quot;yield&quot; keyword do in Python?", "tags": ["python", "iterator", "generator", "yield"], "link": "https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do-in-python", "answer_count": 52, "answers": {"id": 231801, "body": "Think of it this way:   An iterator is just a fancy sounding term for an object that has a  next()  method.  So a yield-ed function ends up being something like this:   Original version:   def some_function():     for i in xrange(4):         yield i  for i in some_function():     print i    This is basically what the Python interpreter does with the above code:   class it:     def __init__(self):         # Start at -1 so that we get 0 when we add 1 below.         self.count = -1      # The __iter__ method will be called once by the 'for' loop.     # The rest of the magic happens on the object returned by this method.     # In this case it is the object itself.     def __iter__(self):         return self      # The next method will be called repeatedly by the 'for' loop     # until it raises StopIteration.     def next(self):         self.count += 1         if self.count < 4:             return self.count         else:             # A StopIteration exception is raised             # to signal that the iterator is done.             # This is caught implicitly by the 'for' loop.             raise StopIteration  def some_func():     return it()  for i in some_func():     print i    For more insight as to what's happening behind the scenes, the  for  loop can be rewritten to this:   iterator = some_func() try:     while 1:         print iterator.next() except StopIteration:     pass    Does that make more sense or just confuse you more?  :)   I should note that this  is  an oversimplification for illustrative purposes. :)", "score": 792}}
{"question": "What does the &quot;yield&quot; keyword do in Python?", "tags": ["python", "iterator", "generator", "yield"], "link": "https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do-in-python", "answer_count": 52, "answers": {"id": 237028, "body": "Shortcut to understanding  yield   When you see a function with  yield  statements, apply this easy trick to understand what will happen:     Insert a line  result = []  at the start of the function.   Replace each  yield expr  with  result.append(expr) .   Insert a line  return result  at the bottom of the function.   Yay - no more  yield  statements! Read and figure out the code.   Compare the function to the original definition.     This trick may give you an idea of the logic behind the function, but what actually happens with  yield  is significantly different than what happens in the list-based approach. In many cases, the yield approach will be a lot more memory efficient and faster too. In other cases, this trick will get you stuck in an infinite loop, even though the original function works just fine. Read on to learn more...   Don't confuse your iterables, iterators, and generators   First, the  iterator protocol  - when you write   for x in mylist:     ...loop body...    Python performs the following two steps:     Gets an iterator for  mylist :   Call  iter(mylist)  -> this returns an object with a  next()  method (or  __next__()  in Python 3).   [This is the step most people forget to tell you about]     Uses the iterator to loop over items:   Keep calling the  next()  method on the iterator returned from step 1. The return value from  next()  is assigned to  x  and the loop body is executed. If an exception  StopIteration  is raised from within  next() , it means there are no more values in the iterator and the loop is exited.       The truth is Python performs the above two steps anytime it wants to  loop over  the contents of an object - so it could be a for loop, but it could also be code like  otherlist.extend(mylist)  (where  otherlist  is a Python list).   Here  mylist  is an  iterable  because it implements the iterator protocol. In a user-defined class, you can implement the  __iter__()  method to make instances of your class iterable. This method should return an  iterator . An iterator is an object with a  next()  method. It is possible to implement both  __iter__()  and  next()  on the same class, and have  __iter__()  return  self . This will work for simple cases, but not when you want two iterators looping over the same object at the same time.   So that's the iterator protocol, many objects implement this protocol:     Built-in lists, dictionaries, tuples, sets, and files.   User-defined classes that implement  __iter__() .   Generators.     Note that a  for  loop doesn't know what kind of object it's dealing with - it just follows the iterator protocol, and is happy to get item after item as it calls  next() . Built-in lists return their items one by one, dictionaries return the  keys  one by one, files return the  lines  one by one, etc. And generators return... well that's where  yield  comes in:   def f123():     yield 1     yield 2     yield 3  for item in f123():     print item    Instead of  yield  statements, if you had three  return  statements in  f123()  only the first would get executed, and the function would exit. But  f123()  is no ordinary function. When  f123()  is called, it  does not  return any of the values in the yield statements! It returns a generator object. Also, the function does not really exit - it goes into a suspended state. When the  for  loop tries to loop over the generator object, the function resumes from its suspended state at the very next line after the  yield  it previously returned from, executes the next line of code, in this case, a  yield  statement, and returns that as the next item. This happens until the function exits, at which point the generator raises  StopIteration , and the loop exits.   So the generator object is sort of like an adapter - at one end it exhibits the iterator protocol, by exposing  __iter__()  and  next()  methods to keep the  for  loop happy. At the other end, however, it runs the function just enough to get the next value out of it and puts it back in suspended mode.   Why use generators?   Usually, you can write code that doesn't use generators but implements the same logic. One option is to use the temporary list 'trick' I mentioned before. That will not work in all cases, for e.g. if you have infinite loops, or it may make inefficient use of memory when you have a really long list. The other approach is to implement a new iterable class  SomethingIter  that keeps the state in instance members and performs the next logical step in its  next()  (or  __next__()  in Python 3) method. Depending on the logic, the code inside the  next()  method may end up looking very complex and prone to bugs. Here generators provide a clean and easy solution.", "score": 2582}}
{"question": "What does the &quot;yield&quot; keyword do in Python?", "tags": ["python", "iterator", "generator", "yield"], "link": "https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do-in-python", "answer_count": 52, "answers": {"id": 231855, "body": "To understand what  yield  does, you must understand what  generators  are. And before you can understand generators, you must understand  iterables .   Iterables   When you create a list, you can read its items one by one. Reading its items one by one is called iteration:   >>> mylist = [1, 2, 3] >>> for i in mylist: ...    print(i) 1 2 3    mylist  is an  iterable . When you use a list comprehension, you create a list, and so an iterable:   >>> mylist = [x*x for x in range(3)] >>> for i in mylist: ...    print(i) 0 1 4    Everything you can use \" for... in... \" on is an iterable;  lists ,  strings , files...   These iterables are handy because you can read them as much as you wish, but you store all the values in memory and this is not always what you want when you have a lot of values.   Generators   Generators are  iterators , a kind of iterable  you can only iterate over once . Generators do not store all the values in memory,  they generate the values on the fly :   >>> mygenerator = (x*x for x in range(3)) >>> for i in mygenerator: ...    print(i) 0 1 4    It is just the same except you used  ()  instead of  [] . BUT, you  cannot  perform  for i in mygenerator  a second time since generators can only be used once: they calculate 0, then forget about it and calculate 1, and end after calculating 4, one by one.   Yield   yield  is a keyword that is used like  return , except the function will return a generator.   >>> def create_generator(): ...    mylist = range(3) ...    for i in mylist: ...        yield i*i ... >>> mygenerator = create_generator() # create a generator >>> print(mygenerator) # mygenerator is an object!   >>> for i in mygenerator: ...     print(i) 0 1 4    Here it's a useless example, but it's handy when you know your function will return a huge set of values that you will only need to read once.   To master  yield , you must understand that  when you call the function, the code you have written in the function body does not run.  The function only returns the generator object, this is a bit tricky.   Then, your code will continue from where it left off each time  for  uses the generator.   Now the hard part:   The first time the  for  calls the generator object created from your function, it will run the code in your function from the beginning until it hits  yield , then it'll return the first value of the loop. Then, each subsequent call will run another iteration of the loop you have written in the function and return the next value. This will continue until the generator is considered empty, which happens when the function runs without hitting  yield . That can be because the loop has come to an end, or because you no longer satisfy an  \"if/else\" .     Your code explained   Generator:   # Here you create the method of the node object that will return the generator def _get_child_candidates(self, distance, min_dist, max_dist):      # Here is the code that will be called each time you use the generator object:      # If there is still a child of the node object on its left     # AND if the distance is ok, return the next child     if self._leftchild and distance - max_dist < self._median:         yield self._leftchild      # If there is still a child of the node object on its right     # AND if the distance is ok, return the next child     if self._rightchild and distance + max_dist >= self._median:         yield self._rightchild      # If the function arrives here, the generator will be considered empty     # There are no more than two values: the left and the right children    Caller:   # Create an empty list and a list with the current object reference result, candidates = list(), [self]  # Loop on candidates (they contain only one element at the beginning) while candidates:      # Get the last candidate and remove it from the list     node = candidates.pop()      # Get the distance between obj and the candidate     distance = node._get_dist(obj)      # If the distance is ok, then you can fill in the result     if distance <= max_dist and distance >= min_dist:         result.extend(node._values)      # Add the children of the candidate to the candidate's list     # so the loop will keep running until it has looked     # at all the children of the children of the children, etc. of the candidate     candidates.extend(node._get_child_candidates(distance, min_dist, max_dist))  return result    This code contains several smart parts:     The loop iterates on a list, but the list expands while the loop is being iterated. It's a concise way to go through all these nested data even if it's a bit dangerous since you can end up with an infinite loop. In this case,  candidates.extend(node._get_child_candidates(distance, min_dist, max_dist))  exhausts all the values of the generator, but  while  keeps creating new generator objects which will produce different values from the previous ones since it's not applied on the same node.     The  extend()  method is a list object method that expects an iterable and adds its values to the list.       Usually, we pass a list to it:   >>> a = [1, 2] >>> b = [3, 4] >>> a.extend(b) >>> print(a) [1, 2, 3, 4]    But in your code, it gets a generator, which is good because:     You don't need to read the values twice.   You may have a lot of children and you don't want them all stored in memory.     And it works because Python does not care if the argument of a method is a list or not. Python expects iterables so it will work with strings, lists, tuples, and generators! This is called duck typing and is one of the reasons why Python is so cool. But this is another story, for another question...   You can stop here, or read a little bit to see an advanced use of a generator:   Controlling a generator exhaustion   >>> class Bank(): # Let's create a bank, building ATMs ...    crisis = False ...    def create_atm(self): ...        while not self.crisis: ...            yield \"$100\" >>> hsbc = Bank() # When everything's ok the ATM gives you as much as you want >>> corner_street_atm = hsbc.create_atm() >>> print(corner_street_atm.next()) $100 >>> print(corner_street_atm.next()) $100 >>> print([corner_street_atm.next() for cash in range(5)]) ['$100', '$100', '$100', '$100', '$100'] >>> hsbc.crisis = True # Crisis is coming, no more money! >>> print(corner_street_atm.next())   >>> wall_street_atm = hsbc.create_atm() # It's even true for new ATMs >>> print(wall_street_atm.next())   >>> hsbc.crisis = False # The trouble is, even post-crisis the ATM remains empty >>> print(corner_street_atm.next())   >>> brand_new_atm = hsbc.create_atm() # Build a new one to get back in business >>> for cash in brand_new_atm: ...    print cash $100 $100 $100 $100 $100 $100 $100 $100 $100 ...    Note:  For Python 3, use print(corner_street_atm.__next__())  or  print(next(corner_street_atm))   It can be useful for various things like controlling access to a resource.   Itertools, your best friend   The  itertools  module contains special functions to manipulate iterables. Ever wish to duplicate a generator? Chain two generators? Group values in a nested list with a one-liner?  Map / Zip  without creating another list?   Then just  import itertools .   An example? Let's see the possible orders of arrival for a four-horse race:   >>> horses = [1, 2, 3, 4] >>> races = itertools.permutations(horses) >>> print(races)   >>> print(list(itertools.permutations(horses))) [(1, 2, 3, 4),  (1, 2, 4, 3),  (1, 3, 2, 4),  (1, 3, 4, 2),  (1, 4, 2, 3),  (1, 4, 3, 2),  (2, 1, 3, 4),  (2, 1, 4, 3),  (2, 3, 1, 4),  (2, 3, 4, 1),  (2, 4, 1, 3),  (2, 4, 3, 1),  (3, 1, 2, 4),  (3, 1, 4, 2),  (3, 2, 1, 4),  (3, 2, 4, 1),  (3, 4, 1, 2),  (3, 4, 2, 1),  (4, 1, 2, 3),  (4, 1, 3, 2),  (4, 2, 1, 3),  (4, 2, 3, 1),  (4, 3, 1, 2),  (4, 3, 2, 1)]    Understanding the inner mechanisms of iteration   Iteration is a process implying iterables (implementing the  __iter__()  method) and iterators (implementing the  __next__()  method). Iterables are any objects you can get an iterator from. Iterators are objects that let you iterate on iterables.   There is more about it in this article about  how  for  loops work .", "score": 18217}}
